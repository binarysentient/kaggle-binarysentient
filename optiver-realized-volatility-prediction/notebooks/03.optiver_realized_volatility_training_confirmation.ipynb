{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2f18e2-9894-4f0c-bd57-8b37afc02309",
   "metadata": {},
   "source": [
    "### Can our model predict current volatility?  (forget future; first it should be capable of predicting current one with given features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c4e895-fd66-490d-a8b8-d28b324d3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from optiver_features_handler import get_features_map_for_stock, get_row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750eb0e3-2860-490b-bc3c-2a512485a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\n",
    "OUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\n",
    "MODEL_OUTPUT_DIRECTORY = os.path.join(OUTPUT_DIRECTORY,\"models\")\n",
    "os.makedirs(OUTPUT_DIRECTORY,exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIRECTORY,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02947ab-aa1d-4af0-9d6e-7a51cff159ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interval_seconds = 2\n",
    "data_intervals_count = int(600/data_interval_seconds)\n",
    "class OptiverRealizedVolatilityDataset(Dataset):\n",
    "    def __init__(self, data_directory, mode=\"train\", lazy_load=True):\n",
    "        \"\"\"initializes Optiver Competition dataset\n",
    "        `mode`: train|test\n",
    "        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n",
    "        \"\"\"\n",
    "        print(\"INIT: OptiverRealizedVolatilityDataset\")\n",
    "        if mode.lower() not in ['train','test']:\n",
    "            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n",
    "        self.data_directory = data_directory\n",
    "        self.mode = mode.lower()\n",
    "        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n",
    "#         if self.mode == 'train':\n",
    "#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n",
    "        if self.mode == 'test':\n",
    "            self.main_df['target'] = 0\n",
    "        \n",
    "        self.cache_stocks_done_set = set()\n",
    "        # this is our final features lookup where we park all our features which can be addressed by row_id\n",
    "        # which is individual train/test.csv row id using 'stock_id`-`time_id`\n",
    "        self.cache_rowid_feature_map = {}\n",
    "        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n",
    "        targets = self.main_df['target'].tolist()\n",
    "        self.stock_possible_timeids_list = {}\n",
    "        for idx, row_id in enumerate(row_id_series.tolist()):\n",
    "            stock_id = int(row_id.split('-')[0])\n",
    "            time_id = int(row_id.split('-')[1])\n",
    "            self.cache_rowid_feature_map[row_id] = {'target':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n",
    "            \n",
    "            # below code is to make sure what timeids we expect from stock data extractor\n",
    "            # in case of missing parquet files we'll have to know the keys to fill default values into\n",
    "            if stock_id not in self.stock_possible_timeids_list:\n",
    "                self.stock_possible_timeids_list[stock_id] = []\n",
    "            self.stock_possible_timeids_list[stock_id].append(time_id)\n",
    "            \n",
    "        \n",
    "        if lazy_load == False:\n",
    "            worker_data = []\n",
    "            for gkey, gdf in self.main_df.groupby(['stock_id']):\n",
    "                worker_data.append((self.data_directory, self.mode, gkey))\n",
    "#             print(\"---------- CPU COUNG:\", multiprocessing.cpu_count())\n",
    "            # NOTE: this was hell of a hunt; this windows and pytorch and jupyter combination is too tedious\n",
    "            #       make sure the function that we distribute don't call pytorch\n",
    "            chunksize = multiprocessing.cpu_count() * 1\n",
    "            processed = 0\n",
    "            for worker_data_chunk in [worker_data[i * chunksize:(i + 1) * chunksize] for i in range((len(worker_data) + chunksize - 1) // chunksize )]:\n",
    "                with Pool(multiprocessing.cpu_count()) as p:\n",
    "                    \n",
    "                    feature_set_list = p.starmap(get_features_map_for_stock, worker_data_chunk)\n",
    "                    \n",
    "                    for feature_map in feature_set_list:\n",
    "                        for rowid, features_dict in feature_map.items():\n",
    "                            for fkey,fval in features_dict.items():\n",
    "                                self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                            self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                        # udpate the indications that we've already fetched this stock and the lazy loader code won't fetch this again\n",
    "                        self.cache_stocks_done_set.add(int(rowid.split('-')[0]))\n",
    "                    \n",
    "                    processed += chunksize\n",
    "                    print(f\"Processed and loaded {processed} stocks features.\")\n",
    "    \n",
    "    def __cache_generate_features(self, main_stock_id, main_time_id):\n",
    "            \n",
    "            main_row_id = get_row_id(main_stock_id, main_time_id)\n",
    "            if main_stock_id not in self.cache_stocks_done_set:\n",
    "#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))   \n",
    "                # we'll combine the featureset with the bigger feature set of all stocks\n",
    "                feature_map = get_features_map_for_stock(self.data_directory, self.mode, main_stock_id)\n",
    "                # NOTE: sometime we might now have parquet files in that case we'll have 3 entried in .csv while only 1 gets returned in feature map\n",
    "                # we need to cover for that disparity\n",
    "                for time_id in self.stock_possible_timeids_list[main_stock_id]:\n",
    "                    expected_row_id = get_row_id(main_stock_id, time_id)\n",
    "                    if expected_row_id not in feature_map:\n",
    "                        feature_map[expected_row_id] = {}\n",
    "                for rowid, features_dict in feature_map.items():\n",
    "                    for fkey,fval in features_dict.items():\n",
    "                        self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                    self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                self.cache_stocks_done_set.add(main_stock_id)\n",
    "#             print(self.cache_rowid_feature_map[main_row_id])\n",
    "#             print(torch.tensor([self.cache_rowid_feature_map[main_row_id].get('book_realized_volatility',0)]))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map[main_row_id].get('log_return1_2s', [0]*(int(600/2)))))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map.get('book_directional_volume1_2s', [0]*(int(600/2)))))\n",
    "            return self.cache_rowid_feature_map[main_row_id]\n",
    "        \n",
    "    @staticmethod\n",
    "    def transform_to_01_realized_volatility_linear_data(features_dict):\n",
    "        return (\n",
    "                {\n",
    "                    'row_id':features_dict['row_id'],\n",
    "                    'stock_id':torch.tensor(features_dict['stock_id'], dtype=torch.float32),\n",
    "                    'seconds_in_bucket_xs': torch.tensor(np.nan_to_num(features_dict.get('seconds_in_bucket_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "#                     'book_realized_volatility':torch.tensor([features_dict.get('book_realized_volatility',0)]),\n",
    "                    # TRADE FEATURES\n",
    "                    'logrett_xs': torch.tensor(np.nan_to_num(features_dict.get('logrett_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_volume_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_volume_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_ordercount_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_ordercount_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "#                     'trade_money_turnover_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_money_turnover_per_order_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_per_order_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    \n",
    "#                     'trade_money_turnover_mean': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_mean', 0)), dtype=torch.float32),\n",
    "#                     'trade_money_turnover_std': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_std', 0)), dtype=torch.float32),\n",
    "                    # BOOK FEATURES\n",
    "                    'logret1_xs': torch.tensor(np.nan_to_num(features_dict.get('logret1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "#                     'logret2_xs': torch.tensor(np.nan_to_num(features_dict.get('logret2_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "#                     'book_directional_volume1_xs': torch.tensor(np.nan_to_num(features_dict.get('book_directional_volume1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "#                     'book_directional_volume2_xs': torch.tensor(np.nan_to_num(features_dict.get('book_directional_volume2_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_price_spread1_xs': torch.tensor(np.nan_to_num(features_dict.get('book_price_spread1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "#                     'book_price_spread2_xs': torch.tensor(np.nan_to_num(features_dict.get('book_price_spread2_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_bid_spread_xs': torch.tensor(np.nan_to_num(features_dict.get('book_bid_spread_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_ask_spread_xs': torch.tensor(np.nan_to_num(features_dict.get('book_ask_spread_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_total_volume_xs': torch.tensor(np.nan_to_num(features_dict.get('book_total_volume_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_volume_imbalance_xs': torch.tensor(np.nan_to_num(features_dict.get('book_volume_imbalance_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "#                     'book_money_turnover1_xs': torch.tensor(np.nan_to_num(features_dict.get('book_money_turnover1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    \n",
    "#                     'askp2_1s':torch.tensor(features_dict.get('askp2_1s', [0]*(int(600/1)))),\n",
    "#                     'book_directional_volume1_1s':torch.tensor(features_dict.get('book_directional_volume1_1s', [0]*(int(600/1)))) \n",
    "                },\n",
    "                torch.tensor([features_dict['target']])\n",
    "#                 [features_dict['target']]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: handle for num_workers more than 0\n",
    "        #      using https://pytorch.org/docs/stable/data.html\n",
    "        #      using torch.util.data.get_worker_info()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        stock_id = self.main_df.at[idx, 'stock_id']\n",
    "        time_id = self.main_df.at[idx, 'time_id']\n",
    "        x,y = self.__cache_generate_features(stock_id,time_id)\n",
    "#         x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efcee691-c9ce-481f-a4e8-dbf97b66b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT: OptiverRealizedVolatilityDataset\n",
      "Processed and loaded 16 stocks features.\n",
      "Processed and loaded 32 stocks features.\n",
      "Processed and loaded 48 stocks features.\n",
      "Processed and loaded 64 stocks features.\n",
      "Processed and loaded 80 stocks features.\n",
      "Processed and loaded 96 stocks features.\n",
      "Processed and loaded 112 stocks features.\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\", lazy_load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e2ec3227-ffc6-4099-b51c-04a3bd2df633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seconds_in_bucket_xs': {'mean': 300.9866943359375, 'std': 173.2118682861328},\n",
       " 'logrett_xs': {'mean': -9.3557550595591e-10, 'std': 0.00035829044645652175},\n",
       " 'trade_volume_xs': {'mean': 106.20071411132812, 'std': 738.740234375},\n",
       " 'trade_ordercount_xs': {'mean': 1.244789481163025, 'std': 5.131072044372559},\n",
       " 'trade_money_turnover_per_order_xs': {'mean': 21.616464614868164,\n",
       "  'std': 68.66466522216797},\n",
       " 'logret1_xs': {'mean': -8.231244019718531e-10, 'std': 0.00041921899537555873},\n",
       " 'book_price_spread1_xs': {'mean': 0.0006618599873036146,\n",
       "  'std': 0.0007574831834062934},\n",
       " 'book_bid_spread_xs': {'mean': 0.00019766353943850845,\n",
       "  'std': 0.0002465708530507982},\n",
       " 'book_ask_spread_xs': {'mean': 0.00020006491104140878,\n",
       "  'std': 0.0002516494132578373},\n",
       " 'book_total_volume_xs': {'mean': 5433.1416015625, 'std': 33311.83984375},\n",
       " 'book_volume_imbalance_xs': {'mean': 756.9369506835938,\n",
       "  'std': 5849.05810546875}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_scaling_feature_map = {}\n",
    "for key,val in all_feature_dict.items():\n",
    "    if type(val[0]) is not list and type(val[0]) is not str and val[0].type() is not str:\n",
    "        if len(val[0].size())>0:\n",
    "            mean = torch.mean(torch.cat(val).reshape(-1)).item()\n",
    "            std = torch.std(torch.cat(val).reshape(-1)).item()\n",
    "            standard_scaling_feature_map[key] = {'mean':mean,'std':std}\n",
    "#             print(key, 'mean', , 'std', )\n",
    "standard_scaling_feature_map       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6352c9e6-a077-479b-abe6-b7e128e98a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seconds_in_bucket_xs': {'mean': 300.9866943359375, 'std': 173.2118682861328},\n",
       " 'logrett_xs': {'mean': -9.3557550595591e-10, 'std': 0.00035829044645652175},\n",
       " 'trade_volume_xs': {'mean': 106.20071411132812, 'std': 738.740234375},\n",
       " 'trade_ordercount_xs': {'mean': 1.244789481163025, 'std': 5.131072044372559},\n",
       " 'trade_money_turnover_per_order_xs': {'mean': 21.616464614868164,\n",
       "  'std': 68.66466522216797},\n",
       " 'logret1_xs': {'mean': -8.231244019718531e-10, 'std': 0.00041921899537555873},\n",
       " 'book_price_spread1_xs': {'mean': 0.0006618599873036146,\n",
       "  'std': 0.0007574831834062934},\n",
       " 'book_bid_spread_xs': {'mean': 0.00019766353943850845,\n",
       "  'std': 0.0002465708530507982},\n",
       " 'book_ask_spread_xs': {'mean': 0.00020006491104140878,\n",
       "  'std': 0.0002516494132578373},\n",
       " 'book_total_volume_xs': {'mean': 5433.1416015625, 'std': 33311.83984375},\n",
       " 'book_volume_imbalance_xs': {'mean': 756.9369506835938,\n",
       "  'std': 5849.05810546875}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_scaling_feature_map = {'seconds_in_bucket_xs': {'mean': 300.9866943359375, 'std': 173.2118682861328},\n",
    " 'logrett_xs': {'mean': -9.3557550595591e-10, 'std': 0.00035829044645652175},\n",
    " 'trade_volume_xs': {'mean': 106.20071411132812, 'std': 738.740234375},\n",
    " 'trade_ordercount_xs': {'mean': 1.244789481163025, 'std': 5.131072044372559},\n",
    " 'trade_money_turnover_per_order_xs': {'mean': 21.616464614868164,\n",
    "  'std': 68.66466522216797},\n",
    " 'logret1_xs': {'mean': -8.231244019718531e-10, 'std': 0.00041921899537555873},\n",
    " 'book_price_spread1_xs': {'mean': 0.0006618599873036146,\n",
    "  'std': 0.0007574831834062934},\n",
    " 'book_bid_spread_xs': {'mean': 0.00019766353943850845,\n",
    "  'std': 0.0002465708530507982},\n",
    " 'book_ask_spread_xs': {'mean': 0.00020006491104140878,\n",
    "  'std': 0.0002516494132578373},\n",
    " 'book_total_volume_xs': {'mean': 5433.1416015625, 'std': 33311.83984375},\n",
    " 'book_volume_imbalance_xs': {'mean': 756.9369506835938,\n",
    "  'std': 5849.05810546875}}\n",
    "standard_scaling_feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d58ff-26aa-4b02-b2ac-991ccb366c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for x in range(0,9):\n",
    "#     print(dataset[x])\n",
    "\n",
    "# dataset[10000] #[0]['bidp1_1s']\n",
    "for key,val in dataset[10000][0].items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2124b8d-39fb-4801-a3c1-378ee390a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset, batch_size=4096,shuffle=True, num_workers=0, pin_memory=True)\n",
    "sizes = set()\n",
    "for train_batch_idx, (feature_dict, feature_y) in enumerate(dataloader_train):\n",
    "    sizes.add(f\"{feature_dict['logrett_xs'].size()}\")\n",
    "        \n",
    "        \n",
    "#         print(val)\n",
    "#         input()\n",
    "#     print(x)\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ca3693b7-4338-49c6-8a4f-9a8205d4aa33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2a72c9b-fab8-4536-b125-abc46cc800b6",
   "metadata": {},
   "source": [
    "### Learnings about model CNN input\n",
    "- it's better to use multiple channel for logreturn1 and logreturn2 than stacking it and using as one channel\n",
    "- 2 channels input for CNN is better than stacking it(dim 2, which is logret1_t1, logret2_t1, logret1_t2, logret2_t2...) and using it as one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4b98ff-b328-465e-a04e-c25eef8ea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = None\n",
    "\n",
    "\n",
    "def loss_fn_mse(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)))\n",
    "\n",
    "def loss_fn_mspe(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)/y))\n",
    "\n",
    "def loss_fn_orig(y, pred):\n",
    "    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a6a979-c114-4525-953f-f5c368fe26d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_stocks = dataset.main_df['stock_id'].max()\n",
    "number_of_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "15e35973-37f1-408a-b47f-3a8fa6c9879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_optiver_feature(feature_name, feature_tensor):\n",
    "    standard_scaling_feature_map = {'seconds_in_bucket_xs': {'mean': 300.9866943359375, 'std': 173.2118682861328},\n",
    "         'logrett_xs': {'mean': -9.3557550595591e-10, 'std': 0.00035829044645652175},\n",
    "         'trade_volume_xs': {'mean': 106.20071411132812, 'std': 738.740234375},\n",
    "         'trade_ordercount_xs': {'mean': 1.244789481163025, 'std': 5.131072044372559},\n",
    "         'trade_money_turnover_per_order_xs': {'mean': 21.616464614868164,\n",
    "          'std': 68.66466522216797},\n",
    "         'logret1_xs': {'mean': -8.231244019718531e-10, 'std': 0.00041921899537555873},\n",
    "         'book_price_spread1_xs': {'mean': 0.0006618599873036146,\n",
    "          'std': 0.0007574831834062934},\n",
    "         'book_bid_spread_xs': {'mean': 0.00019766353943850845,\n",
    "          'std': 0.0002465708530507982},\n",
    "         'book_ask_spread_xs': {'mean': 0.00020006491104140878,\n",
    "          'std': 0.0002516494132578373},\n",
    "         'book_total_volume_xs': {'mean': 5433.1416015625, 'std': 33311.83984375},\n",
    "         'book_volume_imbalance_xs': {'mean': 756.9369506835938,\n",
    "          'std': 5849.05810546875}}\n",
    "    if feature_name in standard_scaling_feature_map:\n",
    "        return (feature_tensor - standard_scaling_feature_map[feature_name]['mean'])/standard_scaling_feature_map[feature_name]['std']\n",
    "    if feature_name in ['logrett_xs',\n",
    "                             'logret1_xs','logret2_xs','book_bid_spread_xs','book_ask_spread_xs']:\n",
    "        return feature_tensor * 10000\n",
    "    if feature_name in ['book_price_spread1_xs','book_price_spread2_xs']:\n",
    "        return feature_tensor * 1000\n",
    "    if feature_name in ['trade_ordercount_xs','trade_volume_xs','trade_money_turnover_xs','trade_money_turnover_per_order_xs',\n",
    "                         'book_total_volume_xs','book_volume_imbalance_xs','book_money_turnover1_xs']:\n",
    "        return torch.log(feature_x + 1)\n",
    "    \n",
    "class StockIdEmbedding(nn.Module):\n",
    "    def __init__(self,number_of_stock_embeddings=126+10, number_of_stock_embedding_dimention=2, mode='train'):\n",
    "        super(StockIdEmbedding, self).__init__()\n",
    "        self.use_stock_id = use_stock_id\n",
    "        \n",
    "        self.number_of_stock_embeddings = number_of_stock_embeddings\n",
    "        self.number_of_stock_embedding_dimention = number_of_stock_embedding_dimention\n",
    "        self.stock_embedding = nn.Embedding(self.number_of_stock_embeddings, self.number_of_stock_embedding_dimention)\n",
    "        self.mode = mode\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(self.number_of_stock_embedding_dimention, 32),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return []\n",
    "    \n",
    "    def set_mode(self,mode):\n",
    "        self.mode = mode\n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "        \n",
    "        stock_id_clamped = torch.clamp(feature_dict['stock_id'],0,self.number_of_stock_embeddings-1)\n",
    "        stock_id_clamped = stock_id_clamped.type(torch.cuda.IntTensor)\n",
    "        stock_id_clamped = stock_id_clamped.to(device).reshape(-1,1)\n",
    "        embedding_logits = self.stock_embedding(stock_id_clamped)\n",
    "        embedding_logits = embedding_logits.reshape(-1,self.number_of_stock_embedding_dimention)\n",
    "        \n",
    "        if self.mode == 'stock_id_embedding':\n",
    "            return embedding_logits\n",
    "\n",
    "            \n",
    "        logits = self.linear_stack(embedding_logits)\n",
    "        return logits\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, feature_generator_mode_hidden_size=64, mode='train'):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.use_stock_id = use_stock_id\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mode = mode\n",
    "        self.feature_generator_mode_hidden_size = feature_generator_mode_hidden_size\n",
    "        self.stock_id_embedding = StockIdEmbedding(number_of_stock_embedding_dimention=2, mode='train')\n",
    "        self.cnn_stack = nn.Sequential(\n",
    "            nn.Conv1d(8, 16, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Hardswish(),\n",
    "#             nn.BatchNorm1d(4),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.Conv1d(16, 24, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv1d(24, 32, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Hardswish(),\n",
    "\n",
    "        )\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.LazyLinear(512),\n",
    "            nn.Hardswish(),\n",
    "#             nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(512, 512),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(256, self.feature_generator_mode_hidden_size),\n",
    "#             nn.Hardswish(),\n",
    "\n",
    "        )\n",
    "        self.linear_hybrid = nn.Sequential(\n",
    "            nn.Linear(self.feature_generator_mode_hidden_size, 32),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(256, 256),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return []\n",
    "    \n",
    "    def set_mode(self,mode):\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train_stock_id_embedding':\n",
    "            self.stock_id_embedding.set_mode('train')\n",
    "        else:\n",
    "            self.stock_id_embedding.set_mode('stock_id_embedding')\n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "#         logits = self.basic_stack(x)\n",
    "#         x = self.flatten(x)\n",
    "        x = torch.cat([\n",
    "                            scale_optiver_feature('logrett_xs',feature_dict['logrett_xs']).to(device), \n",
    "                            scale_optiver_feature('logret1_xs',feature_dict['logret1_xs']).to(device),\n",
    "                            scale_optiver_feature('book_price_spread1_xs',feature_dict['book_price_spread1_xs']).to(device),\n",
    "                            scale_optiver_feature('book_bid_spread_xs',feature_dict['book_bid_spread_xs']).to(device),\n",
    "                            scale_optiver_feature('book_ask_spread_xs',feature_dict['book_ask_spread_xs']).to(device),\n",
    "                            scale_optiver_feature('book_total_volume_xs',feature_dict['book_total_volume_xs']).to(device),\n",
    "                            scale_optiver_feature('book_volume_imbalance_xs',feature_dict['book_volume_imbalance_xs']).to(device),\n",
    "                            scale_optiver_feature('trade_money_turnover_per_order_xs',feature_dict['trade_money_turnover_per_order_xs']).to(device),\n",
    "#                             scale_optiver_feature('logrett_xs',feature_dict['logrett_xs']).to(device),\n",
    "\n",
    "#                                 feature_dict['logret1_xs'].to(device)*10000,\n",
    "                            \n",
    "#                                     feature_dict['book_price_spread1_xs'].to(device)*1000, \n",
    "#                                 feature_dict['book_bid_spread_xs'].to(device)*10000, \n",
    "#                                 feature_dict['book_ask_spread_xs'].to(device)*10000, \n",
    "\n",
    "#                                 torch.log(feature_dict['book_total_volume_xs'].to(device)+1),\n",
    "#                                 torch.log(feature_dict['book_volume_imbalance_xs'].to(device)+1),\n",
    "\n",
    "#                                 torch.log(feature_dict['trade_money_turnover_per_order_xs'].to(device)+1),\n",
    "                          ], 1)\n",
    "\n",
    "#         x = torch.nan_to_num(feature_dict['logrett_xs']).type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        \n",
    "#         print(x)\n",
    "#         input()\n",
    "#         if torch.isnan(x).any():\n",
    "# #             print(x)\n",
    "#             print(feature_dict)\n",
    "#             input()\n",
    "        x = x.to(device)\n",
    "        x = x.reshape(-1,8,data_intervals_count)\n",
    "        \n",
    "        logits = self.cnn_stack(x)\n",
    "        logits = self.flatten(logits)\n",
    "        \n",
    "        #         if self.use_stock_id:\n",
    "        embedding_logits = self.stock_id_embedding(feature_dict)\n",
    "        \n",
    "        if self.mode == 'train_stock_id_embedding':\n",
    "            # in that case embedding logits are predicted volatility\n",
    "            return embedding_logits\n",
    "        \n",
    "#         print('cat',logits.size(), embedding_logits.size())\n",
    "        logits = torch.cat([logits,embedding_logits], 1)\n",
    "#         logits = embedding_logits\n",
    "        \n",
    "        \n",
    "        logits = self.linear_stack(logits)\n",
    "        \n",
    "        if self.mode == 'hidden_generator':\n",
    "            return logits\n",
    "#         logits = torch.cat( [logits, \n",
    "#                              torch.log(feature_dict['trade_money_turnover_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001), \n",
    "#                                            torch.log(feature_dict['trade_money_turnover_std'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                            torch.log(feature_dict['trade_price_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                            torch.log(feature_dict['book_money_turnover_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                            torch.log(feature_dict['book_money_turnover_std'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                            torch.log(feature_dict['book_price_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001)\n",
    "#                                       ], 1)\n",
    "        \n",
    "#         if self.use_stock_id:\n",
    "#             stock_id = torch.tensor(feature_dict['stock_id']).reshape(-1,1)\n",
    "#             stock_id = stock_id.to(device)\n",
    "#             logits = torch.cat([logits, stock_id], 1)\n",
    "            \n",
    "        logits = self.linear_hybrid(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "316cdf05-8827-491b-805c-902be90598c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VolatilityGRU(nn.Module):\n",
    "#     def __init__(self, input_size=1, hidden_size=64, repeated_cells=1):\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "class MultiFetGRU(nn.Module):\n",
    "    def __init__(self,feature_names, hidden_size=64, layers=1, dropout=0, features_out=32, mode=\"train\"):\n",
    "        \"\"\"single feature, feature learner\n",
    "        `mode`: train|feature_generator\n",
    "        \"\"\"\n",
    "        super(MultiFetGRU,self).__init__()\n",
    "        if type(feature_names) == str:\n",
    "            feature_names = [feature_names]\n",
    "        self.feature_names = feature_names\n",
    "        self.input_size_ = len(self.feature_names)\n",
    "        \n",
    "        self.hidden_size_ = hidden_size\n",
    "        self.repeated_lstm_cells_ = layers\n",
    "        self.dropout_ = dropout\n",
    "        self.features_out = features_out\n",
    "        \n",
    "        self.rnn_ = nn.GRU(self.input_size_, self.hidden_size_, self.repeated_lstm_cells_, batch_first=True, dropout=self.dropout_)\n",
    "        \n",
    "        self.linear_feature_stack_ = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size_*self.repeated_lstm_cells_, 128),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(128, self.features_out),\n",
    "        )\n",
    "        \n",
    "        self.linear_trainer_stack_ = nn.Sequential(\n",
    "            nn.Linear(self.features_out, 128),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),   \n",
    "        )\n",
    "        \n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def forward(self, feature_dict, h0_tensor=None):\n",
    "        \n",
    "        feature_x = [scale_optiver_feature(feature_name, feature_dict[feature_name]).to(device) for feature_name in self.feature_names]\n",
    "        feature_x = torch.stack(feature_x,dim=2).reshape(-1, data_intervals_count, self.input_size_)\n",
    "        \n",
    "        if self.mode in [\"feature_generator\",\"train\"]:\n",
    "            if h0_tensor is None:\n",
    "#                 h_0_ = torch.rand(self.repeated_lstm_cells_, feature_x.size(0), self.hidden_size_, device=device) #hidden state\n",
    "                h_0_ = torch.zeros(self.repeated_lstm_cells_, feature_x.size(0), self.hidden_size_, device=device) #hidden state\n",
    "            else:\n",
    "                h_0_ = h0_tensor\n",
    "            output_, hn_ = self.rnn_(feature_x, h_0_) #lstm with input, hidden, and internal state\n",
    "            hn_ = hn_.reshape(-1, self.hidden_size_*self.repeated_lstm_cells_) #reshaping the data for Dense layer next  \n",
    "            \n",
    "            out_ = self.linear_feature_stack_(hn_)\n",
    "            \n",
    "            if self.mode == \"train\":\n",
    "                out_ = self.linear_trainer_stack_(out_)\n",
    "            \n",
    "            return out_\n",
    "            \n",
    "            \n",
    "            \n",
    "class VolatilityBSModel(nn.Module):\n",
    "    def __init__(self, mode=\"hybrid\"):\n",
    "        \"\"\"various rnn features' fusion with fully connected nn\n",
    "        `mode`: hybrid|<feature_name>\n",
    "        \"\"\"\n",
    "        super(VolatilityBSModel, self).__init__()\n",
    "        self.mode = mode\n",
    "#         self.feature_list = ['logrett_xs','trade_volume_xs','trade_ordercount_xs','trade_money_turnover_xs','trade_money_turnover_per_order_xs',\n",
    "#                              'logret1_xs','logret2_xs','book_directional_volume1_xs','book_directional_volume2_xs',\n",
    "#                              'book_price_spread1_xs','book_price_spread2_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "#                              'book_total_volume_xs','book_volume_imbalance_xs','book_money_turnover1_xs']\n",
    "        self.feature_list = [['logrett_xs','trade_volume_xs','trade_money_turnover_per_order_xs'],\n",
    "                             ['logret1_xs'],\n",
    "                             ['book_price_spread1_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "                             'book_total_volume_xs','book_volume_imbalance_xs']]\n",
    "         \n",
    "        self.feature_gen_feature_size = 32\n",
    "        self.feature_gen_models = {}\n",
    "        self.rnn_hidden_size = 64\n",
    "        self.rnn_layers = 1\n",
    "        self.hidden_generator_network = NeuralNetwork(feature_generator_mode_hidden_size=self.rnn_hidden_size*self.rnn_layers)\n",
    "        \n",
    "        for k in self.feature_list:\n",
    "            self.feature_gen_models[str(k)]=MultiFetGRU(k, hidden_size=self.rnn_hidden_size, layers=self.rnn_layers, dropout=0.2, features_out=self.feature_gen_feature_size) \n",
    "            self.feature_gen_models[str(k)].to(device)\n",
    "        \n",
    "        \n",
    "        self.linear_fusion = nn.Sequential(\n",
    "            nn.Linear(self.feature_gen_feature_size*len(self.feature_list) + self.rnn_hidden_size*self.rnn_layers, 512),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "        self.set_mode(self.mode)\n",
    "    \n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return self.feature_list\n",
    "    \n",
    "    def set_mode(self, mode):\n",
    "        print(f\"------- set mode : {mode} -----------\")\n",
    "        self.mode = mode\n",
    "        for feature_gen_model in self.feature_gen_models.values():\n",
    "            feature_gen_model.set_mode('feature_generator' if self.mode in ['hybrid','hybrid_feature_out','hidden_generator'] else 'train')\n",
    "        if self.mode == 'hidden_generator':\n",
    "            self.hidden_generator_network.set_mode('train')\n",
    "        else:\n",
    "            self.hidden_generator_network.set_mode('hidden_generator')\n",
    "    \n",
    "    def parameters(self):\n",
    "        \n",
    "        generator_sources_map = {str(k):[v] for k,v in self.feature_gen_models.items()}\n",
    "        generator_sources_map['hybrid']= [self.linear_fusion, self.hidden_generator_network] + list(self.feature_gen_models.values())\n",
    "        generator_sources_map['hidden_generator'] = [self.hidden_generator_network] \n",
    "        params = []\n",
    "        # mode and key is actually str version of array of strings with feature name as values e.g. ['logret1_xs','volume_xs']\n",
    "        if str(self.mode) in generator_sources_map:\n",
    "            for generator_source in generator_sources_map[str(self.mode)]:\n",
    "                for param in generator_source.parameters():\n",
    "                    params.append(param)\n",
    "        else:\n",
    "            return super(VolatilityBSModel,self).parameters()\n",
    "        return params\n",
    "    \n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "        \n",
    "        \n",
    "        if self.mode in self.feature_list:\n",
    "            \n",
    "            \n",
    "            # pass in some randomness to the initial hidden tensor to force it to learn some stuff on its own\n",
    "            # otherwise as the initial hidden layer contains solid infor to minimize the loss; it'll just use that hidden layer to minimize and instead\n",
    "            # learn to not learn and directly bypass initial hidden\n",
    "#             h0_tensor.masked_fill_((torch.rand(h0_tensor.size()) > 0.5).to(device), 0.0)\n",
    "#             h0_tensor = torch.zeros(self.rnn_layers, -1, self.rnn_hidden_size)\n",
    "            out = self.feature_gen_models[str(self.mode)](feature_dict, h0_tensor=None)\n",
    "            return out\n",
    "        \n",
    "        if self.mode in ['hybrid','hybrid_feature_out']:\n",
    "            generated_features = []\n",
    "            for feature_name, feature_gen_model in self.feature_gen_models.items():\n",
    "#                 h0_tensor = torch.zeros(self.rnn_layers, -1, self.rnn_hidden_size)\n",
    "                features_out = feature_gen_model(feature_dict, h0_tensor=None)\n",
    "                generated_features.append(features_out)\n",
    "                \n",
    "                \n",
    "            combined_features = torch.cat(generated_features, 1)#.reshape(-1, self.feature_gen_feature_size*len(self.feature_list))\n",
    "            \n",
    "            cnn_features = self.hidden_generator_network(feature_dict)\n",
    "#             cnn_features = cnn_features.reshape(self.rnn_layers,-1,self.rnn_hidden_size)\n",
    "            combined_features = torch.cat([combined_features,cnn_features],1)\n",
    "            \n",
    "            if self.mode == 'hybrid_feature_out':\n",
    "                return combined_features\n",
    "            \n",
    "            out = self.linear_fusion(combined_features)\n",
    "            return out\n",
    "        \n",
    "#         input(\"--- out got\")\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "9a33b331-4eb1-4c09-84df-01b687ac3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VolatilityBSModel(use_stock_id=use_stock_id)\n",
    "# # model = NeuralNetwork(use_stock_id=False)\n",
    "# model.to(device)\n",
    "# adam_for_modes = {}\n",
    "\n",
    "# for modeidx, mode in enumerate(['yoyo','trade','experiment','book','hybrid']*1):\n",
    "#     epochs = 1\n",
    "#     model.mode = mode\n",
    "#     print(model.parameters())\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1840c-2bc4-4521-933f-994f1ceb014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VolatilityBSModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57c9218f-b67a-49af-bd37-ff307bd12ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stock_id', 'row_id', 'trade_ordercount_xs', 'book_total_volume_xs', 'seconds_in_bucket_xs', 'logret1_xs', 'book_bid_spread_xs', 'trade_money_turnover_per_order_xs', 'trade_volume_xs', 'book_price_spread1_xs', 'book_volume_imbalance_xs', 'logrett_xs', 'book_ask_spread_xs'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19948/310345075.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_feature_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "\n",
    "all_feature_dict = {key:[] for key in dataset[0][0].keys()}\n",
    "for data_idx in range(len(dataset)):\n",
    "    for feature_key,feature_val in dataset[data_idx][0].items():\n",
    "        all_feature_dict[feature_key].append(feature_val)\n",
    "        \n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49a121-34ca-498f-9267-4c20a04e2c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.children():\n",
    "    print()\n",
    "    print()\n",
    "    print(layer)\n",
    "    print(\"-------\")\n",
    "    for l in layer.children():\n",
    "        print([x for x in l.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1f24f-c586-4e1b-aad6-fb4671c87232",
   "metadata": {},
   "source": [
    "#### analyze the initial weights (or change them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "931890cf-e329-4a15-b0d0-352dc27f985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @torch.no_grad()\n",
    "# def init_weights(m):\n",
    "# #     print(m)\n",
    "#     if type(m) == nn.Linear:\n",
    "# #         m.weight.fill_(1.0)\n",
    "#         torch.nn.init.xavier_uniform_(m.weight,gain=10)\n",
    "#         m.bias.data.uniform_(-1,1)\n",
    "# #     elif type(m) == nn.ReLU:\n",
    "# #         print(m.data)\n",
    "#     else:\n",
    "#         print(type(m))\n",
    "# #         print(m.weight)\n",
    "# model.apply(init_weights)\n",
    "# # for param in model.parameters():\n",
    "# # #     print(param)\n",
    "# #       print(param.data.size(), param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c79a7d-9e08-4433-9c77-1af049f349d6",
   "metadata": {},
   "source": [
    "### LEarning rate: our base line is 0.34 loss as that's what the optiver guys have when they use current 10 min realize vol and use it as target (copy to prediction). We create simplest neural network and work with learning rates to figure out what's best and when we see something in range of 0.35 then we've found good Learning rate\n",
    "- #### SGD: 1e-7 works best\n",
    "- #### ADAM: 1e-5, (NOTE: 1e-3 makes it behave dumb where some deep local minima gets stuck and produces constant output!)\n",
    "- TODO: analyze that constant output phenomenon more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5cb3871f-5215-4afb-92fc-47ae2d385fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-4\n",
    "# batch_size = 4096\n",
    "# epochs = 100\n",
    "\n",
    "# input_scaling = 1\n",
    "# output_scaling = 1\n",
    "\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "# strategyname = \"ret1_n_ret2\"\n",
    "# summary_writer = SummaryWriter(f'../output/training_tensorboard/{strategyname}_scaleIn{input_scaling}Out{output_scaling}_{learning_rate}_{batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9aadc54b-6009-4af6-bfb6-f6619c6acd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a7ed1-80f8-4a57-90b4-223345dde76b",
   "metadata": {},
   "source": [
    "### Learnings about training\n",
    "- (non scaling)logreturns input and volatility output; non scaled makes the model predict constant output with no variety(close to 0 std dev)\n",
    "- scaling input rids of variety issue, \n",
    "- scaling output makes the model start with low rmse initially so there's less ground to cover and we can iterate over ideas rapidly due to less epochs needed to achieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cef34a31-7317-47c0-a94f-9bebe2fd2e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n",
      "------- set mode : hybrid -----------\n",
      "------- set mode : hybrid -----------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "---------- ultimate_hybridEXP14_CNN_5s_StkFalse_0.001_64 hybrid ----------\n",
      "train: 0.44169340432802245 [17088/343145]\n",
      "train: 0.289942085353741 [34240/343145]\n",
      "train: 0.2713211289426284 [51392/343145]\n",
      "train: 0.27778813026067034 [68544/343145]\n",
      "train: 0.2719646344211564 [85696/343145]\n",
      "train: 0.2758025379656856 [102848/343145]\n",
      "train: 0.2587891684761688 [120000/343145]\n",
      "train: 0.2599740578278677 [137152/343145]\n",
      "train: 0.25592305222109185 [154304/343145]\n",
      "train: 0.2493434187525244 [171456/343145]\n",
      "train: 0.2518371048925528 [188608/343145]\n",
      "train: 0.25216196872182745 [205760/343145]\n",
      "train: 0.2493102168525333 [222912/343145]\n",
      "train: 0.25088072818384244 [240064/343145]\n",
      "train: 0.24891239237874302 [257216/343145]\n",
      "train: 0.24789061508516766 [274368/343145]\n",
      "train: 0.24864679402602252 [291520/343145]\n",
      "train: 0.25151087688421137 [308672/343145]\n",
      "train: 0.24996580711718816 [325824/343145]\n",
      "train: 0.253161470058249 [342976/343145]\n",
      "train: 0.34214217960834503 test: 0.24336283600081446 [343104/343145]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "---------- ultimate_hybridEXP14_CNN_5s_StkFalse_0.001_64 hybrid ----------\n",
      "train: 0.24730479085755613 [17024/343145]\n",
      "train: 0.24811142705269715 [34176/343145]\n",
      "train: 0.2471546166534744 [51328/343145]\n",
      "train: 0.244528886153182 [68480/343145]\n",
      "train: 0.25159323326687316 [85632/343145]\n",
      "train: 0.24984744452496074 [102784/343145]\n",
      "train: 0.2472096733304102 [119936/343145]\n",
      "train: 0.2485214868254626 [137088/343145]\n",
      "train: 0.2405056308454542 [154240/343145]\n",
      "train: 0.2475076479031079 [171392/343145]\n",
      "train: 0.2506682714308376 [188544/343145]\n",
      "train: 0.2711159593578595 [205696/343145]\n",
      "train: 0.24257625800682536 [222848/343145]\n",
      "train: 0.24207504560698323 [240000/343145]\n",
      "train: 0.2462840610698088 [257152/343145]\n",
      "train: 0.24588917534965188 [274304/343145]\n",
      "train: 0.24298515143011934 [291456/343145]\n",
      "train: 0.24388518134382234 [308608/343145]\n",
      "train: 0.23849579091392345 [325760/343145]\n",
      "train: 0.24052644293032474 [342912/343145]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19948/1972392830.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0moutput_scaling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m                     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFeature_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn_orig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                     \u001b[0mlosses_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19948/2521588724.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, feature_dict)\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_gen_model\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_gen_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;31m#                 h0_tensor = torch.zeros(self.rnn_layers, -1, self.rnn_hidden_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[0mfeatures_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_gen_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m                 \u001b[0mgenerated_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19948/2521588724.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, feature_dict, h0_tensor)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mh_0_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh0_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0moutput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhn_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_0_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#lstm with input, hidden, and internal state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m             \u001b[0mhn_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhn_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size_\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeated_lstm_cells_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#reshaping the data for Dense layer next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    836\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 838\u001b[1;33m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    839\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model = None\n",
    "strategyname = \"ultimate_hybridEXP14_CNN_5s\"\n",
    "\n",
    "\n",
    "print(\"DEVICE:\", device)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "training_configs = []\n",
    "learning_rates_to_try = [1e-3]# 1e-4]\n",
    "batch_sizes_to_try = [64]#, 512]#,10000, 128]\n",
    "# input_scalings_to_try = [1000]\n",
    "# output_scalings_to_try = [10000]\n",
    "output_scaling = 10000\n",
    "for learning_rate in learning_rates_to_try:\n",
    "    for batch_size in batch_sizes_to_try:\n",
    "        for use_stock_id in [False]:\n",
    "            training_configs.append({\n",
    "                'learning_rate':learning_rate,\n",
    "                'batch_size':batch_size,\n",
    "                'use_stock_id': use_stock_id\n",
    "            })\n",
    "\n",
    "epochs = 200\n",
    "for training_config in training_configs:\n",
    "    \n",
    "    learning_rate = training_config['learning_rate']\n",
    "    batch_size = training_config['batch_size']\n",
    "    use_stock_id = training_config['use_stock_id']\n",
    "    # TRAINING SETUP\n",
    "    \n",
    "    #refresh the model\n",
    "    \n",
    "    STRATEGY_NAME_WITH_ATTRS = f\"{strategyname}_Stk{use_stock_id}_{learning_rate}_{batch_size}\"\n",
    "    summary_writer = SummaryWriter(f'../output/training_tensorboard/{STRATEGY_NAME_WITH_ATTRS}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    model = VolatilityBSModel()\n",
    "#     model = NeuralNetwork()\n",
    "#     model = StockIdEmbedding(number_of_stock_embedding_dimention=3)\n",
    "#     model\n",
    "    model.to(device)\n",
    "    optimizer_for_modes = {}\n",
    "    \n",
    "    feature_modes = ['logrett_xs','trade_volume_xs','trade_ordercount_xs','trade_money_turnover_per_order_xs',\n",
    "                             'logret1_xs',\n",
    "                             'book_price_spread1_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "                             'book_total_volume_xs','book_volume_imbalance_xs']\n",
    "#     feature_modes = model.get_feature_gen_train_modes()\n",
    "    for modeidx, mode in enumerate(['hybrid']):#+model.get_feature_gen_train_modes()+['hybrid']):#'train_stock_id_embedding','train']):#+['hidden_generator']*2 + feature_modes + ['hybrid']*2):\n",
    "        epochs = 6\n",
    "        model.set_mode(mode)\n",
    "#         print(model.parameters())\n",
    "#         input()\n",
    "#         continue\n",
    "        \n",
    "        if str(mode) not in optimizer_for_modes:\n",
    "            optimizer_for_modes[str(mode)] = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "#             optimizer_for_modes[mode] = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        optimizer = optimizer_for_modes[str(mode)]\n",
    "\n",
    "        \n",
    "        # TRAINING SETUP DONE\n",
    "\n",
    "        \n",
    "\n",
    "        data_ohlc_sample_len = 1 # 1 for each of open high low close\n",
    "        losses_train = []\n",
    "        for t in range(epochs):\n",
    "            t = modeidx*epochs + t\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            print(\"----------\", STRATEGY_NAME_WITH_ATTRS, mode,\"----------\")\n",
    "\n",
    "            dataloader_train = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                shuffle=True, num_workers=0, pin_memory=True)\n",
    "#             model.train()\n",
    "\n",
    "            for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "                \n",
    "                y = feature_y.to(device) * output_scaling \n",
    "\n",
    "                pred = model(Feature_X)\n",
    "#                 pred.to(device)\n",
    "#                 print(pred)\n",
    "#                 input()\n",
    "                loss_orig = loss_fn_orig(y, pred)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss_orig.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "                losses_train.append(loss_orig.item())\n",
    "\n",
    "                if (t*int(train_size/batch_size) + train_batch_idx + 1) % int(train_size/20/batch_size) == 0:\n",
    "\n",
    "                    # NOTE: real loss is same as upscaled normalized loss as it's percentage loss (rmspe)\n",
    "                    prediction_variety = np.std((pred/output_scaling).reshape(-1).tolist()) * 100\n",
    "                    #NOTE: prediction variety is important as model sometimes predits a constant value! regardless of the input, then per batch variety is lowest(0 std dev)\n",
    "\n",
    "\n",
    "                    summary_writer.add_scalar(\"Prediction Variety\", prediction_variety, t*(train_size) + (train_batch_idx*batch_size))\n",
    "                    summary_writer.add_scalar(\"Training Loss\", np.mean(losses_train), t*(train_size) + (train_batch_idx*batch_size))\n",
    "\n",
    "                    print(\"train:\", np.mean(losses_train), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\")\n",
    "                    losses_train = []\n",
    "\n",
    "            dataloader_test = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                    shuffle=True, num_workers=0, pin_memory=True)\n",
    "            dataset_size = len(dataloader_test.dataset)\n",
    "            \n",
    "#             model.eval()\n",
    "\n",
    "            losses_test = []\n",
    "            for _, (Feature_X, feature_y) in enumerate(dataloader_test):\n",
    "                with torch.no_grad():\n",
    "                    y = feature_y.to(device) * output_scaling\n",
    "                    pred = model(Feature_X)\n",
    "                    loss = loss_fn_orig(y, pred)\n",
    "                    losses_test.append(loss.item())\n",
    "\n",
    "\n",
    "    #                 summary_writer.add_scalar(\"Epoch Training Loss\", np.mean(losses_train), (t+1)*train_size)\n",
    "            summary_writer.add_scalar(\"Test Loss\", np.mean(losses_test), t*(train_size) + (train_batch_idx*batch_size))\n",
    "            print(\"train:\", np.mean(losses_train), \"test:\", np.mean(losses_test), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\")\n",
    "            losses_test = []\n",
    "            if (t+1)%50==0:\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{loss:.4f}.pth\"))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "28b838c3-3aea-416a-8b20-cf2eacddb2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_predictor = model.hidden_generator_network.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':torch.tensor([i for i in range(0, 112)])})\n",
    "datapoints = pred.tolist()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "99ec365f-201b-44f7-826c-2017b8bbc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"13_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))\n",
    "model_statedict = {'base':model.state_dict()}\n",
    "for k,v in model.feature_gen_models.items():\n",
    "    model_statedict[k] = v.state_dict()\n",
    "\n",
    "torch.save(model_statedict, os.path.join(MODEL_OUTPUT_DIRECTORY,f\"16_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "50cf1a35-5d02-43e8-b242-66d082ac237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0a1f5760-8926-4064-8bf2-bddd62c80bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93720"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74227704-6e5f-41fd-b9a2-242d0b8e9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optiver_custom_collate_func(batch):\n",
    "    output_x = {}\n",
    "    for k,v in batch[0][0].items():\n",
    "        output_x[k] = []\n",
    "    \n",
    "    for x_dict in [x[0] for x in batch]:\n",
    "        for k,v in x_dict.items():\n",
    "            output_x[k].append(v)\n",
    "    \n",
    "    for k,v in batch[0][0].items():\n",
    "        if type(output_x[k][0]) != str:\n",
    "            output_x[k] = torch.stack(output_x[k])\n",
    "        \n",
    "    output_y = []\n",
    "    for y in [x[1] for x in batch]:\n",
    "        output_y.append(y)\n",
    "    output_y = torch.stack(output_y)\n",
    "    \n",
    "    return (output_x, output_y)\n",
    "#     input()\n",
    "#     print(batch)\n",
    "# #     return batch\n",
    "#     input()\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "17cea68b-9cf2-4a6c-8fab-d0e6b8f7654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id\n",
      "['96-13771']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_id\n",
      "tensor([96.])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds_in_bucket_xs\n",
      "tensor([[  5.,  10.,  15.,  20.,  25.,  30.,  35.,  40.,  45.,  50.,  55.,  60.,\n",
      "          65.,  70.,  75.,  80.,  85.,  90.,  95., 100., 105., 110., 115., 120.,\n",
      "         125., 130., 135., 140., 145., 150., 155., 160., 165., 170., 175., 180.,\n",
      "         185., 190., 195., 200., 205., 210., 215., 220., 225., 230., 235., 240.,\n",
      "         245., 250., 255., 260., 265., 270., 275., 280., 285., 290., 295., 300.,\n",
      "         305., 310., 315., 320., 325., 330., 335., 340., 345., 350., 355., 360.,\n",
      "         365., 370., 375., 380., 385., 390., 395., 400., 405., 410., 415., 420.,\n",
      "         425., 430., 435., 440., 445., 450., 455., 460., 465., 470., 475., 480.,\n",
      "         485., 490., 495., 500., 505., 510., 515., 520., 525., 530., 535., 540.,\n",
      "         545., 550., 555., 560., 565., 570., 575., 580., 585., 590., 595., 600.]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logrett_xs\n",
      "tensor([[-0.2583, -0.2583, -0.2583, -0.2583, -0.2565, -0.2560, -0.2566, -1.2919,\n",
      "         -0.0621, -0.0615, -0.0621, -0.0621, -0.2757, -0.2751,  2.5545, -0.3759,\n",
      "          1.5375, -1.2732, -1.4517, -0.6660, -0.6666, -0.6661, -0.0943, -0.8810,\n",
      "         -0.8745, -3.8643, -2.0211, -0.0137, -1.3876, -0.1028, -0.2772, -0.2766,\n",
      "         -2.2825, -0.0269,  1.6510, -1.3265,  2.1354,  2.1350,  0.5107,  0.5112,\n",
      "         -0.6612, -0.1995, -0.1995, -2.8131, -2.8139, -1.4733, -1.5446,  0.1518,\n",
      "          1.3928, -0.8829, -0.2122, -0.2122, -0.4472, -0.4466, -0.4472,  1.6626,\n",
      "          1.2654,  1.2647,  1.1127,  1.5446,  1.5443,  2.4143,  0.7716,  0.7715,\n",
      "         -0.4849, -0.8379, -1.7267, -0.9193, -0.5514, -0.5508, -0.2246, -0.2252,\n",
      "         -0.2252, -0.2247,  0.4583,  0.6620,  1.0186,  1.1863, -0.5597,  0.2849,\n",
      "          0.2855,  0.2849, -0.0747, -0.8219, -0.8220, -0.0287, -0.0287, -0.0287,\n",
      "          2.1707, -0.2222, -0.2222, -0.2222, -0.7705, -0.7706, -0.7707, -0.7707,\n",
      "          0.2205,  0.6614,  0.6619,  0.8823, -0.8029, -0.8023, -0.8030, -2.0030,\n",
      "         -0.0884,  0.2498,  0.2504,  0.2498, -1.9602, -2.1202, -5.4583, -0.6376,\n",
      "         -0.6383,  0.7615,  0.7620,  0.0221, -0.6903, -0.6897, -0.6903, -0.6904]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_volume_xs\n",
      "tensor([[5.3033, 0.0000, 0.0000, 5.3375, 0.0000, 0.0000, 5.3230, 5.2095, 0.0000,\n",
      "         0.0000, 0.0000, 2.1972, 0.0000, 5.1417, 4.7875, 6.5191, 4.1431, 4.8203,\n",
      "         4.6634, 0.0000, 0.0000, 6.4630, 3.0445, 2.8332, 6.4677, 4.8363, 4.5951,\n",
      "         6.3648, 4.7958, 5.5175, 0.0000, 5.3566, 6.4568, 6.1203, 5.9081, 3.2581,\n",
      "         0.0000, 6.5568, 0.0000, 0.6931, 2.1972, 0.0000, 4.6634, 0.0000, 5.7268,\n",
      "         4.3175, 1.3863, 4.9836, 0.6931, 0.6931, 0.0000, 6.2916, 0.0000, 0.0000,\n",
      "         4.6151, 6.0113, 0.0000, 0.6931, 5.3033, 0.0000, 6.1485, 4.6250, 0.0000,\n",
      "         4.6151, 6.2226, 5.3230, 2.9444, 4.6151, 0.0000, 3.0445, 0.0000, 0.0000,\n",
      "         0.0000, 5.7746, 3.6376, 0.6931, 6.0331, 1.6094, 5.2883, 0.0000, 0.0000,\n",
      "         5.7170, 4.7274, 0.0000, 4.9558, 0.0000, 0.0000, 4.9345, 4.6250, 0.0000,\n",
      "         0.0000, 5.3327, 0.0000, 0.0000, 0.0000, 1.0986, 0.6931, 0.0000, 1.0986,\n",
      "         0.6931, 0.0000, 0.0000, 5.6312, 4.6250, 5.7557, 0.0000, 0.0000, 1.0986,\n",
      "         5.7203, 4.7274, 6.2710, 0.0000, 0.6931, 0.0000, 4.9488, 4.8283, 0.0000,\n",
      "         0.0000, 0.0000, 4.6250]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_ordercount_xs\n",
      "tensor([[1.9459, 0.0000, 0.0000, 2.1972, 0.0000, 0.0000, 1.7918, 2.0794, 0.0000,\n",
      "         0.0000, 0.0000, 1.9459, 0.0000, 1.0986, 2.0794, 2.7081, 1.3863, 1.3863,\n",
      "         1.3863, 0.0000, 0.0000, 2.1972, 0.6931, 1.0986, 2.0794, 1.0986, 1.3863,\n",
      "         2.6391, 1.6094, 2.0794, 0.0000, 2.0794, 2.7081, 2.5649, 2.3979, 0.6931,\n",
      "         0.0000, 2.6391, 0.0000, 0.6931, 1.3863, 0.0000, 1.0986, 0.0000, 2.0794,\n",
      "         1.3863, 0.6931, 1.3863, 0.6931, 0.6931, 0.0000, 2.4849, 0.0000, 0.0000,\n",
      "         1.0986, 2.5649, 0.0000, 0.6931, 1.3863, 0.0000, 2.9444, 1.0986, 0.0000,\n",
      "         1.0986, 2.3979, 1.6094, 1.6094, 1.3863, 0.0000, 0.6931, 0.0000, 0.0000,\n",
      "         0.0000, 2.3979, 1.0986, 0.6931, 2.7081, 0.6931, 1.9459, 0.0000, 0.0000,\n",
      "         2.0794, 1.3863, 0.0000, 1.3863, 0.0000, 0.0000, 2.4849, 1.3863, 0.0000,\n",
      "         0.0000, 1.7918, 0.0000, 0.0000, 0.0000, 1.0986, 0.6931, 0.0000, 1.0986,\n",
      "         0.6931, 0.0000, 0.0000, 1.7918, 1.3863, 2.6391, 0.0000, 0.0000, 1.0986,\n",
      "         2.6391, 1.9459, 2.4849, 0.0000, 0.6931, 0.0000, 2.6391, 1.7918, 0.0000,\n",
      "         0.0000, 0.0000, 1.0986]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_money_turnover_xs\n",
      "tensor([[5.3024, 0.0000, 0.0000, 5.3366, 0.0000, 0.0000, 5.3220, 5.2083, 0.0000,\n",
      "         0.0000, 0.0000, 2.1962, 0.0000, 5.1404, 4.7865, 6.5181, 4.1423, 4.8193,\n",
      "         4.6623, 0.0000, 0.0000, 6.4617, 3.0432, 2.8319, 6.4662, 4.8344, 4.5930,\n",
      "         6.3626, 4.7936, 5.5152, 0.0000, 5.3543, 6.4542, 6.1178, 5.9057, 3.2557,\n",
      "         0.0000, 6.5547, 0.0000, 0.6922, 2.1954, 0.0000, 4.6614, 0.0000, 5.7242,\n",
      "         4.3147, 1.3841, 4.9807, 0.6917, 0.6917, 0.0000, 6.2886, 0.0000, 0.0000,\n",
      "         4.6121, 6.0084, 0.0000, 0.6918, 5.3008, 0.0000, 6.1462, 4.6230, 0.0000,\n",
      "         4.6133, 6.2206, 5.3210, 2.9424, 4.6129, 0.0000, 3.0423, 0.0000, 0.0000,\n",
      "         0.0000, 5.7721, 3.6353, 0.6920, 6.0309, 1.6078, 5.2861, 0.0000, 0.0000,\n",
      "         5.7150, 4.7253, 0.0000, 4.9536, 0.0000, 0.0000, 4.9322, 4.6229, 0.0000,\n",
      "         0.0000, 5.3307, 0.0000, 0.0000, 0.0000, 1.0970, 0.6920, 0.0000, 1.0971,\n",
      "         0.6921, 0.0000, 0.0000, 5.6288, 4.6224, 5.7532, 0.0000, 0.0000, 1.0969,\n",
      "         5.7175, 4.7245, 6.2677, 0.0000, 0.6913, 0.0000, 4.9453, 4.8249, 0.0000,\n",
      "         0.0000, 0.0000, 4.6213]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_money_turnover_per_order_xs\n",
      "tensor([[3.5353, 0.0000, 0.0000, 3.2903, 0.0000, 0.0000, 3.7319, 3.7785, 0.0000,\n",
      "         0.0000, 0.0000, 1.2229, 0.0000, 4.4531, 2.8894, 4.6450, 3.0749, 3.7367,\n",
      "         3.9878, 0.0000, 0.0000, 5.2162, 3.0432, 2.1959, 4.5295, 4.8344, 4.5930,\n",
      "         4.7957, 3.9296, 4.8843, 0.0000, 3.4363, 4.5166, 4.8029, 4.8006, 3.2557,\n",
      "         0.0000, 5.0585, 0.0000, 0.6922, 1.2978, 0.0000, 3.9776, 0.0000, 3.7977,\n",
      "         3.2425, 1.3841, 3.8957, 0.6917, 0.6917, 0.0000, 3.9091, 0.0000, 0.0000,\n",
      "         3.9288, 3.5501, 0.0000, 0.6918, 5.0114, 0.0000, 3.2916, 3.9396, 0.0000,\n",
      "         3.9300, 4.0547, 4.9082, 1.7030, 3.5339, 0.0000, 3.0423, 0.0000, 0.0000,\n",
      "         0.0000, 4.3371, 2.9681, 0.6920, 4.2836, 1.6078, 3.5194, 0.0000, 0.0000,\n",
      "         3.7886, 3.6443, 0.0000, 3.8690, 0.0000, 0.0000, 3.0954, 3.9492, 0.0000,\n",
      "         0.0000, 4.0278, 0.0000, 0.0000, 0.0000, 0.6919, 0.6920, 0.0000, 0.6920,\n",
      "         0.6921, 0.0000, 0.0000, 4.0336, 3.5432, 4.0198, 0.0000, 0.0000, 0.6919,\n",
      "         3.4135, 3.4472, 4.2308, 0.0000, 0.6913, 0.0000, 4.6442, 3.2470, 0.0000,\n",
      "         0.0000, 0.0000, 4.6213]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logret1_xs\n",
      "tensor([[-7.7680e-01, -7.7680e-01, -6.8623e-02,  6.7328e-01, -4.8532e-02,\n",
      "          3.7172e-01,  5.6857e-02, -2.5074e-01, -9.9375e-01,  1.7405e-01,\n",
      "          6.8433e-02,  7.0038e-01,  1.2492e-01, -1.3495e+00,  1.0666e+00,\n",
      "          5.1070e-01, -4.1751e-02, -6.4015e-03, -6.7136e-01, -2.6296e-02,\n",
      "          3.9817e-02, -9.8889e-01, -8.4128e-01, -1.1107e+00, -2.2903e+00,\n",
      "         -1.5481e+00, -1.5099e+00, -9.2775e-01, -7.4993e-01, -1.3067e+00,\n",
      "          1.8113e-01, -3.6812e-01, -2.8201e+00,  1.3158e-01,  1.6369e+00,\n",
      "          1.9151e-01,  3.7371e-01,  1.7664e+00,  1.1575e+00, -1.1125e-01,\n",
      "          7.9329e-02,  8.9109e-01, -3.4531e+00, -2.2108e+00, -1.5161e+00,\n",
      "         -5.4590e-01, -7.1532e-01,  1.3627e-01, -5.4038e-01,  9.0180e-01,\n",
      "         -7.0203e-01, -1.4106e+00, -1.5298e-01, -1.5299e-01, -1.0891e-03,\n",
      "          2.9099e+00, -2.2905e-01, -4.5718e-01,  2.1538e+00,  1.5167e-01,\n",
      "          4.0605e+00,  1.8936e-01,  5.3510e-01,  1.5238e+00,  4.5908e-01,\n",
      "         -1.1990e+00, -7.9428e-01, -1.3792e+00,  2.2514e-01, -9.8336e-01,\n",
      "          8.9667e-01, -5.0048e-01, -5.0050e-01, -9.5374e-01,  6.0001e-02,\n",
      "          2.2383e-01,  3.2025e-01,  1.1402e+00,  5.8908e-01, -1.5750e+00,\n",
      "         -3.1343e-02,  1.3332e+00,  5.6344e-01,  9.6391e-01,  3.1342e-01,\n",
      "         -3.0081e+00, -5.3374e-01, -6.4902e-01,  1.7861e+00, -1.9742e-01,\n",
      "          1.2865e+00, -3.0998e-01, -1.3622e+00,  5.5502e-01,  0.0000e+00,\n",
      "         -2.9586e+00,  1.2638e+00,  7.6171e-01,  6.1231e-01,  9.0020e-01,\n",
      "         -1.0147e-03, -1.3182e+00, -1.3183e+00, -7.5227e-01, -1.5973e+00,\n",
      "         -8.1991e-01,  1.1986e+00,  1.4711e-01, -2.0264e+00, -1.3291e+00,\n",
      "         -5.3050e+00, -2.0951e+00,  5.3387e-01, -7.8693e-02,  5.3789e-01,\n",
      "          1.9783e+00, -2.8291e+00,  1.2949e+00, -1.7899e-01, -4.8892e-02]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logret2_xs\n",
      "tensor([[ 0.9859,  0.9859, -0.1455, -1.2756,  1.5009, -0.0909, -0.0406, -0.5837,\n",
      "         -0.5796, -0.9822,  0.3893,  0.4608, -0.0628, -0.1517,  0.0359,  0.9919,\n",
      "         -0.5060,  0.0652, -1.9585, -0.5432,  0.4822, -0.3305, -0.2238,  0.0935,\n",
      "         -2.9143, -2.2891, -1.2099, -1.0210, -2.9866,  0.9696,  0.1230, -1.1505,\n",
      "         -1.7135,  0.2416,  1.8676, -0.2321,  0.1684,  1.8445, -0.0817, -1.3762,\n",
      "          1.8866,  1.9385, -2.1587, -1.1067, -4.4063, -0.5619, -0.2444, -0.4791,\n",
      "         -0.6453, -0.6869,  1.8901, -2.1611, -0.0178, -0.0178, -0.3498,  0.6256,\n",
      "          0.5493,  2.6553,  1.5744, -0.6500,  2.3321,  2.1582,  1.2279,  0.2242,\n",
      "         -0.4004, -0.2631, -2.0794, -0.5214,  0.0000,  1.0671, -0.2156, -0.0635,\n",
      "         -0.0635, -3.0902,  1.6110,  0.0915, -0.6223,  1.2058, -0.1832,  0.1907,\n",
      "         -0.1133,  1.5597,  0.1667, -1.8668, -0.6013, -0.3198,  1.0539, -1.2939,\n",
      "          2.3039,  1.6961,  0.4793, -3.0270, -1.4520,  1.6252, -0.0483, -1.6339,\n",
      "          0.6728,  1.1467, -0.3846, -0.1781,  1.4351, -0.7536, -0.7537, -1.5527,\n",
      "         -2.6237,  0.2717, -0.5195,  0.2717,  0.9765, -3.4961, -3.2493, -3.2187,\n",
      "         -0.4303,  0.1247,  1.2807,  0.0454, -2.5902,  1.0193,  0.6861, -0.4485]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_directional_volume1_xs\n",
      "tensor([[   4.2000,   70.0000,  106.3333,   34.7500,  245.0000,  212.0000,\n",
      "          303.2000,  -21.7500,  -24.3333,  -51.5000,  -70.2000,  -80.0000,\n",
      "         -127.0000,   -1.0000,   12.2500,   61.5000,   82.0000,   57.6667,\n",
      "         -217.0000, -191.0000, -199.0000, -127.8000, -184.5000, -257.2500,\n",
      "          -76.4000, -102.6000,  -63.4000,  -81.2500, -136.7500,   20.0000,\n",
      "          -77.7500, -219.5000,  -30.7500,   14.0000,   -4.5000,  -96.0000,\n",
      "           -4.0000,   64.7500,  -74.0000,  -94.6000,  -91.0000,  -99.0000,\n",
      "          -40.0000,   97.0000,   81.7500, -100.0000, -100.6667, -118.3333,\n",
      "          -33.0000,  -98.2500,  -50.0000, -121.7500,    0.0000,  -99.0000,\n",
      "          -99.0000,  -98.6000, -152.5000,    0.0000,   35.6000,  148.5000,\n",
      "          -74.5000,   76.7500,  157.0000, -159.2000, -155.4000,  -12.7500,\n",
      "          -58.2000,  -11.0000, -111.0000,  -21.6000, -104.3333,    0.0000,\n",
      "          -28.0000,  -46.2500,  -99.0000,  -98.5000,   26.7500,   -1.5000,\n",
      "          -19.2500,   88.7500,   80.5000,  111.4000,   -8.2500,  -66.0000,\n",
      "          -85.5000,  -19.6000,   -2.5000,   75.0000,  143.5000,  176.0000,\n",
      "          100.0000,  -32.2000,    2.0000,    7.0000,    7.0000,   10.0000,\n",
      "          -12.5000,  -23.0000,  -98.5000, -188.0000,  -99.0000,    0.0000,\n",
      "          -80.0000,  -24.7500,   -3.0000,    0.8000,    1.0000,    0.6667,\n",
      "           21.0000,  -49.5000,  -33.0000,  -44.0000, -137.4000, -115.0000,\n",
      "          -28.7500,  -14.6667,   11.3333,  -23.3333,  -93.2500, -123.7500]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_directional_volume2_xs\n",
      "tensor([[  17.8000,  -99.0000,  -99.0000,   26.2500,  -41.7500,    0.0000,\n",
      "           34.0000,    5.7500,  -95.6667,  -46.5000,  -89.8000,  -77.0000,\n",
      "          -44.6000,  -24.0000,  -44.0000,   51.5000,   33.0000,  -10.6667,\n",
      "           18.0000,   55.0000,   16.0000,   -8.0000, -165.7500, -162.7500,\n",
      "          -50.6000,  -72.4000, -159.6000,  -76.2500,   56.2500,   -7.5000,\n",
      "          -50.0000,   68.0000,  -71.2500,  -93.5000, -121.2500,  -68.4000,\n",
      "          -77.0000,   23.5000,  -19.5000,   78.2000,   15.6000,  -78.0000,\n",
      "         -126.6667, -101.0000,   50.5000,  -15.7500,  -48.3333,  -87.6667,\n",
      "           57.6667,  244.5000, -154.0000,  -52.7500,    0.0000,  -99.0000,\n",
      "          -59.0000,   61.6000,   38.0000, -100.0000,  -95.6000,   47.5000,\n",
      "           37.2500,  101.2500,  -39.0000,  -51.2000,    1.8000,   13.2500,\n",
      "           26.4000,    0.0000,    0.0000, -145.4000,  -91.3333,    0.0000,\n",
      "          -95.0000,   14.2500,  -60.0000,  -59.5000,  113.5000,  -10.2500,\n",
      "          105.7500,   -4.0000,   50.7500,   -2.4000,  -19.7500,  112.0000,\n",
      "          164.7500,   71.0000,   -0.5000,   55.2000,    3.2500,  -11.7500,\n",
      "          -20.0000,   78.8000,  178.0000,   -3.0000,   -2.0000,    0.0000,\n",
      "          -10.5000,  -22.0000,  -12.5000,  -13.5000, -157.0000,    0.0000,\n",
      "         -276.0000, -144.2500,   32.2500,    1.6000,   50.0000,    6.6667,\n",
      "          -96.4000,  -43.7500, -127.2500, -120.8000,  -86.8000,  -89.3333,\n",
      "          -79.7500,   15.3333,   35.3333,  -50.0000,  -13.7500,   -7.7500]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_price_spread1_xs\n",
      "tensor([[0.2467, 0.1835, 0.1762, 0.1817, 0.2643, 0.2423, 0.2291, 0.2313, 0.2644,\n",
      "         0.2313, 0.2115, 0.2159, 0.1939, 0.2644, 0.1982, 0.2863, 0.1982, 0.1909,\n",
      "         0.2129, 0.2203, 0.1982, 0.2379, 0.2754, 0.1598, 0.2645, 0.3086, 0.2337,\n",
      "         0.1764, 0.2701, 0.2371, 0.1433, 0.1709, 0.2592, 0.1930, 0.2812, 0.4852,\n",
      "         0.2315, 0.3583, 0.3032, 0.2161, 0.1852, 0.2095, 0.4852, 0.3639, 0.3033,\n",
      "         0.2869, 0.2648, 0.2059, 0.1839, 0.1986, 0.1986, 0.2428, 0.2042, 0.1655,\n",
      "         0.1765, 0.3839, 0.2923, 0.3089, 0.2912, 0.1985, 0.2426, 0.2150, 0.2425,\n",
      "         0.2160, 0.2028, 0.2976, 0.1940, 0.3308, 0.3308, 0.2823, 0.2646, 0.2536,\n",
      "         0.2426, 0.2812, 0.2647, 0.1765, 0.1599, 0.1102, 0.3583, 0.2646, 0.2316,\n",
      "         0.2249, 0.4355, 0.3528, 0.3307, 0.3573, 0.2095, 0.2250, 0.2316, 0.3087,\n",
      "         0.2205, 0.2602, 0.3307, 0.2867, 0.2867, 0.2206, 0.2206, 0.2867, 0.2867,\n",
      "         0.3308, 0.2425, 0.2536, 0.2647, 0.2702, 0.2702, 0.2603, 0.0662, 0.0882,\n",
      "         0.2162, 0.2152, 0.3423, 0.3048, 0.2739, 0.2576, 0.2484, 0.3165, 0.3239,\n",
      "         0.3092, 0.2485, 0.2264]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_price_spread2_xs\n",
      "tensor([[0.3083, 0.2569, 0.2423, 0.2863, 0.4240, 0.4185, 0.3788, 0.3689, 0.3672,\n",
      "         0.2864, 0.2732, 0.2908, 0.3172, 0.3084, 0.2808, 0.3359, 0.3480, 0.3230,\n",
      "         0.3451, 0.3304, 0.3084, 0.3305, 0.3690, 0.3250, 0.3482, 0.3880, 0.2954,\n",
      "         0.2977, 0.4190, 0.3749, 0.2977, 0.3088, 0.3584, 0.3253, 0.4631, 0.6175,\n",
      "         0.4742, 0.5071, 0.4631, 0.4145, 0.4101, 0.3748, 0.6028, 0.5072, 0.4964,\n",
      "         0.4523, 0.3751, 0.3163, 0.2795, 0.2538, 0.2647, 0.2979, 0.2704, 0.2428,\n",
      "         0.2560, 0.4457, 0.4192, 0.4853, 0.5559, 0.3639, 0.3473, 0.3693, 0.4079,\n",
      "         0.3439, 0.2998, 0.3858, 0.3704, 0.5292, 0.5292, 0.3837, 0.3528, 0.3418,\n",
      "         0.3308, 0.5128, 0.3419, 0.2647, 0.2812, 0.3252, 0.5568, 0.3749, 0.2977,\n",
      "         0.3043, 0.5181, 0.4410, 0.4576, 0.4367, 0.2702, 0.3484, 0.3969, 0.4355,\n",
      "         0.3748, 0.4587, 0.4852, 0.3970, 0.3970, 0.3970, 0.3529, 0.3970, 0.3970,\n",
      "         0.4631, 0.3749, 0.4301, 0.4853, 0.4026, 0.3971, 0.3618, 0.2868, 0.2941,\n",
      "         0.4192, 0.3531, 0.5354, 0.3755, 0.3357, 0.3018, 0.3698, 0.5152, 0.5889,\n",
      "         0.4417, 0.3147, 0.2871]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_bid_spread_xs\n",
      "tensor([[0.3964, 0.4404, 0.4404, 0.3303, 0.2752, 0.4404, 0.4404, 0.5507, 0.4405,\n",
      "         0.3304, 0.3969, 0.5289, 0.7491, 0.2202, 0.3306, 0.2206, 1.0133, 1.1015,\n",
      "         0.8815, 0.8815, 0.8815, 0.2203, 0.5510, 0.7161, 0.2203, 0.5292, 0.2204,\n",
      "         0.2204, 1.1580, 0.9374, 0.4414, 0.7718, 0.2205, 0.2757, 1.0479, 0.6619,\n",
      "         1.9852, 0.3310, 0.9923, 1.1028, 0.7498, 0.5510, 0.5145, 0.9926, 1.1034,\n",
      "         0.9933, 0.8093, 0.5885, 0.4412, 0.3309, 0.4412, 0.2758, 0.4138, 0.5519,\n",
      "         0.5742, 0.3976, 0.6624, 0.2206, 1.6769, 0.6618, 0.6062, 0.8820, 0.9924,\n",
      "         0.6174, 0.5732, 0.6612, 1.2788, 0.2205, 0.2205, 0.3969, 0.6614, 0.6614,\n",
      "         0.6614, 1.0478, 0.3308, 0.5512, 0.6614, 1.0477, 0.6066, 0.2205, 0.2205,\n",
      "         0.5732, 0.6064, 0.6617, 0.7722, 0.4854, 0.2211, 0.6616, 0.8821, 0.6062,\n",
      "         0.8823, 0.9704, 0.8824, 0.8824, 0.8824, 0.2205, 0.2205, 0.2205, 0.4416,\n",
      "         0.6615, 0.6620, 0.7724, 0.8827, 0.7172, 0.8826, 0.2650, 1.5443, 1.5443,\n",
      "         0.4854, 0.9381, 0.6624, 0.2208, 0.2208, 0.2208, 0.7176, 1.6193, 0.9574,\n",
      "         1.1046, 0.4415, 0.3864]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_ask_spread_xs\n",
      "tensor([[0.2201, 0.2937, 0.2201, 0.7155, 1.3213, 1.3213, 1.0569, 0.8260, 0.5873,\n",
      "         0.2202, 0.2202, 0.2202, 0.4847, 0.2202, 0.4958, 0.2752, 0.4844, 0.2203,\n",
      "         0.4405, 0.2202, 0.2202, 0.7049, 0.3855, 0.9361, 0.6170, 0.2644, 0.3966,\n",
      "         0.9920, 0.3306, 0.4410, 1.1027, 0.6063, 0.7722, 1.0477, 0.7716, 0.6612,\n",
      "         0.4412, 1.1571, 0.6063, 0.8821, 1.4989, 1.1017, 0.6612, 0.4408, 0.8271,\n",
      "         0.6616, 0.2944, 0.5150, 0.5150, 0.2211, 0.2205, 0.2757, 0.2482, 0.2206,\n",
      "         0.2206, 0.2205, 0.6066, 1.5440, 0.9702, 0.9923, 0.4408, 0.6612, 0.6616,\n",
      "         0.6614, 0.3967, 0.2205, 0.4848, 1.7637, 1.7637, 0.6173, 0.2204, 0.2204,\n",
      "         0.2204, 1.2680, 0.4409, 0.3307, 0.5515, 1.1026, 1.3777, 0.8820, 0.4409,\n",
      "         0.2204, 0.2203, 0.2204, 0.4960, 0.3088, 0.3857, 0.5732, 0.7713, 0.6611,\n",
      "         0.6611, 1.0142, 0.6617, 0.2204, 0.2204, 1.5439, 1.1028, 0.8823, 0.6616,\n",
      "         0.6618, 0.6618, 0.9925, 1.3233, 0.6068, 0.3860, 0.7503, 0.6621, 0.5149,\n",
      "         1.5444, 0.4411, 1.2692, 0.4860, 0.3977, 0.2211, 0.4967, 0.3677, 1.6927,\n",
      "         0.2207, 0.2207, 0.2207]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_total_volume_xs\n",
      "tensor([[7.0309, 6.2653, 6.4536, 6.9670, 7.5380, 6.1377, 7.9356, 6.8617, 5.9738,\n",
      "         6.4877, 6.7581, 6.9295, 7.1538, 4.0775, 6.9866, 8.0020, 8.0449, 7.4685,\n",
      "         6.9315, 6.6958, 6.8459, 7.1808, 7.6401, 7.7558, 6.8200, 6.8896, 7.6658,\n",
      "         6.9575, 7.2086, 7.2800, 7.4512, 7.1982, 7.6113, 7.3402, 7.2034, 7.4343,\n",
      "         5.5568, 7.4372, 6.7811, 7.0379, 7.0527, 6.1026, 6.8512, 6.1841, 6.9866,\n",
      "         6.2577, 6.9660, 6.9949, 7.1025, 7.5486, 6.0426, 7.0148, 0.0000, 6.0039,\n",
      "         7.0992, 7.2485, 7.2086, 6.2166, 7.6192, 6.4877, 7.1808, 7.4691, 6.8035,\n",
      "         7.3746, 7.6540, 6.5028, 6.6412, 4.0943, 5.0752, 6.9508, 6.5073, 0.0000,\n",
      "         6.3333, 6.6053, 5.7900, 5.8021, 7.5262, 6.8669, 7.0344, 6.6067, 6.8565,\n",
      "         7.4110, 7.0344, 6.7811, 7.0493, 6.3491, 5.3423, 7.0951, 6.8669, 6.8835,\n",
      "         5.7961, 6.9037, 5.4337, 5.4765, 5.4806, 4.7958, 4.6151, 3.9512, 5.4510,\n",
      "         6.1985, 5.7071, 0.0000, 6.1026, 7.0758, 6.1985, 5.6937, 4.7095, 3.5553,\n",
      "         7.0510, 6.7238, 6.7499, 6.7923, 7.1884, 6.5876, 6.9670, 6.7105, 6.3404,\n",
      "         6.2672, 6.3784, 6.6107]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_volume_imbalance_xs\n",
      "tensor([[4.6653, 3.4012, 2.1203, 4.1271, 5.3193, 5.3613, 5.8236, 5.0783, 4.7958,\n",
      "         4.5951, 5.0814, 5.0626, 5.1510, 3.2581, 3.8011, 4.9090, 5.0999, 4.1795,\n",
      "         5.2983, 4.9200, 5.2149, 4.9185, 5.8615, 6.0426, 4.8520, 5.1705, 5.4116,\n",
      "         5.2730, 4.4886, 4.6492, 4.8579, 5.1090, 4.6347, 5.0073, 4.8422, 5.1084,\n",
      "         4.4067, 4.9541, 4.5747, 2.8565, 4.3360, 5.1818, 5.1259, 4.6052, 4.8922,\n",
      "         4.7600, 5.0106, 5.3327, 4.7391, 4.9921, 5.3230, 5.1676, 0.0000, 5.2933,\n",
      "         5.0739, 3.6376, 4.7493, 4.6151, 4.9459, 5.2832, 4.0818, 5.1874, 4.7791,\n",
      "         5.3538, 5.0408, 3.8607, 4.2370, 2.4849, 4.7185, 5.1240, 5.2815, 0.0000,\n",
      "         4.8203, 3.9120, 5.0752, 5.0689, 4.9505, 3.3759, 4.8363, 4.4514, 4.8847,\n",
      "         4.7005, 3.4177, 4.1217, 4.5191, 4.1141, 3.5115, 4.8828, 4.9955, 5.1075,\n",
      "         4.3944, 4.7808, 5.1985, 1.6094, 1.7918, 2.4849, 3.1781, 3.8286, 4.7185,\n",
      "         5.3107, 5.5491, 0.0000, 5.8777, 5.1417, 3.4095, 3.8330, 3.9512, 2.1203,\n",
      "         4.4864, 4.5512, 5.0830, 5.1108, 5.4170, 5.3246, 4.8714, 2.1972, 4.5609,\n",
      "         4.3086, 4.6821, 4.8866]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_money_turnover1_xs\n",
      "tensor([[2.1963, 1.9449, 1.9449, 4.6624, 5.5824, 4.6141, 5.3171, 2.3016, 1.3854,\n",
      "         4.3808, 1.7908, 1.7909, 1.7909, 1.0978, 3.3663, 5.7858, 4.6624, 4.6434,\n",
      "         4.1261, 4.1421, 1.3855, 2.0784, 5.3458, 5.7607, 4.7690, 1.9444, 4.6520,\n",
      "         1.7901, 5.4909, 6.0569, 1.6077, 1.7899, 6.0472, 5.3156, 5.1156, 5.2235,\n",
      "         1.0971, 5.5032, 4.6423, 3.3303, 3.8267, 1.0973, 4.6323, 3.2165, 5.1621,\n",
      "         1.6073, 5.3153, 1.3842, 1.3842, 1.7895, 1.0968, 4.6414, 0.0000, 1.0967,\n",
      "         1.7893, 4.7069, 5.3105, 4.6124, 5.4137, 4.6610, 5.9969, 4.7938, 5.3499,\n",
      "         1.7902, 3.2942, 4.9180, 1.7901, 0.6921, 0.6921, 2.3006, 2.7706, 0.0000,\n",
      "         4.8419, 1.6076, 1.0971, 1.3846, 5.9917, 5.9767, 5.1337, 4.5304, 4.5304,\n",
      "         5.6038, 6.0190, 4.6519, 1.6079, 1.9440, 3.5532, 1.9439, 4.8421, 4.5725,\n",
      "         4.6131, 2.3960, 0.6920, 4.5517, 4.5517, 1.3844, 1.0970, 1.0971, 1.3846,\n",
      "         1.0972, 0.6921, 0.0000, 3.0888, 5.4091, 4.2879, 1.7895, 1.0969, 1.3844,\n",
      "         4.7423, 1.6071, 3.9855, 2.6357, 2.6357, 3.2154, 4.6600, 5.5181, 3.8676,\n",
      "         1.3837, 3.3288, 1.6066]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0023]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id\n",
      "['9-633']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_id\n",
      "tensor([9.])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds_in_bucket_xs\n",
      "tensor([[  5.,  10.,  15.,  20.,  25.,  30.,  35.,  40.,  45.,  50.,  55.,  60.,\n",
      "          65.,  70.,  75.,  80.,  85.,  90.,  95., 100., 105., 110., 115., 120.,\n",
      "         125., 130., 135., 140., 145., 150., 155., 160., 165., 170., 175., 180.,\n",
      "         185., 190., 195., 200., 205., 210., 215., 220., 225., 230., 235., 240.,\n",
      "         245., 250., 255., 260., 265., 270., 275., 280., 285., 290., 295., 300.,\n",
      "         305., 310., 315., 320., 325., 330., 335., 340., 345., 350., 355., 360.,\n",
      "         365., 370., 375., 380., 385., 390., 395., 400., 405., 410., 415., 420.,\n",
      "         425., 430., 435., 440., 445., 450., 455., 460., 465., 470., 475., 480.,\n",
      "         485., 490., 495., 500., 505., 510., 515., 520., 525., 530., 535., 540.,\n",
      "         545., 550., 555., 560., 565., 570., 575., 580., 585., 590., 595., 600.]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15084/2440942144.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         )\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "\n",
    "    \n",
    "dataloader_train = DataLoader(dataset, batch_size=1,\n",
    "                                shuffle=True, num_workers=0, pin_memory=False)#, collate_fn=optiver_custom_collate_func)\n",
    "# #             model.train()\n",
    "# i = 0\n",
    "# stockid = set()\n",
    "\n",
    "def feature_transform(feature_x, feature_name):\n",
    "        if feature_name in ['logrett_xs',\n",
    "                             'logret1_xs','logret2_xs','book_bid_spread_xs','book_ask_spread_xs']:\n",
    "            return feature_x * 10000\n",
    "        if feature_name in ['book_price_spread1_xs','book_price_spread2_xs']:\n",
    "            return feature_x * 1000\n",
    "        if feature_name in ['trade_ordercount_xs','trade_volume_xs','trade_money_turnover_xs','trade_money_turnover_per_order_xs',\n",
    "                             'book_total_volume_xs','book_volume_imbalance_xs','book_money_turnover1_xs']:\n",
    "            return torch.log(feature_x + 1)\n",
    "        return feature_x\n",
    "    \n",
    "for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "    i += 1\n",
    "    for k,v in Feature_X.items():\n",
    "        print(k)\n",
    "        print(feature_transform(v,k))\n",
    "        input()\n",
    "    print(feature_y)\n",
    "    input()\n",
    "# batch = []\n",
    "# for idx in range(len(dataset)):\n",
    "#     batch.append(dataset[idx])\n",
    "#     if idx % 128 == 0:\n",
    "#         features_x = [x[0] for x in batch]\n",
    "#         features_y = [x[1] for x in batch]\n",
    "#         features_y = torch.tensor(features_y).reshape(-1,1)\n",
    "# #         print(features_y)\n",
    "# #         input()\n",
    "#         batch = []\n",
    "    \n",
    "#     y = feature_y.to(device) * output_scaling \n",
    "#     print(Feature_X['logret1_xs'].type())\n",
    "#     pred = model(Feature_X)\n",
    "#     print(pred.type())\n",
    "#     input()\n",
    "#     for stk in Feature_X['row_id']:\n",
    "        \n",
    "#         stockid.add(stk.split(\"-\")[0])\n",
    "# for i in range(len(dataset)-10):\n",
    "#     dataset[i]\n",
    "print(\"-->\", (time.time()-stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f03435-5cde-40bf-a86b-656ab4515ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce561c-5b64-44bf-882b-41f6250b1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated(device)/1024/1024/1024\n",
    "# model.to(\"cpu\")\n",
    "# torch.cuda.memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a966ee2c-8547-48be-a912-8d05bf48b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.init()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
