{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2f18e2-9894-4f0c-bd57-8b37afc02309",
   "metadata": {},
   "source": [
    "### Can our model predict current volatility?  (forget future; first it should be capable of predicting current one with given features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c4e895-fd66-490d-a8b8-d28b324d3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import types\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from optiver_features_handler import get_features_map_for_stock, get_row_id, realized_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750eb0e3-2860-490b-bc3c-2a512485a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\n",
    "OUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\n",
    "MODEL_OUTPUT_DIRECTORY = os.path.join(OUTPUT_DIRECTORY,\"models\")\n",
    "os.makedirs(OUTPUT_DIRECTORY,exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIRECTORY,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02947ab-aa1d-4af0-9d6e-7a51cff159ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interval_seconds = 6\n",
    "data_intervals_count = int(600/data_interval_seconds)\n",
    "class OptiverRealizedVolatilityDataset(Dataset):\n",
    "    def __init__(self, data_directory, mode=\"train\", lazy_load=True):\n",
    "        \"\"\"initializes Optiver Competition dataset\n",
    "        `mode`: train|test\n",
    "        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n",
    "        \"\"\"\n",
    "        print(\"INIT: OptiverRealizedVolatilityDataset\")\n",
    "        if mode.lower() not in ['train','test']:\n",
    "            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n",
    "        self.data_directory = data_directory\n",
    "        self.mode = mode.lower()\n",
    "        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n",
    "#         if self.mode == 'train':\n",
    "#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n",
    "        if self.mode == 'test':\n",
    "            self.main_df['target'] = 0\n",
    "        \n",
    "        self.cache_stocks_done_set = set()\n",
    "        # this is our final features lookup where we park all our features which can be addressed by row_id\n",
    "        # which is individual train/test.csv row id using 'stock_id`-`time_id`\n",
    "        self.cache_rowid_feature_map = {}\n",
    "        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n",
    "        targets = self.main_df['target'].tolist()\n",
    "        self.stock_possible_timeids_list = {}\n",
    "        for idx, row_id in enumerate(row_id_series.tolist()):\n",
    "            stock_id = int(row_id.split('-')[0])\n",
    "            time_id = int(row_id.split('-')[1])\n",
    "            self.cache_rowid_feature_map[row_id] = {'target_realized_volatility':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n",
    "            \n",
    "            # below code is to make sure what timeids we expect from stock data extractor\n",
    "            # in case of missing parquet files we'll have to know the keys to fill default values into\n",
    "            if stock_id not in self.stock_possible_timeids_list:\n",
    "                self.stock_possible_timeids_list[stock_id] = []\n",
    "            self.stock_possible_timeids_list[stock_id].append(time_id)\n",
    "            \n",
    "        \n",
    "        if lazy_load == False:\n",
    "            worker_data = []\n",
    "            for gkey, gdf in self.main_df.groupby(['stock_id']):\n",
    "                worker_data.append((self.data_directory, self.mode, gkey))\n",
    "#             print(\"---------- CPU COUNG:\", multiprocessing.cpu_count())\n",
    "            # NOTE: this was hell of a hunt; this windows and pytorch and jupyter combination is too tedious\n",
    "            #       make sure the function that we distribute don't call pytorch\n",
    "            chunksize = multiprocessing.cpu_count() * 1\n",
    "            processed = 0\n",
    "            for worker_data_chunk in [worker_data[i * chunksize:(i + 1) * chunksize] for i in range((len(worker_data) + chunksize - 1) // chunksize )]:\n",
    "                with Pool(multiprocessing.cpu_count()) as p:\n",
    "                    \n",
    "                    feature_set_list = p.starmap(get_features_map_for_stock, worker_data_chunk)\n",
    "                    \n",
    "                    for feature_map in feature_set_list:\n",
    "                        for rowid, features_dict in feature_map.items():\n",
    "                            for fkey,fval in features_dict.items():\n",
    "                                self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                            self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                        # udpate the indications that we've already fetched this stock and the lazy loader code won't fetch this again\n",
    "                        self.cache_stocks_done_set.add(int(rowid.split('-')[0]))\n",
    "                    \n",
    "                    processed += chunksize\n",
    "                    print(f\"Processed and loaded {processed} stocks features.\")\n",
    "    \n",
    "    def __cache_generate_features(self, main_stock_id, main_time_id):\n",
    "            \n",
    "            main_row_id = get_row_id(main_stock_id, main_time_id)\n",
    "            if main_stock_id not in self.cache_stocks_done_set:\n",
    "#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))   \n",
    "                # we'll combine the featureset with the bigger feature set of all stocks\n",
    "                feature_map = get_features_map_for_stock(self.data_directory, self.mode, main_stock_id)\n",
    "                # NOTE: sometime we might now have parquet files in that case we'll have 3 entried in .csv while only 1 gets returned in feature map\n",
    "                # we need to cover for that disparity\n",
    "                for time_id in self.stock_possible_timeids_list[main_stock_id]:\n",
    "                    expected_row_id = get_row_id(main_stock_id, time_id)\n",
    "                    if expected_row_id not in feature_map:\n",
    "                        feature_map[expected_row_id] = {}\n",
    "                for rowid, features_dict in feature_map.items():\n",
    "                    for fkey,fval in features_dict.items():\n",
    "                        self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                    self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                self.cache_stocks_done_set.add(main_stock_id)\n",
    "#             print(self.cache_rowid_feature_map[main_row_id])\n",
    "#             print(torch.tensor([self.cache_rowid_feature_map[main_row_id].get('book_realized_volatility',0)]))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map[main_row_id].get('log_return1_2s', [0]*(int(600/2)))))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map.get('book_directional_volume1_2s', [0]*(int(600/2)))))\n",
    "            return self.cache_rowid_feature_map[main_row_id]\n",
    "        \n",
    "    @staticmethod\n",
    "    def transform_to_01_realized_volatility_linear_data(features_dict):\n",
    "        feature_x  = {\n",
    "                    'row_id':features_dict['row_id'],\n",
    "                    'stock_id':torch.tensor(features_dict['stock_id'], dtype=torch.float32),\n",
    "                    'sequence_mask_xs': torch.tensor(features_dict.get('sequence_mask_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.bool),\n",
    "                    'seconds_in_bucket_xs': torch.tensor(features_dict.get('seconds_in_bucket_xs', [(idx) for idx in range(0,int(data_intervals_count))]), dtype=torch.float32),\n",
    "                    'has_trade_data_xs': torch.tensor(features_dict.get('has_trade_data_xs', [0]*(int(600/data_interval_seconds))), dtype=torch.float32),\n",
    "                }\n",
    "        overview_aggregations = {\n",
    "#         'wap1': ['sum', 'std'],\n",
    "#         'wap2': [np.sum, np.std],\n",
    "        'logret1': [realized_volatility],\n",
    "        'logret2': [realized_volatility],\n",
    "        'logret_price': [realized_volatility],\n",
    "#         'wap_balance': ['sum', 'max'],\n",
    "#         'price_spread1': ['sum', 'max'],\n",
    "#         'bid_spread': ['sum', 'max'],\n",
    "#         'ask_spread': ['sum', 'max'],\n",
    "        'total_volume': ['sum', 'max'],\n",
    "        'volume_imbalance': ['sum', 'max'],\n",
    "#         \"bid_ask_spread\": ['sum', 'max'],\n",
    "        'size':  ['sum', 'max','min'],\n",
    "        'order_count': ['sum', 'max'],\n",
    "        'trade_money_turnover': ['sum', 'max','min'],\n",
    "        }\n",
    "        \n",
    "        for key, aggs in overview_aggregations.items():\n",
    "            for agg in aggs:\n",
    "                if isinstance(agg, types.FunctionType):\n",
    "                    agg = agg.__name__\n",
    "                feature_x[f'{key}_{agg}'] = torch.tensor(features_dict.get(f'{key}_{agg}', -0.01), dtype=torch.float32)\n",
    "                \n",
    "#         for feature_name in ['wap1','wap_balance','logret1','logret2','logrett',\n",
    "#                              'price_spread1','bid_spread','ask_spread','total_volume','volume_imbalance',\n",
    "#                             'size','order_count','trade_money_turnover','trade_book_price_spread']:\n",
    "            \n",
    "#             feature_x[f'{feature_name}_sum_xs'] = torch.tensor(features_dict.get(f'{feature_name}_sum_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "#             feature_x[f'{feature_name}_max_xs'] = torch.tensor(features_dict.get(f'{feature_name}_max_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "        for feature_name in ['logret1','logret_bid_price1','logret_ask_price1','logret_bid_price2','logret_ask_price2',\n",
    "                        'price_spread1','bid_spread','ask_spread','logret_directional_volume1','logret_directional_volume2','logret_trade_money_turnover','trade_price_push_on_book','logret_price','order_count']: #'bid_price2','bid_size2','ask_price2','ask_size2',\n",
    "            feature_x[f'{feature_name}_xs'] = torch.tensor(features_dict.get(f'{feature_name}_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "#             feature_x[f'{feature_name}_v_xs'] = torch.tensor(features_dict.get(f'{feature_name}_v_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "#             np.concatenate([groupdf[feature_name].to_numpy(),[0]*(intervals_count-sequence_length)])\n",
    "        return (\n",
    "                feature_x,\n",
    "                {'target_realized_volatility':torch.tensor([features_dict['target_realized_volatility']])}\n",
    "#                 [features_dict['target']]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: handle for num_workers more than 0\n",
    "        #      using https://pytorch.org/docs/stable/data.html\n",
    "        #      using torch.util.data.get_worker_info()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        stock_id = self.main_df.at[idx, 'stock_id']\n",
    "        time_id = self.main_df.at[idx, 'time_id']\n",
    "        x,y = self.__cache_generate_features(stock_id,time_id)\n",
    "#         x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efcee691-c9ce-481f-a4e8-dbf97b66b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT: OptiverRealizedVolatilityDataset\n",
      "Processed and loaded 16 stocks features.\n",
      "Processed and loaded 32 stocks features.\n",
      "Processed and loaded 48 stocks features.\n",
      "Processed and loaded 64 stocks features.\n",
      "Processed and loaded 80 stocks features.\n",
      "Processed and loaded 96 stocks features.\n",
      "Processed and loaded 112 stocks features.\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\", lazy_load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d085c-aaed-4a28-9d44-93eb8e2d4e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in range(0,9):\n",
    "#     print(dataset[x])\n",
    "\n",
    "# dataset[10000] #[0]['bidp1_1s']\n",
    "for key,val in dataset[10000][0].items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eaeb87d1-d4be-4351-b483-22ac0c168909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_idx in range(len(dataset)):\n",
    "#     dataset[data_idx][0]['book_realized_volatility'] = dataset[data_idx][0]['book_realized_volatility'].type(torch.float32).to('cpu')\n",
    "#     print(dataset[data_idx][0]['book_realized_volatility'])\n",
    "#     input()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ec3227-ffc6-4099-b51c-04a3bd2df633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stock_id': {'mean': 62.43794250488281, 'std': 37.12644958496094},\n",
       " 'seconds_in_bucket_xs': {'mean': 46.53627014160156,\n",
       "  'std': 30.269084930419922},\n",
       " 'has_trade_data_xs': {'mean': 0.9432891011238098, 'std': 0.23128946125507355},\n",
       " 'logret1_realized_volatility': {'mean': 0.005850940477102995,\n",
       "  'std': 0.004778958857059479},\n",
       " 'logret2_realized_volatility': {'mean': 0.007210279814898968,\n",
       "  'std': 0.005797804333269596},\n",
       " 'logret_price_realized_volatility': {'mean': 0.004610072355717421,\n",
       "  'std': 0.004089121241122484},\n",
       " 'total_volume_sum': {'mean': 1629942.5, 'std': 9067553.0},\n",
       " 'total_volume_max': {'mean': 6443.4765625, 'std': 27511.509765625},\n",
       " 'volume_imbalance_sum': {'mean': 396575.46875, 'std': 2442117.5},\n",
       " 'volume_imbalance_max': {'mean': 3689.99755859375, 'std': 13807.1123046875},\n",
       " 'size_sum': {'mean': 31860.21484375, 'std': 70259.2109375},\n",
       " 'size_max': {'mean': 3035.258544921875, 'std': 8191.23974609375},\n",
       " 'size_min': {'mean': 6.389898777008057, 'std': 352.6045227050781},\n",
       " 'order_count_sum': {'mean': 373.43682861328125, 'std': 608.5436401367188},\n",
       " 'order_count_max': {'mean': 26.246469497680664, 'std': 50.72904968261719},\n",
       " 'trade_money_turnover_sum': {'mean': 31859.123046875, 'std': 70262.2265625},\n",
       " 'trade_money_turnover_max': {'mean': 3035.2578125, 'std': 8194.078125},\n",
       " 'trade_money_turnover_min': {'mean': 6.387502670288086,\n",
       "  'std': 352.42071533203125},\n",
       " 'logret1_xs': {'mean': -2.4693693756461244e-09, 'std': 0.0006871781661175191},\n",
       " 'logret_bid_price1_xs': {'mean': -1.9815225016373006e-09,\n",
       "  'std': 0.0006457031704485416},\n",
       " 'logret_ask_price1_xs': {'mean': -2.840470081366675e-09,\n",
       "  'std': 0.0006457548006437719},\n",
       " 'logret_bid_price2_xs': {'mean': -2.0001771350308672e-09,\n",
       "  'std': 0.0006615926395170391},\n",
       " 'logret_ask_price2_xs': {'mean': -2.7945690206365725e-09,\n",
       "  'std': 0.0006625199457630515},\n",
       " 'price_spread1_xs': {'mean': 0.0022263764403760433,\n",
       "  'std': 0.0026683351024985313},\n",
       " 'bid_spread_xs': {'mean': 0.0007556164055131376,\n",
       "  'std': 0.0009447085903957486},\n",
       " 'ask_spread_xs': {'mean': 0.0007635528454557061,\n",
       "  'std': 0.0009570387774147093},\n",
       " 'logret_directional_volume1_xs': {'mean': -1.8204282525857707e-09,\n",
       "  'std': 0.022533627226948738},\n",
       " 'logret_directional_volume2_xs': {'mean': 1.4707172146799508e-09,\n",
       "  'std': 0.02200956828892231},\n",
       " 'logret_trade_money_turnover_xs': {'mean': -2.9323007311177207e-07,\n",
       "  'std': 1.6745622158050537},\n",
       " 'trade_price_push_on_book_xs': {'mean': -1.7589563583442214e-07,\n",
       "  'std': 5.7160297728842124e-05},\n",
       " 'logret_price_xs': {'mean': -2.801246123951273e-09,\n",
       "  'std': 0.0006271468009799719},\n",
       " 'order_count_xs': {'mean': 3.734368324279785, 'std': 10.645439147949219}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feature_dict = {key:[] for key in dataset[0][0].keys()}\n",
    "for data_idx in range(len(dataset)):\n",
    "    for feature_key,feature_val in dataset[data_idx][0].items():\n",
    "        all_feature_dict[feature_key].append(feature_val)\n",
    "        \n",
    "standard_scaling_feature_map = {}\n",
    "for key,val in all_feature_dict.items():\n",
    "    if type(val[0]) is not list and type(val[0]) is not str and val[0].type() is not str:\n",
    "        if len(val[0].size())>0 and \"bool\" not in str(val[0].type()).lower():\n",
    "#             print(val[0].type())\n",
    "            mean = torch.mean(torch.cat(val).reshape(-1)).item()\n",
    "            std = torch.std(torch.cat(val).reshape(-1)).item()\n",
    "            standard_scaling_feature_map[key] = {'mean':mean,'std':std}\n",
    "        elif \"bool\" not in str(val[0].type()).lower():\n",
    "            mean = torch.mean(torch.stack(val).reshape(-1)).item()\n",
    "            std = torch.std(torch.stack(val).reshape(-1)).item()\n",
    "            standard_scaling_feature_map[key] = {'mean':mean,'std':std}\n",
    "#             print(key, 'mean', , 'std', )\n",
    "standard_scaling_feature_map       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a72c9b-fab8-4536-b125-abc46cc800b6",
   "metadata": {},
   "source": [
    "### Learnings about model CNN input\n",
    "- it's better to use multiple channel for logreturn1 and logreturn2 than stacking it and using as one channel\n",
    "- 2 channels input for CNN is better than stacking it(dim 2, which is logret1_t1, logret2_t1, logret1_t2, logret2_t2...) and using it as one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4b98ff-b328-465e-a04e-c25eef8ea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = None\n",
    "\n",
    "\n",
    "def loss_fn_mse(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)))\n",
    "\n",
    "def loss_fn_mspe(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)/y))\n",
    "\n",
    "def loss_fn_orig(y, pred):\n",
    "    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a6a979-c114-4525-953f-f5c368fe26d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_stocks = dataset.main_df['stock_id'].max()\n",
    "number_of_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7de51c7-de13-45ff-8513-5d890d691352",
   "metadata": {},
   "outputs": [],
   "source": [
    "realize_volatility_scale_factor = 1000\n",
    "def scale_optiver_feature(feature_name, feature_tensor):\n",
    "#     standard_scaling_feature_map = {'stock_id': {'mean': 62.43794250488281, 'std': 37.12644958496094},\n",
    "#  'seconds_in_bucket_xs': {'mean': 193.01312255859375,\n",
    "#   'std': 199.03518676757812},\n",
    "#  'has_trade_data_xs': {'mean': 0.14914073050022125, 'std': 0.3562271296977997},\n",
    "#  'bid_price1_^_xs': {'mean': 0.649695634841919, 'std': 0.4768790304660797},\n",
    "#  'bid_price1_v_xs': {'mean': 0.6500793695449829, 'std': 0.477160781621933},\n",
    "#  'bid_size1_^_xs': {'mean': 603.4517822265625, 'std': 4682.9423828125},\n",
    "#  'bid_size1_v_xs': {'mean': 0.04587273672223091, 'std': 0.1751980185508728},\n",
    "#  'ask_price1_^_xs': {'mean': 0.650066614151001, 'std': 0.47715136408805847},\n",
    "#  'ask_price1_v_xs': {'mean': 0.6497082710266113, 'std': 0.4768883287906647},\n",
    "#  'ask_size1_^_xs': {'mean': 600.0850830078125, 'std': 4266.17578125},\n",
    "#  'ask_size1_v_xs': {'mean': 0.04358990117907524, 'std': 0.16984671354293823},\n",
    "#  'price_^_xs': {'mean': 0.14412905275821686, 'std': 0.3583465814590454},\n",
    "#  'price_v_xs': {'mean': 0.14414073526859283, 'std': 0.3583745062351227},\n",
    "#  'size_^_xs': {'mean': 53.09535217285156, 'std': 497.29290771484375},\n",
    "#  'size_v_xs': {'mean': 0.007974185980856419, 'std': 0.09658150374889374},\n",
    "#  'order_count_^_xs': {'mean': 0.6173872351646423, 'std': 3.3599042892456055},\n",
    "#  'order_count_v_xs': {'mean': 0.07179050892591476, 'std': 0.23097625374794006}}\n",
    "#     print(feature_name, feature_tensor.size())\n",
    "    \n",
    "#     if feature_name in ['book_realized_volatility_xs','trade_realized_volatility_xs']:\n",
    "#         # we expect feature_tensor to be log returns tensor\n",
    "#         feature_tensor = feature_tensor ** 2\n",
    "# #         print(feature_tensor)\n",
    "#         feature_tensor = torch.cumsum(feature_tensor,1)\n",
    "#         # scale it to make each step realize volatility extrapolatable to 10 min window\n",
    "# #         feature_tensor = feature_tensor * torch.tensor([data_intervals_count/idx for idx in range(1,data_intervals_count+1,1)])\n",
    "#         feature_tensor = torch.sqrt(feature_tensor) * realize_volatility_scale_factor\n",
    "        \n",
    "    if feature_name == 'sequence_mask_xs':\n",
    "        feature_tensor = feature_tensor.type(torch.float32)\n",
    "        return feature_tensor\n",
    "    if feature_name == 'has_trade_data_xs':\n",
    "        #TODO: we'll pre convert it so directly reutrn feature_tensor without converting\n",
    "        return feature_tensor.type(torch.float32)\n",
    "    if feature_name == 'seconds_in_bucket_xs':\n",
    "        return feature_tensor / standard_scaling_feature_map[feature_name]['std']/100\n",
    "    if feature_name in standard_scaling_feature_map:\n",
    "        return (feature_tensor - standard_scaling_feature_map[feature_name]['mean'])/standard_scaling_feature_map[feature_name]['std']\n",
    "#         return feature_tensor/standard_scaling_feature_map[feature_name]['std']/2\n",
    "    if feature_name in ['trade_price_local_standardized_xs','book_wap1_local_standardized_xs']:\n",
    "        #TODO: the kaggle version of pytorch dont have nan_to_num, do something here!\n",
    "        feature_tensor = torch.masked_fill(feature_tensor, torch.isinf(feature_tensor),0)\n",
    "#         feature_tensor = torch.nan_to_num(feature_tensor,nan=0, posinf=0, neginf=0)\n",
    "#     print(feature_tensor)\n",
    "#     print(torch.any(torch.isnan(feature_tensor)))\n",
    "#     input()\n",
    "    return feature_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b110429-4ffd-4923-8f5e-392e119ee6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key,val in dataset[10000][0].items():\n",
    "#     print(key)\n",
    "# #     print(val)\n",
    "#     if key not in ['row_id']:\n",
    "#         print(scale_optiver_feature(key, val.reshape(1,-1)))\n",
    "#     input()\n",
    "    \n",
    "    \n",
    "# for idx in range(len(dataset)):\n",
    "#     feature_x = dataset[idx][0]\n",
    "#     test = scale_optiver_feature('logret1_xs', feature_x['logret1_xs'].reshape(1,-1))\n",
    "#     print(feature_x['logret1_xs'].tolist())\n",
    "#     print(test.tolist())\n",
    "#     input()\n",
    "#     realized_volatility_xs = scale_optiver_feature('realized_volatility_xs', feature_x['logret1_xs'].reshape(1,-1))\n",
    "#     print(feature_x['book_realized_volatility'])\n",
    "# #     print(feature_x['logret1_xs'])\n",
    "#     print(realized_volatility_xs)\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e35973-37f1-408a-b47f-3a8fa6c9879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockIdEmbedding(nn.Module):\n",
    "    def __init__(self,number_of_stock_embeddings=126+10, number_of_stock_embedding_dimention=2, mode='train'):\n",
    "        super(StockIdEmbedding, self).__init__()\n",
    "        \n",
    "        self.number_of_stock_embeddings = number_of_stock_embeddings\n",
    "        self.number_of_stock_embedding_dimention = number_of_stock_embedding_dimention\n",
    "        self.stock_embedding = nn.Embedding(self.number_of_stock_embeddings, self.number_of_stock_embedding_dimention)\n",
    "        self.mode = mode\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(self.number_of_stock_embedding_dimention, 32),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return []\n",
    "    \n",
    "    def set_mode(self,mode):\n",
    "        self.mode = mode\n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "        \n",
    "        stock_id_clamped = torch.clamp(feature_dict['stock_id'],0,self.number_of_stock_embeddings-1)\n",
    "        stock_id_clamped = stock_id_clamped.type(torch.int64)\n",
    "        stock_id_clamped = stock_id_clamped.to(device).reshape(-1,1)\n",
    "        embedding_logits = self.stock_embedding(stock_id_clamped)\n",
    "        embedding_logits = embedding_logits.reshape(-1,self.number_of_stock_embedding_dimention)\n",
    "        \n",
    "        if self.mode == 'stock_id_embedding':\n",
    "            return embedding_logits\n",
    "\n",
    "            \n",
    "        logits = self.linear_stack(embedding_logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ae512-810e-410f-a177-127168b56106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len,1, d_model)\n",
    "#         print(pe.size())\n",
    "        pe[:,0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:,0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "#         print(\"x\",x.size())\n",
    "#         print(\"PE\",self.pe[:,:x.size(1)].size())\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class MultiFetTransformer(nn.Module):\n",
    "    def __init__(self,feature_names=None, overview_feature_names=None, mask_feature_name='sequence_mask_xs',sequence_feature_name='seconds_in_bucket_xs', mode=\"train\"):\n",
    "        \"\"\"single feature, feature learner\n",
    "        `mode`: train|feature_generator\n",
    "        \"\"\"\n",
    "        super(MultiFetTransformer,self).__init__()\n",
    "        if feature_names is None:\n",
    "             \n",
    "#             feature_names = ['seconds_in_bucket_xs_group','has_trade_data_xs'] + [ f'{feature_name}_sum_xs' for feature_name in ['wap1','wap_balance','logret1','logret2','logrett',\n",
    "#                              'price_spread1','bid_spread','ask_spread','total_volume','volume_imbalance',\n",
    "#                             'size','order_count','trade_money_turnover','trade_book_price_spread']] + [ f'{feature_name}_max_xs' for feature_name in ['wap1','wap_balance','logret1','logret2','logrett',\n",
    "#                              'price_spread1','bid_spread','ask_spread','total_volume','volume_imbalance',\n",
    "#                             'size','order_count','trade_money_turnover','trade_book_price_spread']]\n",
    "            # 'bid_price2','bid_size2','ask_price2','ask_size2',\n",
    "            feature_names = [f'{feature_name}_xs' for feature_name in ['has_trade_data','seconds_in_bucket'] + \n",
    "                                                                         ['logret1','logret_bid_price1','logret_ask_price1','logret_bid_price2','logret_ask_price2','price_spread1',\n",
    "                                                        'bid_spread','ask_spread','logret_directional_volume1','logret_directional_volume2','logret_trade_money_turnover','trade_price_push_on_book','logret_price','order_count']]\n",
    "        \n",
    "            \n",
    "#             feature_names = ['seconds_in_bucket_xs_group','has_trade_data_xs']\n",
    "        \n",
    "        if overview_feature_names is None:\n",
    "            overview_feature_names = []\n",
    "            overview_aggregations = {\n",
    "    #         'wap1': ['sum', 'std'],\n",
    "    #         'wap2': [np.sum, np.std],\n",
    "            'logret1': [realized_volatility],\n",
    "            'logret2': [realized_volatility],\n",
    "            'logret_price': [realized_volatility],\n",
    "    #         'wap_balance': ['sum', 'max'],\n",
    "    #         'price_spread1': ['sum', 'max'],\n",
    "    #         'bid_spread': ['sum', 'max'],\n",
    "    #         'ask_spread': ['sum', 'max'],\n",
    "            'total_volume': ['sum', 'max'],\n",
    "            'volume_imbalance': ['sum', 'max'],\n",
    "    #         \"bid_ask_spread\": ['sum', 'max'],\n",
    "            'size':  ['sum', 'max','min'],\n",
    "            'order_count': ['sum', 'max'],\n",
    "            'trade_money_turnover': ['sum', 'max','min'],\n",
    "            }\n",
    "        \n",
    "            for key, aggs in overview_aggregations.items():\n",
    "                for agg in aggs:\n",
    "                    if isinstance(agg, types.FunctionType):\n",
    "                        agg = agg.__name__\n",
    "                    overview_feature_names.append(f'{key}_{agg}')\n",
    "    #             overview_feature_names = ['wap1_sum', 'wap1_std', 'logret1_realized_volatility', 'logret2_realized_volatility', 'logrett_realized_volatility', 'wap_balance_sum', 'wap_balance_max', 'price_spread1_sum', 'price_spread1_max', 'bid_spread_sum', 'bid_spread_max', 'ask_spread_sum', 'ask_spread_max', 'total_volume_sum', 'total_volume_max', 'volume_imbalance_sum',\n",
    "    #                                       'volume_imbalance_max', 'bid_ask_spread_sum', 'bid_ask_spread_max', 'size_sum', 'size_max', 'size_min', 'order_count_sum', 'order_count_max', 'trade_money_turnover_sum', 'trade_money_turnover_max', 'trade_money_turnover_min']\n",
    "\n",
    "        \n",
    "        \n",
    "        if type(feature_names) == str:\n",
    "            feature_names = [feature_names]\n",
    "            \n",
    "        self.stock_id_embedding_dimension = 3\n",
    "        self.stock_id_embedding = StockIdEmbedding(number_of_stock_embedding_dimention=self.stock_id_embedding_dimension, mode='stock_id_embedding')\n",
    "        \n",
    "        self.feature_names = feature_names\n",
    "        self.overview_feature_names = overview_feature_names\n",
    "        self.mask_feature_name = mask_feature_name\n",
    "        self.sequence_feature_name = sequence_feature_name\n",
    "        self.features_count = len(self.feature_names)\n",
    "#         self.scaled_feature_dimension = 4\n",
    "#         self.feature_scalers = nn.ModuleList([nn.Sequential(\n",
    "#                     # one is original feature, then stock,\n",
    "#                     nn.Linear(1+self.stock_id_embedding_dimension, 32),\n",
    "#                     nn.GELU(),\n",
    "#                     nn.Linear(32, self.scaled_feature_dimension)\n",
    "#                 ) for _ in self.feature_names])\n",
    "#         for feature_name in self.feature_names:\n",
    "#             self.feature_scalers.append()\n",
    "        \n",
    "        self.output_dimensions = 512\n",
    "        self.transformer_input_dimension = 16\n",
    "#         self.positional_encoding = PositionalEncoding(d_model=self.transformer_input_dimension)\n",
    "        self.mode = mode\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.transformer_input_dimension, nhead=8, dropout=0.15, activation='gelu')\n",
    "        self.encoder_stack = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "        \n",
    "        self.feature_to_feature_embedding = nn.Sequential(\n",
    "            nn.Linear(self.features_count  + self.stock_id_embedding_dimension, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, self.transformer_input_dimension)\n",
    "        )\n",
    "        \n",
    "        self.linear_feature_stack_ = nn.Sequential(\n",
    "#             nn.Conv1d(self.transformer_input_dimension, 32, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(32, 24, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Flatten(),\n",
    "            nn.Linear(data_intervals_count*self.transformer_input_dimension,512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512,self.output_dimensions),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, self.features_out),\n",
    "        )\n",
    "        \n",
    "        self.linear_trainer_stack_ = nn.Sequential(\n",
    "#             nn.Linear(self.output_dimensions + len(self.overview_feature_names) + self.stock_id_embedding_dimension, 256),\n",
    "            nn.Linear(self.output_dimensions+len(self.overview_feature_names)+self.stock_id_embedding_dimension, 256),\n",
    "#             nn.Linear(len(self.overview_feature_names) + self.stock_id_embedding_dimension, 256),\n",
    "#             nn.Linear(self.features_out, 1),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.Linear(self.features_out, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 128),\n",
    "#             nn.Linear(self.features_out, 1),\n",
    "            nn.ReLU(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.Hardswish(),\n",
    "            nn.Linear(128, 1),   \n",
    "        )\n",
    "        \n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def forward(self, feature_dict, h0_tensor=None):    \n",
    "        feature_x = []\n",
    "        \n",
    "#         stock_embedding_logits = stock_embedding_logits.unsqueeze(1)#.unsqueeze(0)\n",
    "#         stock_embedding_logits = stock_embedding_logits.repeat(1,data_intervals_count,1)\n",
    "        \n",
    "        for idx,feature_name in enumerate(self.feature_names):\n",
    "            feature_tensor = scale_optiver_feature(feature_name, feature_dict[feature_name]).to(device)\n",
    "            feature_tensor = feature_tensor.reshape(-1,feature_tensor.size(1),1)\n",
    "#             feature_transformed = torch.cat([feature_tensor, stock_embedding_logits], 2)\n",
    "#             self.feature_scalers[idx](feature_transformed)\n",
    "            feature_x.append(feature_tensor)\n",
    "            \n",
    "        \n",
    "        #combine all the features activated tensors\n",
    "        feature_x = torch.cat(feature_x,dim=2) #.reshape(-1, data_intervals_count, self.input_size_) #+[stock_embedding_logits]\n",
    "\n",
    "#         self.feature_to_feature_embedding = self.feature_to_feature_embedding.to(device)\n",
    "#         feature_x = self.feature_to_feature_embedding(feature_x)\n",
    "        \n",
    "        #positional encoding\n",
    "        position_encodings = feature_dict[self.sequence_feature_name]\n",
    "        position_encodings = (position_encodings.to(device)+1)/601\n",
    "        position_encodings = position_encodings.reshape(-1,feature_x.size(1),1)\n",
    "        feature_x = torch.add(feature_x, position_encodings)\n",
    "        \n",
    "        #Mask prepare\n",
    "        mask = feature_dict[self.mask_feature_name].to(device)\n",
    "        \n",
    "        \n",
    "        # make them sequence first!!\n",
    "        # RANT: all of this due to Kaggle using pytorch 1.7.0\n",
    "        feature_x = torch.stack(feature_x.unbind(0),dim=1)\n",
    "        \n",
    "        out = self.encoder_stack(feature_x, src_key_padding_mask=mask)\n",
    "        # back to batch first!\n",
    "        out = torch.stack(out.unbind(0),dim=1)\n",
    "        out = out.reshape(-1, self.transformer_input_dimension*data_intervals_count)\n",
    "        \n",
    "\n",
    "        out_ = self.linear_feature_stack_(out)\n",
    "        # we add overview features here CAT them; that's why \n",
    "#         print(out_.size(),stock_embedding_logits.size())\n",
    "        stock_embedding_logits = self.stock_id_embedding(feature_dict)\n",
    "    \n",
    "        feature_x = []\n",
    "        for idx,feature_name in enumerate(self.overview_feature_names):\n",
    "            feature_tensor = scale_optiver_feature(feature_name, feature_dict[feature_name]).to(device)\n",
    "#             print(feature_tensor.size())\n",
    "            feature_tensor = feature_tensor.reshape(-1,1)\n",
    "            \n",
    "            feature_x.append(feature_tensor)\n",
    "        feature_x = torch.cat(feature_x,dim=1)\n",
    "#         print(feature_x.size())\n",
    "#         print(out_.size())\n",
    "        out_ = torch.cat([out_,feature_x,stock_embedding_logits],dim=1)\n",
    "        out_ = self.linear_trainer_stack_(out_)\n",
    "\n",
    "        return out_\n",
    "\n",
    "# model = MultiFetTransformer()\n",
    "# model.to(device)\n",
    "# dataloader_train = DataLoader(dataset, batch_size=2,\n",
    "#                                 shuffle=True, num_workers=0, pin_memory=False)#, collate_fn=optiver_custom_collate_func)\n",
    "\n",
    "\n",
    " \n",
    "# >>> src = torch.rand(10, 32, 512)\n",
    "# >>> out = encoder_layer(src)   \n",
    "# for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "#     model(Feature_X)\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58e3006a-19df-4243-b389-07e09236408e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1f24f-c586-4e1b-aad6-fb4671c87232",
   "metadata": {},
   "source": [
    "#### analyze the initial weights (or change them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "931890cf-e329-4a15-b0d0-352dc27f985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @torch.no_grad()\n",
    "# def init_weights(m):\n",
    "# #     print(m)\n",
    "#     if type(m) == nn.Linear:\n",
    "# #         m.weight.fill_(1.0)\n",
    "#         torch.nn.init.xavier_uniform_(m.weight,gain=10)\n",
    "#         m.bias.data.uniform_(-1,1)\n",
    "# #     elif type(m) == nn.ReLU:\n",
    "# #         print(m.data)\n",
    "#     else:\n",
    "#         print(type(m))\n",
    "# #         print(m.weight)\n",
    "# model.apply(init_weights)\n",
    "# # for param in model.parameters():\n",
    "# # #     print(param)\n",
    "# #       print(param.data.size(), param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c79a7d-9e08-4433-9c77-1af049f349d6",
   "metadata": {},
   "source": [
    "### LEarning rate: our base line is 0.34 loss as that's what the optiver guys have when they use current 10 min realize vol and use it as target (copy to prediction). We create simplest neural network and work with learning rates to figure out what's best and when we see something in range of 0.35 then we've found good Learning rate\n",
    "- #### SGD: 1e-7 works best\n",
    "- #### ADAM: 1e-5, (NOTE: 1e-3 makes it behave dumb where some deep local minima gets stuck and produces constant output!)\n",
    "- TODO: analyze that constant output phenomenon more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cb3871f-5215-4afb-92fc-47ae2d385fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-4\n",
    "# batch_size = 4096\n",
    "# epochs = 100\n",
    "\n",
    "# input_scaling = 1\n",
    "# output_scaling = 1\n",
    "\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "# strategyname = \"ret1_n_ret2\"\n",
    "# summary_writer = SummaryWriter(f'../output/training_tensorboard/{strategyname}_scaleIn{input_scaling}Out{output_scaling}_{learning_rate}_{batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aadc54b-6009-4af6-bfb6-f6619c6acd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a7ed1-80f8-4a57-90b4-223345dde76b",
   "metadata": {},
   "source": [
    "### Learnings about training\n",
    "- (non scaling)logreturns input and volatility output; non scaled makes the model predict constant output with no variety(close to 0 std dev)\n",
    "- scaling input rids of variety issue, \n",
    "- scaling output makes the model start with low rmse initially so there's less ground to cover and we can iterate over ideas rapidly due to less epochs needed to achieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef34a31-7317-47c0-a94f-9bebe2fd2e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n",
      "Current epochs: 200 LR : 0.001 batch_size: 512\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "---------- 10sData_40klr_dropout1pt5_solidOvvFet_512linsize_0.001_512 train ----------\n",
      "train: 0.9964911800164443 [ 6144/343145] recent lr: 4.1957319583913684e-07\n",
      "train: 0.9958656888741714 [12800/343145] recent lr: 8.391463916782737e-07\n",
      "train: 0.994503874045152 [19456/343145] recent lr: 1.2587195875174106e-06\n",
      "train: 0.9925631193014292 [26112/343145] recent lr: 1.6782927833565474e-06\n",
      "train: 0.989629314495967 [32768/343145] recent lr: 2.0978659791956843e-06\n",
      "train: 0.9859242622668927 [39424/343145] recent lr: 2.5174391750348213e-06\n",
      "train: 0.9806639001919673 [46080/343145] recent lr: 2.9370123708739578e-06\n",
      "train: 0.9728157428594736 [52736/343145] recent lr: 3.3565855667130947e-06\n",
      "train: 0.961376258960137 [59392/343145] recent lr: 3.7761587625522317e-06\n",
      "train: 0.9431583697979267 [66048/343145] recent lr: 4.195731958391369e-06\n",
      "train: 0.9160048136344323 [72704/343145] recent lr: 4.615305154230506e-06\n",
      "train: 0.8733362417954665 [79360/343145] recent lr: 5.0348783500696425e-06\n",
      "train: 0.8182539573082557 [86016/343145] recent lr: 5.454451545908779e-06\n",
      "train: 0.7447206607231727 [92672/343145] recent lr: 5.8740247417479156e-06\n",
      "train: 0.6657102153851435 [99328/343145] recent lr: 6.293597937587053e-06\n",
      "train: 0.6029207752301142 [105984/343145] recent lr: 6.7131711334261894e-06\n",
      "train: 0.5841186229999249 [112640/343145] recent lr: 7.132744329265326e-06\n",
      "train: 0.5759168634047875 [119296/343145] recent lr: 7.552317525104463e-06\n",
      "train: 0.5667783342874967 [125952/343145] recent lr: 7.9718907209436e-06\n",
      "train: 0.5728694750712469 [132608/343145] recent lr: 8.391463916782737e-06\n",
      "train: 0.5592767642094538 [139264/343145] recent lr: 8.811037112621874e-06\n",
      "train: 0.5578334239812998 [145920/343145] recent lr: 9.230610308461012e-06\n",
      "train: 0.5471239365064181 [152576/343145] recent lr: 9.650183504300148e-06\n",
      "train: 0.5446822414031396 [159232/343145] recent lr: 1.0069756700139285e-05\n",
      "train: 0.5325562357902527 [165888/343145] recent lr: 1.0489329895978422e-05\n",
      "train: 0.5119042946742132 [172544/343145] recent lr: 1.0908903091817558e-05\n",
      "train: 0.5356978796995603 [179200/343145] recent lr: 1.1328476287656695e-05\n",
      "train: 0.5013135029719427 [185856/343145] recent lr: 1.1748049483495831e-05\n",
      "train: 0.45634743112784165 [192512/343145] recent lr: 1.216762267933497e-05\n",
      "train: 0.4377225110164055 [199168/343145] recent lr: 1.2587195875174106e-05\n",
      "train: 0.39499433911763704 [205824/343145] recent lr: 1.3006769071013242e-05\n",
      "train: 0.3754379359575418 [212480/343145] recent lr: 1.3426342266852379e-05\n",
      "train: 0.3434309730162987 [219136/343145] recent lr: 1.3845915462691515e-05\n",
      "train: 0.35518136620521545 [225792/343145] recent lr: 1.4265488658530652e-05\n",
      "train: 0.3474239936241737 [232448/343145] recent lr: 1.468506185436979e-05\n",
      "train: 0.33959595285929167 [239104/343145] recent lr: 1.5104635050208927e-05\n",
      "train: 0.37348488660959095 [245760/343145] recent lr: 1.5524208246048063e-05\n",
      "train: 0.3299653140398172 [252416/343145] recent lr: 1.59437814418872e-05\n",
      "train: 0.35489533497737 [259072/343145] recent lr: 1.6363354637726336e-05\n",
      "train: 0.3238860873075632 [265728/343145] recent lr: 1.6782927833565474e-05\n",
      "train: 0.3272938522008749 [272384/343145] recent lr: 1.720250102940461e-05\n",
      "train: 0.31682004836889416 [279040/343145] recent lr: 1.7622074225243748e-05\n",
      "train: 0.3275439670452705 [285696/343145] recent lr: 1.8041647421082882e-05\n",
      "train: 0.3178098729023567 [292352/343145] recent lr: 1.8461220616922024e-05\n",
      "train: 0.31881438768827 [299008/343145] recent lr: 1.888079381276116e-05\n",
      "train: 0.3140119979014763 [305664/343145] recent lr: 1.9300367008600297e-05\n",
      "train: 0.3132270116072435 [312320/343145] recent lr: 1.9719940204439432e-05\n",
      "train: 0.3052660089272719 [318976/343145] recent lr: 2.013951340027857e-05\n",
      "train: 0.324341567663046 [325632/343145] recent lr: 2.0559086596117705e-05\n",
      "train: 0.3062843611607185 [332288/343145] recent lr: 2.0978659791956843e-05\n",
      "train: 0.31772756347289455 [338944/343145] recent lr: 2.1398232987795978e-05\n",
      "train: 0.2955051176249981 test: 0.30961689902913003 [343040/343145] recent lr: 2.1656431877543138e-05\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "---------- 10sData_40klr_dropout1pt5_solidOvvFet_512linsize_0.001_512 train ----------\n",
      "train: 0.304264743413244 [ 2560/343145] recent lr: 2.1850081044853512e-05\n",
      "train: 0.3014147167022412 [ 9216/343145] recent lr: 2.2269654240692647e-05\n",
      "train: 0.3061613623912518 [15872/343145] recent lr: 2.2689227436531785e-05\n",
      "train: 0.29345375987199634 [22528/343145] recent lr: 2.310880063237092e-05\n",
      "train: 0.3036251366138458 [29184/343145] recent lr: 2.352837382821006e-05\n",
      "train: 0.29134670816935027 [35840/343145] recent lr: 2.3947947024049193e-05\n",
      "train: 0.30647395436580366 [42496/343145] recent lr: 2.436752021988833e-05\n",
      "train: 0.2978292818252857 [49152/343145] recent lr: 2.478709341572747e-05\n",
      "train: 0.29841686212099516 [55808/343145] recent lr: 2.5206666611566608e-05\n",
      "train: 0.29070321642435515 [62464/343145] recent lr: 2.5626239807405743e-05\n",
      "train: 0.2953530137355511 [69120/343145] recent lr: 2.604581300324488e-05\n",
      "train: 0.2910796816532428 [75776/343145] recent lr: 2.646538619908402e-05\n",
      "train: 0.28009222791745114 [82432/343145] recent lr: 2.6884959394923154e-05\n",
      "train: 0.2949387499919304 [89088/343145] recent lr: 2.7304532590762292e-05\n",
      "train: 0.3034009268650642 [95744/343145] recent lr: 2.7724105786601427e-05\n",
      "train: 0.2904637662264017 [102400/343145] recent lr: 2.8143678982440565e-05\n",
      "train: 0.29007031596623933 [109056/343145] recent lr: 2.85632521782797e-05\n",
      "train: 0.28709129645274234 [115712/343145] recent lr: 2.898282537411884e-05\n",
      "train: 0.2922698144729321 [122368/343145] recent lr: 2.9402398569957973e-05\n",
      "train: 0.2921376182482793 [129024/343145] recent lr: 2.982197176579711e-05\n",
      "train: 0.28255117856539214 [135680/343145] recent lr: 3.024154496163625e-05\n",
      "train: 0.2706405956011552 [142336/343145] recent lr: 3.0661118157475384e-05\n",
      "train: 0.27441596067868745 [148992/343145] recent lr: 3.108069135331452e-05\n",
      "train: 0.2758568525314331 [155648/343145] recent lr: 3.150026454915366e-05\n",
      "train: 0.27864639117167544 [162304/343145] recent lr: 3.191983774499279e-05\n",
      "train: 0.2798159076617314 [168960/343145] recent lr: 3.233941094083193e-05\n",
      "train: 0.28119625265781695 [175616/343145] recent lr: 3.2758984136671076e-05\n",
      "train: 0.281541102207624 [182272/343145] recent lr: 3.317855733251021e-05\n",
      "train: 0.2712925901779762 [188928/343145] recent lr: 3.3598130528349345e-05\n",
      "train: 0.27127275787867033 [195584/343145] recent lr: 3.4017703724188483e-05\n",
      "train: 0.2717861601939568 [202240/343145] recent lr: 3.443727692002762e-05\n",
      "train: 0.2809658898757054 [208896/343145] recent lr: 3.485685011586675e-05\n",
      "train: 0.2745107045540443 [215552/343145] recent lr: 3.527642331170589e-05\n",
      "train: 0.27086954850416917 [222208/343145] recent lr: 3.569599650754503e-05\n",
      "train: 0.26996835951621717 [228864/343145] recent lr: 3.611556970338417e-05\n",
      "train: 0.27791162637563854 [235520/343145] recent lr: 3.65351428992233e-05\n",
      "train: 0.3193782545053042 [242176/343145] recent lr: 3.695471609506244e-05\n",
      "train: 0.2856369614601135 [248832/343145] recent lr: 3.7374289290901576e-05\n",
      "train: 0.2735623029562143 [255488/343145] recent lr: 3.7793862486740714e-05\n",
      "train: 0.26884154631541324 [262144/343145] recent lr: 3.821343568257985e-05\n",
      "train: 0.2664608462498738 [268800/343145] recent lr: 3.8633008878418983e-05\n",
      "train: 0.28298412148769087 [275456/343145] recent lr: 3.905258207425812e-05\n",
      "train: 0.26965487691072315 [282112/343145] recent lr: 3.947215527009726e-05\n",
      "train: 0.2653094782279088 [288768/343145] recent lr: 3.98917284659364e-05\n",
      "train: 0.27422396494792056 [295424/343145] recent lr: 4.031130166177553e-05\n",
      "train: 0.26856921040094817 [302080/343145] recent lr: 4.073087485761467e-05\n",
      "train: 0.2851005471669711 [308736/343145] recent lr: 4.1150448053453806e-05\n",
      "train: 0.26956660930926984 [315392/343145] recent lr: 4.1570021249292944e-05\n",
      "train: 0.2719626185985712 [322048/343145] recent lr: 4.1989594445132076e-05\n",
      "train: 0.29137889926250166 [328704/343145] recent lr: 4.2409167640971214e-05\n",
      "train: 0.27584784076764035 [335360/343145] recent lr: 4.282874083681035e-05\n",
      "train: 0.2783960046676489 [342016/343145] recent lr: 4.32483140326495e-05\n",
      "train: 0.2688102275133133 test: 0.27008159919863656 [343040/343145] recent lr: 4.3312863755086276e-05\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "---------- 10sData_40klr_dropout1pt5_solidOvvFet_512linsize_0.001_512 train ----------\n",
      "train: 0.29896192465509686 [ 5632/343145] recent lr: 4.3700162089707025e-05\n",
      "train: 0.2754565156423129 [12288/343145] recent lr: 4.411973528554616e-05\n",
      "train: 0.2683011476810162 [18944/343145] recent lr: 4.4539308481385294e-05\n",
      "train: 0.27000610874249387 [25600/343145] recent lr: 4.495888167722443e-05\n",
      "train: 0.2689071618593656 [32256/343145] recent lr: 4.537845487306357e-05\n",
      "train: 0.2722941843362955 [38912/343145] recent lr: 4.579802806890271e-05\n",
      "train: 0.2632578955246852 [45568/343145] recent lr: 4.621760126474184e-05\n",
      "train: 0.2675267159938812 [52224/343145] recent lr: 4.663717446058098e-05\n",
      "train: 0.27147487952159 [58880/343145] recent lr: 4.705674765642012e-05\n",
      "train: 0.26238021369163805 [65536/343145] recent lr: 4.7476320852259255e-05\n",
      "train: 0.26887628665337193 [72192/343145] recent lr: 4.789589404809839e-05\n",
      "train: 0.27159714928040135 [78848/343145] recent lr: 4.8315467243937525e-05\n",
      "train: 0.26696239984952486 [85504/343145] recent lr: 4.873504043977666e-05\n",
      "train: 0.2639346443689786 [92160/343145] recent lr: 4.91546136356158e-05\n",
      "train: 0.26031622290611267 [98816/343145] recent lr: 4.957418683145494e-05\n",
      "train: 0.2669194902365024 [105472/343145] recent lr: 4.999376002729408e-05\n",
      "train: 0.26245717245798844 [112128/343145] recent lr: 5.0413333223133216e-05\n",
      "train: 0.27138622907491833 [118784/343145] recent lr: 5.0832906418972354e-05\n",
      "train: 0.28180325719026417 [125440/343145] recent lr: 5.1252479614811486e-05\n",
      "train: 0.2643438990299518 [132096/343145] recent lr: 5.1672052810650624e-05\n",
      "train: 0.2773570796618095 [138752/343145] recent lr: 5.209162600648976e-05\n",
      "train: 0.2755071268631862 [145408/343145] recent lr: 5.25111992023289e-05\n",
      "train: 0.26364355362378633 [152064/343145] recent lr: 5.293077239816804e-05\n",
      "train: 0.26555898785591125 [158720/343145] recent lr: 5.335034559400717e-05\n",
      "train: 0.2608637809753418 [165376/343145] recent lr: 5.376991878984631e-05\n",
      "train: 0.2634204247823128 [172032/343145] recent lr: 5.4189491985685446e-05\n",
      "train: 0.2624346178311568 [178688/343145] recent lr: 5.4609065181524585e-05\n",
      "train: 0.25839967796435726 [185344/343145] recent lr: 5.5028638377363716e-05\n",
      "train: 0.26591769548562855 [192000/343145] recent lr: 5.5448211573202854e-05\n",
      "train: 0.26694607390807223 [198656/343145] recent lr: 5.586778476904199e-05\n",
      "train: 0.2573603724057858 [205312/343145] recent lr: 5.628735796488113e-05\n",
      "train: 0.2613687492333926 [211968/343145] recent lr: 5.670693116072026e-05\n",
      "train: 0.26894684021289533 [218624/343145] recent lr: 5.71265043565594e-05\n",
      "train: 0.26143311766477734 [225280/343145] recent lr: 5.754607755239854e-05\n",
      "train: 0.2643181555546247 [231936/343145] recent lr: 5.796565074823768e-05\n",
      "train: 0.25520422710822177 [238592/343145] recent lr: 5.838522394407681e-05\n",
      "train: 0.263706053678806 [245248/343145] recent lr: 5.8804797139915946e-05\n",
      "train: 0.2648102388932155 [251904/343145] recent lr: 5.9224370335755085e-05\n",
      "train: 0.2621880861429068 [258560/343145] recent lr: 5.964394353159422e-05\n",
      "train: 0.26150074142676133 [265216/343145] recent lr: 6.006351672743336e-05\n",
      "train: 0.2676574014700376 [271872/343145] recent lr: 6.04830899232725e-05\n",
      "train: 0.2837167244691115 [278528/343145] recent lr: 6.090266311911164e-05\n",
      "train: 0.29084575863984913 [285184/343145] recent lr: 6.132223631495077e-05\n",
      "train: 0.2703522031123822 [291840/343145] recent lr: 6.174180951078991e-05\n",
      "train: 0.2631922925894077 [298496/343145] recent lr: 6.216138270662905e-05\n",
      "train: 0.2571729135054808 [305152/343145] recent lr: 6.258095590246818e-05\n",
      "train: 0.26760368736890644 [311808/343145] recent lr: 6.300052909830732e-05\n",
      "train: 0.2731508303147096 [318464/343145] recent lr: 6.342010229414645e-05\n",
      "train: 0.26713543557203734 [325120/343145] recent lr: 6.383967548998558e-05\n",
      "train: 0.259091018484189 [331776/343145] recent lr: 6.425924868582472e-05\n",
      "train: 0.2642615735530853 [338432/343145] recent lr: 6.467882188166386e-05\n",
      "train: 0.26043521530098385 test: 0.26049283102509524 [343040/343145] recent lr: 6.496929563262943e-05\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "---------- 10sData_40klr_dropout1pt5_solidOvvFet_512linsize_0.001_512 train ----------\n",
      "train: 0.2587284062589918 [ 2048/343145] recent lr: 6.51306699387214e-05\n",
      "train: 0.25274927799518293 [ 8704/343145] recent lr: 6.555024313456053e-05\n",
      "train: 0.2543192035876788 [15360/343145] recent lr: 6.596981633039967e-05\n",
      "train: 0.25892034975382 [22016/343145] recent lr: 6.638938952623881e-05\n",
      "train: 0.25826738201654875 [28672/343145] recent lr: 6.680896272207793e-05\n",
      "train: 0.26069320623691267 [35328/343145] recent lr: 6.722853591791707e-05\n",
      "train: 0.25783831912737626 [41984/343145] recent lr: 6.764810911375621e-05\n",
      "train: 0.2609142672557097 [48640/343145] recent lr: 6.806768230959535e-05\n",
      "train: 0.2579659934227283 [55296/343145] recent lr: 6.848725550543449e-05\n",
      "train: 0.25627896648186904 [61952/343145] recent lr: 6.890682870127363e-05\n",
      "train: 0.256498512167197 [68608/343145] recent lr: 6.932640189711276e-05\n",
      "train: 0.26380139818558324 [75264/343145] recent lr: 6.97459750929519e-05\n",
      "train: 0.2509322441541232 [81920/343145] recent lr: 7.016554828879104e-05\n",
      "train: 0.2627191451879648 [88576/343145] recent lr: 7.058512148463017e-05\n",
      "train: 0.26818111538887024 [95232/343145] recent lr: 7.10046946804693e-05\n",
      "train: 0.2608977991801042 [101888/343145] recent lr: 7.142426787630844e-05\n",
      "train: 0.2619001395427264 [108544/343145] recent lr: 7.18438410721476e-05\n",
      "train: 0.27034726624305433 [115200/343145] recent lr: 7.226341426798673e-05\n",
      "train: 0.2608663749236327 [121856/343145] recent lr: 7.268298746382587e-05\n",
      "train: 0.25864747625130874 [128512/343145] recent lr: 7.310256065966501e-05\n",
      "train: 0.2613967851950572 [135168/343145] recent lr: 7.352213385550415e-05\n",
      "train: 0.25771385431289673 [141824/343145] recent lr: 7.394170705134327e-05\n",
      "train: 0.2516327603505208 [148480/343145] recent lr: 7.436128024718241e-05\n",
      "train: 0.25195432626284087 [155136/343145] recent lr: 7.478085344302155e-05\n",
      "train: 0.25766004392733943 [161792/343145] recent lr: 7.520042663886069e-05\n",
      "train: 0.2578516751527786 [168448/343145] recent lr: 7.561999983469982e-05\n",
      "train: 0.2533161766253985 [175104/343145] recent lr: 7.603957303053896e-05\n",
      "train: 0.2556739082703224 [181760/343145] recent lr: 7.64591462263781e-05\n",
      "train: 0.26558757057556737 [188416/343145] recent lr: 7.687871942221724e-05\n",
      "train: 0.25833792182115406 [195072/343145] recent lr: 7.729829261805638e-05\n",
      "train: 0.26021305414346546 [201728/343145] recent lr: 7.77178658138955e-05\n",
      "train: 0.2529339285997244 [208384/343145] recent lr: 7.813743900973464e-05\n",
      "train: 0.2678314516177544 [215040/343145] recent lr: 7.855701220557378e-05\n",
      "train: 0.2727783436958606 [221696/343145] recent lr: 7.897658540141292e-05\n",
      "train: 0.26289837750104755 [228352/343145] recent lr: 7.939615859725205e-05\n",
      "train: 0.2579732651893909 [235008/343145] recent lr: 7.981573179309119e-05\n",
      "train: 0.257229120685504 [241664/343145] recent lr: 8.023530498893033e-05\n",
      "train: 0.2610128934566791 [248320/343145] recent lr: 8.065487818476947e-05\n",
      "train: 0.2583288699388504 [254976/343145] recent lr: 8.10744513806086e-05\n",
      "train: 0.25460062004052675 [261632/343145] recent lr: 8.149402457644773e-05\n",
      "train: 0.25080473950276005 [268288/343145] recent lr: 8.191359777228687e-05\n",
      "train: 0.26207926640143764 [274944/343145] recent lr: 8.233317096812601e-05\n",
      "train: 0.26469921607237595 [281600/343145] recent lr: 8.275274416396515e-05\n",
      "train: 0.2637176559521602 [288256/343145] recent lr: 8.317231735980428e-05\n",
      "train: 0.26414719223976135 [294912/343145] recent lr: 8.359189055564342e-05\n",
      "train: 0.2612339430130445 [301568/343145] recent lr: 8.401146375148256e-05\n",
      "train: 0.258701435648478 [308224/343145] recent lr: 8.443103694732169e-05\n",
      "train: 0.2542925924062729 [314880/343145] recent lr: 8.485061014316082e-05\n",
      "train: 0.25749921110960156 [321536/343145] recent lr: 8.527018333899996e-05\n",
      "train: 0.2569940158954033 [328192/343145] recent lr: 8.56897565348391e-05\n",
      "train: 0.2554824925385989 [334848/343145] recent lr: 8.610932973067824e-05\n",
      "train: 0.25933199318555683 [341504/343145] recent lr: 8.652890292651738e-05\n",
      "train: 0.2615383019049962 test: 0.2584362327165547 [343040/343145] recent lr: 8.662572751017255e-05\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "---------- 10sData_40klr_dropout1pt5_solidOvvFet_512linsize_0.001_512 train ----------\n",
      "train: 0.2606026881507465 [ 5120/343145] recent lr: 8.698075098357491e-05\n",
      "train: 0.25035894834078276 [11776/343145] recent lr: 8.740032417941405e-05\n",
      "train: 0.25284444941924167 [18432/343145] recent lr: 8.781989737525319e-05\n",
      "train: 0.2575796189216467 [25088/343145] recent lr: 8.823947057109233e-05\n",
      "train: 0.252907462991201 [31744/343145] recent lr: 8.865904376693145e-05\n",
      "train: 0.24594179139687464 [38400/343145] recent lr: 8.907861696277059e-05\n",
      "train: 0.25442353693338543 [45056/343145] recent lr: 8.949819015860973e-05\n",
      "train: 0.25250923748199755 [51712/343145] recent lr: 8.991776335444887e-05\n",
      "train: 0.24945617868350103 [58368/343145] recent lr: 9.0337336550288e-05\n",
      "train: 0.2564523300299278 [65024/343145] recent lr: 9.075690974612714e-05\n",
      "train: 0.25200853324853456 [71680/343145] recent lr: 9.117648294196628e-05\n",
      "train: 0.2489136904478073 [78336/343145] recent lr: 9.159605613780542e-05\n",
      "train: 0.2561165873820965 [84992/343145] recent lr: 9.201562933364454e-05\n",
      "train: 0.2535748573449942 [91648/343145] recent lr: 9.243520252948368e-05\n",
      "train: 0.25644901394844055 [98304/343145] recent lr: 9.285477572532282e-05\n",
      "train: 0.25106530579236835 [104960/343145] recent lr: 9.327434892116196e-05\n",
      "train: 0.2524611938458223 [111616/343145] recent lr: 9.36939221170011e-05\n",
      "train: 0.25348071180857146 [118272/343145] recent lr: 9.411349531284023e-05\n",
      "train: 0.2585717233327719 [124928/343145] recent lr: 9.453306850867937e-05\n",
      "train: 0.25135787633749157 [131584/343145] recent lr: 9.495264170451851e-05\n",
      "train: 0.2514242266233151 [138240/343145] recent lr: 9.537221490035765e-05\n",
      "train: 0.25566282753761 [144896/343145] recent lr: 9.579178809619677e-05\n",
      "train: 0.2586373652403171 [151552/343145] recent lr: 9.621136129203591e-05\n",
      "train: 0.24828651891304895 [158208/343145] recent lr: 9.663093448787505e-05\n",
      "train: 0.2497515586706308 [164864/343145] recent lr: 9.705050768371419e-05\n",
      "train: 0.24968440716083234 [171520/343145] recent lr: 9.747008087955333e-05\n",
      "train: 0.25505305368166703 [178176/343145] recent lr: 9.788965407539246e-05\n",
      "train: 0.2529902102855536 [184832/343145] recent lr: 9.83092272712316e-05\n",
      "train: 0.24904916607416594 [191488/343145] recent lr: 9.872880046707074e-05\n",
      "train: 0.25428562668653637 [198144/343145] recent lr: 9.914837366290988e-05\n",
      "train: 0.25957362812298995 [204800/343145] recent lr: 9.956794685874902e-05\n",
      "train: 0.25375651166989255 [211456/343145] recent lr: 9.998752005458816e-05\n",
      "train: 0.2527429828277001 [218112/343145] recent lr: 0.0001004070932504273\n",
      "train: 0.25206775963306427 [224768/343145] recent lr: 0.00010082666644626643\n",
      "train: 0.25289060175418854 [231424/343145] recent lr: 0.00010124623964210557\n",
      "train: 0.27274778943795425 [238080/343145] recent lr: 0.00010166581283794471\n",
      "train: 0.252105645262278 [244736/343145] recent lr: 0.00010208538603378385\n",
      "train: 0.25313590123103213 [251392/343145] recent lr: 0.00010250495922962297\n",
      "train: 0.2609176899378116 [258048/343145] recent lr: 0.00010292453242546211\n",
      "train: 0.2582081246834535 [264704/343145] recent lr: 0.00010334410562130125\n",
      "train: 0.2557274538737077 [271360/343145] recent lr: 0.00010376367881714039\n",
      "train: 0.24580274522304535 [278016/343145] recent lr: 0.00010418325201297952\n",
      "train: 0.2556776450230525 [284672/343145] recent lr: 0.00010460282520881866\n",
      "train: 0.2545509659326993 [291328/343145] recent lr: 0.0001050223984046578\n",
      "train: 0.2558230310678482 [297984/343145] recent lr: 0.00010544197160049694\n",
      "train: 0.2587682283841647 [304640/343145] recent lr: 0.00010586154479633608\n",
      "train: 0.25755105224939495 [311296/343145] recent lr: 0.0001062811179921752\n",
      "train: 0.25645342698464024 [317952/343145] recent lr: 0.00010670069118801434\n",
      "train: 0.2653576009548627 [324608/343145] recent lr: 0.00010712026438385348\n",
      "train: 0.2596444831444667 [331264/343145] recent lr: 0.00010753983757969262\n",
      "train: 0.263804344030527 [337920/343145] recent lr: 0.00010795941077553175\n",
      "train: 0.258742119371891 test: 0.25512058864391984 [343040/343145] recent lr: 0.0001082821593877157\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "---------- 10sData_40klr_dropout1pt5_solidOvvFet_512linsize_0.001_512 train ----------\n",
      "train: 0.2571346323404993 [ 1536/343145] recent lr: 0.00010841125883258929\n",
      "train: 0.24900531424925879 [ 8192/343145] recent lr: 0.00010883083202842843\n",
      "train: 0.25104923202441287 [14848/343145] recent lr: 0.00010925040522426757\n",
      "train: 0.25068870645302993 [21504/343145] recent lr: 0.0001096699784201067\n",
      "train: 0.2468193505819027 [28160/343145] recent lr: 0.00011008955161594584\n",
      "train: 0.2417631779725735 [34816/343145] recent lr: 0.00011050912481178497\n",
      "train: 0.25107370202357954 [41472/343145] recent lr: 0.0001109286980076241\n",
      "train: 0.25274658203125 [48128/343145] recent lr: 0.00011134827120346324\n",
      "train: 0.24848881019995764 [54784/343145] recent lr: 0.00011176784439930238\n",
      "train: 0.2528028258910546 [61440/343145] recent lr: 0.00011218741759514152\n",
      "train: 0.25159445061133456 [68096/343145] recent lr: 0.00011260699079098066\n",
      "train: 0.25061217982035416 [74752/343145] recent lr: 0.0001130265639868198\n",
      "train: 0.254999109185659 [81408/343145] recent lr: 0.00011344613718265893\n",
      "train: 0.2546483748234235 [88064/343145] recent lr: 0.00011386571037849806\n",
      "train: 0.2515922291920735 [94720/343145] recent lr: 0.0001142852835743372\n",
      "train: 0.25743095576763153 [101376/343145] recent lr: 0.00011470485677017634\n",
      "train: 0.2549807062515846 [108032/343145] recent lr: 0.00011512442996601547\n",
      "train: 0.24764530819195968 [114688/343145] recent lr: 0.00011554400316185461\n",
      "train: 0.24994359222742227 [121344/343145] recent lr: 0.00011596357635769375\n",
      "train: 0.2475152348096554 [128000/343145] recent lr: 0.00011638314955353289\n",
      "train: 0.24448960217145774 [134656/343145] recent lr: 0.00011680272274937203\n",
      "train: 0.2500045998738362 [141312/343145] recent lr: 0.00011722229594521115\n",
      "train: 0.250152329985912 [147968/343145] recent lr: 0.00011764186914105029\n",
      "train: 0.2480934216425969 [154624/343145] recent lr: 0.00011806144233688943\n",
      "train: 0.2497111719388228 [161280/343145] recent lr: 0.00011848101553272857\n",
      "train: 0.25602612587121815 [167936/343145] recent lr: 0.0001189005887285677\n",
      "train: 0.25008853238362533 [174592/343145] recent lr: 0.00011932016192440684\n",
      "train: 0.2518295359153014 [181248/343145] recent lr: 0.00011973973512024598\n",
      "train: 0.24700960173056677 [187904/343145] recent lr: 0.00012015930831608512\n",
      "train: 0.2541953061635678 [194560/343145] recent lr: 0.00012057888151192426\n",
      "train: 0.25203293103438157 [201216/343145] recent lr: 0.00012099845470776338\n",
      "train: 0.24808852947675264 [207872/343145] recent lr: 0.00012141802790360252\n",
      "train: 0.24879813538147852 [214528/343145] recent lr: 0.00012183760109944166\n",
      "train: 0.24905865926008958 [221184/343145] recent lr: 0.0001222571742952808\n",
      "train: 0.24829748387520129 [227840/343145] recent lr: 0.00012267674749111992\n",
      "train: 0.244126551426374 [234496/343145] recent lr: 0.00012309632068695906\n",
      "train: 0.24294066314513868 [241152/343145] recent lr: 0.0001235158938827982\n"
     ]
    }
   ],
   "source": [
    "# model = None\n",
    "strategyname = \"10sData_40klr_dropout1pt5_solidOvvFet_512linsize\"\n",
    "\n",
    "\n",
    "print(\"DEVICE:\", device)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "training_configs = []\n",
    "learning_rates_to_try = [1e-3]# 1e-4]\n",
    "batch_sizes_to_try = [512]#, 512]#,10000, 128]\n",
    "# input_scalings_to_try = [1000]\n",
    "# output_scalings_to_try = [10000]\n",
    "\n",
    "for learning_rate in learning_rates_to_try:\n",
    "    for batch_size in batch_sizes_to_try:\n",
    "        for use_stock_id in [False]:\n",
    "            training_configs.append({\n",
    "                'learning_rate':learning_rate,\n",
    "                'batch_size':batch_size,\n",
    "                'use_stock_id': use_stock_id\n",
    "            })\n",
    "\n",
    "epochs = 200\n",
    "for training_config in training_configs:\n",
    "    \n",
    "    learning_rate = training_config['learning_rate']\n",
    "    batch_size = training_config['batch_size']\n",
    "    use_stock_id = training_config['use_stock_id']\n",
    "    # TRAINING SETUP\n",
    "    \n",
    "    #refresh the model\n",
    "    \n",
    "    STRATEGY_NAME_WITH_ATTRS = f\"{strategyname}_{learning_rate}_{batch_size}\"\n",
    "    summary_writer = SummaryWriter(f'../output/training_tensorboard/{STRATEGY_NAME_WITH_ATTRS}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "#     model = VolatilityBSModel()\n",
    "#MultiFetGRU\n",
    "#MultiFetTransformer\n",
    "    model = MultiFetTransformer()\n",
    "    #, hidden_size=128,layers=8,dropout=0.2,features_out=128)\n",
    "# model.to(device)\n",
    "#     model = NeuralNetwork()\n",
    "#     model = StockIdEmbedding(number_of_stock_embedding_dimention=3)\n",
    "#     model\n",
    "    model.to(device)\n",
    "    optimizer_for_modes = {}\n",
    "    optimizer_scheduler_for_modes = {}\n",
    "    recent_lr = 0.001\n",
    "    feature_modes = ['logrett_xs','trade_volume_xs','trade_ordercount_xs','trade_money_turnover_per_order_xs',\n",
    "                             'logret1_xs',\n",
    "                             'book_price_spread1_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "                             'book_total_volume_xs','book_volume_imbalance_xs']\n",
    "#     feature_modes = model.get_feature_gen_train_modes()\n",
    "    done_epochs = -1\n",
    "    for modeidx, mode in enumerate(['train']):#model.get_feature_gen_train_modes()+['hidden_generator']+['hybrid']+['ultimate']):#+model.get_feature_gen_train_modes()+['hybrid']):#'train_stock_id_embedding','train']):#+['hidden_generator']*2 + feature_modes + ['hybrid']*2):\n",
    "        model.set_mode(mode)\n",
    "        \n",
    "#         batch_size\n",
    "        if mode == 'ultimate':\n",
    "            epochs = 100\n",
    "#             learning_rate = 1e-3\n",
    "#             batch_size = 1024\n",
    "        if mode in ['hybrid']:\n",
    "            epochs = 10\n",
    "#             learning_rate = 1e-3\n",
    "#             batch_size = 256\n",
    "#         if mode in model.get_feature_gen_train_modes()+['hidden_generator']:\n",
    "            \n",
    "#             epochs = 10\n",
    "#             learning_rate = 1e-3\n",
    "#             batch_size = 64\n",
    "            \n",
    "        print(f\"Current epochs: {epochs} LR : {learning_rate} batch_size: {batch_size}\")\n",
    "#             batch_size = 2\n",
    "#             learning_rate = 1e-5\n",
    "#             batch_size = 16\n",
    "#             learning_rate=1e-4\n",
    "        \n",
    "        \n",
    "#         print(model.parameters())\n",
    "#         input()\n",
    "#         continue\n",
    "        \n",
    "        if str(mode) not in optimizer_for_modes:\n",
    "#             optimizer_for_modes[str(mode)] = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "            optimizer_for_modes[str(mode)] = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "#             optimizer_scheduler_for_modes[str(mode)] = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_for_modes[str(mode)], factor=0.1, patience=10, threshold=0.0001)\n",
    "#             optimizer_for_modes[str(mode)] = torch.optim.RMSprop(model.parameters())\n",
    "#             if mode in model.get_feature_gen_train_modes():\n",
    "#                 optimizer_for_modes[str(mode)] = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        optimizer = optimizer_for_modes[str(mode)]\n",
    "#         optimizer_scheduler = optimizer_scheduler_for_modes[str(mode)]\n",
    "        \n",
    "        # TRAINING SETUP DONE\n",
    "\n",
    "        \n",
    "\n",
    "        data_ohlc_sample_len = 1 # 1 for each of open high low close\n",
    "        losses_train = []\n",
    "        step_num = 0\n",
    "        for t in range(epochs):\n",
    "            done_epochs += 1\n",
    "            \n",
    "            \n",
    "            print(f\"Epoch {done_epochs+1}\\n-------------------------------\")\n",
    "            print(\"----------\", STRATEGY_NAME_WITH_ATTRS, mode,\"----------\")\n",
    "\n",
    "            dataloader_train = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                shuffle=True, num_workers=0, pin_memory=True)\n",
    "            model.train()\n",
    "\n",
    "            for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "                step_num += 1\n",
    "                \n",
    "#                 y = scale_optiver_feature('book_realized_volatility',feature_y['target_realized_volatility']).to(device) # * realize_volatility_scale_factor \n",
    "                y = feature_y['target_realized_volatility'].to(device) * realize_volatility_scale_factor\n",
    "                pred = model(Feature_X)\n",
    "#                 pred.to(device)\n",
    "#                 print(pred)\n",
    "#                 input()\n",
    "                loss_orig = loss_fn_orig(y, pred)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss_orig.backward()\n",
    "                \n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 15**-0.5 * min(step_num**-0.5, step_num*40000**-1.5)\n",
    "                    recent_lr = param_group['lr']\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "                losses_train.append(loss_orig.item())\n",
    "\n",
    "                if (t*int(train_size/batch_size) + train_batch_idx + 1) % int(train_size/50/batch_size) == 0:\n",
    "\n",
    "                    # NOTE: real loss is same as upscaled normalized loss as it's percentage loss (rmspe)\n",
    "                    prediction_variety = np.std((pred/realize_volatility_scale_factor).reshape(-1).tolist()) * 100\n",
    "                    #NOTE: prediction variety is important as model sometimes predits a constant value! regardless of the input, then per batch variety is lowest(0 std dev)\n",
    "\n",
    "\n",
    "                    summary_writer.add_scalar(\"Prediction Variety\", prediction_variety, done_epochs*(train_size) + (train_batch_idx*batch_size))\n",
    "                    summary_writer.add_scalar(\"Training Loss\", np.mean(losses_train), done_epochs*(train_size) + (train_batch_idx*batch_size))\n",
    "\n",
    "                    print(\"train:\", np.mean(losses_train), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\", f\"recent lr: {recent_lr}\")\n",
    "                    losses_train = []\n",
    "            \n",
    "            dataloader_test = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                    shuffle=True, num_workers=0, pin_memory=True)\n",
    "            dataset_size = len(dataloader_test.dataset)\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            losses_test = []\n",
    "            for _, (Feature_X, feature_y) in enumerate(dataloader_test):\n",
    "                with torch.no_grad():\n",
    "#                     y = scale_optiver_feature('book_realized_volatility',feature_y['target_realized_volatility']).to(device) # * realize_volatility_scale_factor\n",
    "                    y = feature_y['target_realized_volatility'].to(device) * realize_volatility_scale_factor\n",
    "                    pred = model(Feature_X)\n",
    "                    loss = loss_fn_orig(y, pred)\n",
    "                    \n",
    "                    losses_test.append(loss.item())\n",
    "#             optimizer_scheduler.step(np.mean(losses_test))\n",
    "\n",
    "    #                 summary_writer.add_scalar(\"Epoch Training Loss\", np.mean(losses_train), (t+1)*train_size)\n",
    "            summary_writer.add_scalar(\"Test Loss\", np.mean(losses_test), done_epochs*(train_size) + (train_batch_idx*batch_size))\n",
    "            print(\"train:\", np.mean(losses_train), \"test:\", np.mean(losses_test), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\", f\"recent lr: {recent_lr}\")\n",
    "            losses_test = []\n",
    "            if (t+1)%30==0:\n",
    "#                 torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{loss:.4f}.pth\"))\n",
    "                # torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"13_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))\n",
    "                model_statedict = {'base':model.state_dict()}\n",
    "#                 for k,v in model.feature_gen_models.items():\n",
    "#                     model_statedict[k] = v.state_dict()\n",
    "\n",
    "                torch.save(model_statedict, os.path.join(MODEL_OUTPUT_DIRECTORY,f\"16_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93bc88db-cae6-4c72-847f-890e0a68d299",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiFetTransformer' object has no attribute 'feature_gen_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2056/226505782.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"13_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_statedict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'base'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_gen_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mmodel_statedict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m-> 1131\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultiFetTransformer' object has no attribute 'feature_gen_models'"
     ]
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"13_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))\n",
    "model_statedict = {'base':model.state_dict()}\n",
    "for k,v in model.feature_gen_models.items():\n",
    "    model_statedict[k] = v.state_dict()\n",
    "\n",
    "torch.save(model_statedict, os.path.join(MODEL_OUTPUT_DIRECTORY,f\"17_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c40a983-cebc-400b-9b8b-95c2e4874894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f978d-06d8-408d-9d32-8d0c971f2c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c865e507-c49e-401a-b561-c0c1cba9e80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>stock_id</th>\n",
       "      <th>70</th>\n",
       "      <th>75</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.811194</td>\n",
       "      <td>0.705222</td>\n",
       "      <td>0.579442</td>\n",
       "      <td>0.449084</td>\n",
       "      <td>0.813176</td>\n",
       "      <td>0.743353</td>\n",
       "      <td>0.573379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.744891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.589083</td>\n",
       "      <td>0.469144</td>\n",
       "      <td>0.732183</td>\n",
       "      <td>0.739383</td>\n",
       "      <td>0.533038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.808020</td>\n",
       "      <td>0.754178</td>\n",
       "      <td>0.580484</td>\n",
       "      <td>0.473728</td>\n",
       "      <td>0.749465</td>\n",
       "      <td>0.756734</td>\n",
       "      <td>0.525109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.760358</td>\n",
       "      <td>0.660425</td>\n",
       "      <td>0.516831</td>\n",
       "      <td>0.285894</td>\n",
       "      <td>0.710278</td>\n",
       "      <td>0.650910</td>\n",
       "      <td>0.364603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.770228</td>\n",
       "      <td>0.753502</td>\n",
       "      <td>0.611220</td>\n",
       "      <td>0.576678</td>\n",
       "      <td>0.745659</td>\n",
       "      <td>0.778682</td>\n",
       "      <td>0.663163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.576149</td>\n",
       "      <td>0.589083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474793</td>\n",
       "      <td>0.560188</td>\n",
       "      <td>0.614096</td>\n",
       "      <td>0.499395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.340020</td>\n",
       "      <td>0.469144</td>\n",
       "      <td>0.474793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.376075</td>\n",
       "      <td>0.555407</td>\n",
       "      <td>0.634929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.844946</td>\n",
       "      <td>0.732183</td>\n",
       "      <td>0.560188</td>\n",
       "      <td>0.376075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758198</td>\n",
       "      <td>0.506475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.765444</td>\n",
       "      <td>0.739383</td>\n",
       "      <td>0.614096</td>\n",
       "      <td>0.555407</td>\n",
       "      <td>0.758198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.622576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.754608</td>\n",
       "      <td>0.721407</td>\n",
       "      <td>0.589519</td>\n",
       "      <td>0.621860</td>\n",
       "      <td>0.706099</td>\n",
       "      <td>0.779024</td>\n",
       "      <td>0.685978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.870479</td>\n",
       "      <td>0.735199</td>\n",
       "      <td>0.600032</td>\n",
       "      <td>0.445861</td>\n",
       "      <td>0.825894</td>\n",
       "      <td>0.803625</td>\n",
       "      <td>0.532197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.740288</td>\n",
       "      <td>0.692165</td>\n",
       "      <td>0.533340</td>\n",
       "      <td>0.539652</td>\n",
       "      <td>0.685939</td>\n",
       "      <td>0.747160</td>\n",
       "      <td>0.595898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.820951</td>\n",
       "      <td>0.752925</td>\n",
       "      <td>0.626719</td>\n",
       "      <td>0.525911</td>\n",
       "      <td>0.791776</td>\n",
       "      <td>0.792502</td>\n",
       "      <td>0.716967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.816374</td>\n",
       "      <td>0.731109</td>\n",
       "      <td>0.570766</td>\n",
       "      <td>0.402027</td>\n",
       "      <td>0.779568</td>\n",
       "      <td>0.763643</td>\n",
       "      <td>0.542926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.745952</td>\n",
       "      <td>0.719873</td>\n",
       "      <td>0.629644</td>\n",
       "      <td>0.498651</td>\n",
       "      <td>0.741755</td>\n",
       "      <td>0.753600</td>\n",
       "      <td>0.632786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.763163</td>\n",
       "      <td>0.714092</td>\n",
       "      <td>0.562613</td>\n",
       "      <td>0.474837</td>\n",
       "      <td>0.735885</td>\n",
       "      <td>0.754644</td>\n",
       "      <td>0.622048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.911003</td>\n",
       "      <td>0.725142</td>\n",
       "      <td>0.549348</td>\n",
       "      <td>0.298335</td>\n",
       "      <td>0.832846</td>\n",
       "      <td>0.761384</td>\n",
       "      <td>0.454619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.854270</td>\n",
       "      <td>0.744462</td>\n",
       "      <td>0.602939</td>\n",
       "      <td>0.477151</td>\n",
       "      <td>0.841559</td>\n",
       "      <td>0.802465</td>\n",
       "      <td>0.571472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.880381</td>\n",
       "      <td>0.746706</td>\n",
       "      <td>0.591059</td>\n",
       "      <td>0.443440</td>\n",
       "      <td>0.810087</td>\n",
       "      <td>0.780505</td>\n",
       "      <td>0.619799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.781451</td>\n",
       "      <td>0.753629</td>\n",
       "      <td>0.601444</td>\n",
       "      <td>0.494994</td>\n",
       "      <td>0.720945</td>\n",
       "      <td>0.736164</td>\n",
       "      <td>0.565407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "stock_id        70        75        80        81        82        83        8 \n",
       "stock_id                                                                      \n",
       "74        0.811194  0.705222  0.579442  0.449084  0.813176  0.743353  0.573379\n",
       "75        0.744891  1.000000  0.589083  0.469144  0.732183  0.739383  0.533038\n",
       "76        0.808020  0.754178  0.580484  0.473728  0.749465  0.756734  0.525109\n",
       "77        0.760358  0.660425  0.516831  0.285894  0.710278  0.650910  0.364603\n",
       "78        0.770228  0.753502  0.611220  0.576678  0.745659  0.778682  0.663163\n",
       "80        0.576149  0.589083  1.000000  0.474793  0.560188  0.614096  0.499395\n",
       "81        0.340020  0.469144  0.474793  1.000000  0.376075  0.555407  0.634929\n",
       "82        0.844946  0.732183  0.560188  0.376075  1.000000  0.758198  0.506475\n",
       "83        0.765444  0.739383  0.614096  0.555407  0.758198  1.000000  0.622576\n",
       "84        0.754608  0.721407  0.589519  0.621860  0.706099  0.779024  0.685978\n",
       "85        0.870479  0.735199  0.600032  0.445861  0.825894  0.803625  0.532197\n",
       "86        0.740288  0.692165  0.533340  0.539652  0.685939  0.747160  0.595898\n",
       "87        0.820951  0.752925  0.626719  0.525911  0.791776  0.792502  0.716967\n",
       "88        0.816374  0.731109  0.570766  0.402027  0.779568  0.763643  0.542926\n",
       "89        0.745952  0.719873  0.629644  0.498651  0.741755  0.753600  0.632786\n",
       "90        0.763163  0.714092  0.562613  0.474837  0.735885  0.754644  0.622048\n",
       "93        0.911003  0.725142  0.549348  0.298335  0.832846  0.761384  0.454619\n",
       "94        0.854270  0.744462  0.602939  0.477151  0.841559  0.802465  0.571472\n",
       "95        0.880381  0.746706  0.591059  0.443440  0.810087  0.780505  0.619799\n",
       "96        0.781451  0.753629  0.601444  0.494994  0.720945  0.736164  0.565407"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p = dataset.main_df.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "corr[[70,75,80,81,82,83,8]][65:85]\n",
    "# corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "28b838c3-3aea-416a-8b20-cf2eacddb2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Learned ----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Default ---------------------\n",
      "------- set mode : hybrid -----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"----------- Learned ----------\")\n",
    "stock_ids = torch.tensor(dataset.main_df['stock_id'].unique())\n",
    "# print(stock_ids)\n",
    "\n",
    "embedding_predictor = model.hidden_generator_network.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':stock_ids})\n",
    "datapoints = pred.tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([75,70])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([76,78,82,85,87,88,94,95])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "embedding_predictor = model.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':stock_ids})\n",
    "datapoints = pred.tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([75,70])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([76,78,82,85,87,88,94,95])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()\n",
    "\n",
    "print(\"------------- Default ---------------------\")\n",
    "default_model = VolatilityBSModel().to(device)\n",
    "embedding_predictor = default_model.hidden_generator_network.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':stock_ids})\n",
    "datapoints = pred.tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([75,70])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([76,78,82,85,87,88,94,95])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "embedding_predictor = default_model.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':stock_ids})\n",
    "datapoints = pred.tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([75,70])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([76,78,82,85,87,88,94,95])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "99ec365f-201b-44f7-826c-2017b8bbc55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "50cf1a35-5d02-43e8-b242-66d082ac237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a1f5760-8926-4064-8bf2-bddd62c80bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74227704-6e5f-41fd-b9a2-242d0b8e9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optiver_custom_collate_func(batch):\n",
    "    output_x = {}\n",
    "    for k,v in batch[0][0].items():\n",
    "        output_x[k] = []\n",
    "    \n",
    "    for x_dict in [x[0] for x in batch]:\n",
    "        for k,v in x_dict.items():\n",
    "            output_x[k].append(v)\n",
    "    \n",
    "    for k,v in batch[0][0].items():\n",
    "        if type(output_x[k][0]) != str:\n",
    "            output_x[k] = torch.stack(output_x[k])\n",
    "        \n",
    "    output_y = []\n",
    "    for y in [x[1] for x in batch]:\n",
    "        output_y.append(y)\n",
    "    output_y = torch.stack(output_y)\n",
    "    \n",
    "    return (output_x, output_y)\n",
    "#     input()\n",
    "#     print(batch)\n",
    "# #     return batch\n",
    "#     input()\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82320bc2-fca3-4696-8609-6034f371e2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "17cea68b-9cf2-4a6c-8fab-d0e6b8f7654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id\n",
      "['96-13771']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_id\n",
      "tensor([96.])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds_in_bucket_xs\n",
      "tensor([[  5.,  10.,  15.,  20.,  25.,  30.,  35.,  40.,  45.,  50.,  55.,  60.,\n",
      "          65.,  70.,  75.,  80.,  85.,  90.,  95., 100., 105., 110., 115., 120.,\n",
      "         125., 130., 135., 140., 145., 150., 155., 160., 165., 170., 175., 180.,\n",
      "         185., 190., 195., 200., 205., 210., 215., 220., 225., 230., 235., 240.,\n",
      "         245., 250., 255., 260., 265., 270., 275., 280., 285., 290., 295., 300.,\n",
      "         305., 310., 315., 320., 325., 330., 335., 340., 345., 350., 355., 360.,\n",
      "         365., 370., 375., 380., 385., 390., 395., 400., 405., 410., 415., 420.,\n",
      "         425., 430., 435., 440., 445., 450., 455., 460., 465., 470., 475., 480.,\n",
      "         485., 490., 495., 500., 505., 510., 515., 520., 525., 530., 535., 540.,\n",
      "         545., 550., 555., 560., 565., 570., 575., 580., 585., 590., 595., 600.]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logrett_xs\n",
      "tensor([[-0.2583, -0.2583, -0.2583, -0.2583, -0.2565, -0.2560, -0.2566, -1.2919,\n",
      "         -0.0621, -0.0615, -0.0621, -0.0621, -0.2757, -0.2751,  2.5545, -0.3759,\n",
      "          1.5375, -1.2732, -1.4517, -0.6660, -0.6666, -0.6661, -0.0943, -0.8810,\n",
      "         -0.8745, -3.8643, -2.0211, -0.0137, -1.3876, -0.1028, -0.2772, -0.2766,\n",
      "         -2.2825, -0.0269,  1.6510, -1.3265,  2.1354,  2.1350,  0.5107,  0.5112,\n",
      "         -0.6612, -0.1995, -0.1995, -2.8131, -2.8139, -1.4733, -1.5446,  0.1518,\n",
      "          1.3928, -0.8829, -0.2122, -0.2122, -0.4472, -0.4466, -0.4472,  1.6626,\n",
      "          1.2654,  1.2647,  1.1127,  1.5446,  1.5443,  2.4143,  0.7716,  0.7715,\n",
      "         -0.4849, -0.8379, -1.7267, -0.9193, -0.5514, -0.5508, -0.2246, -0.2252,\n",
      "         -0.2252, -0.2247,  0.4583,  0.6620,  1.0186,  1.1863, -0.5597,  0.2849,\n",
      "          0.2855,  0.2849, -0.0747, -0.8219, -0.8220, -0.0287, -0.0287, -0.0287,\n",
      "          2.1707, -0.2222, -0.2222, -0.2222, -0.7705, -0.7706, -0.7707, -0.7707,\n",
      "          0.2205,  0.6614,  0.6619,  0.8823, -0.8029, -0.8023, -0.8030, -2.0030,\n",
      "         -0.0884,  0.2498,  0.2504,  0.2498, -1.9602, -2.1202, -5.4583, -0.6376,\n",
      "         -0.6383,  0.7615,  0.7620,  0.0221, -0.6903, -0.6897, -0.6903, -0.6904]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_volume_xs\n",
      "tensor([[5.3033, 0.0000, 0.0000, 5.3375, 0.0000, 0.0000, 5.3230, 5.2095, 0.0000,\n",
      "         0.0000, 0.0000, 2.1972, 0.0000, 5.1417, 4.7875, 6.5191, 4.1431, 4.8203,\n",
      "         4.6634, 0.0000, 0.0000, 6.4630, 3.0445, 2.8332, 6.4677, 4.8363, 4.5951,\n",
      "         6.3648, 4.7958, 5.5175, 0.0000, 5.3566, 6.4568, 6.1203, 5.9081, 3.2581,\n",
      "         0.0000, 6.5568, 0.0000, 0.6931, 2.1972, 0.0000, 4.6634, 0.0000, 5.7268,\n",
      "         4.3175, 1.3863, 4.9836, 0.6931, 0.6931, 0.0000, 6.2916, 0.0000, 0.0000,\n",
      "         4.6151, 6.0113, 0.0000, 0.6931, 5.3033, 0.0000, 6.1485, 4.6250, 0.0000,\n",
      "         4.6151, 6.2226, 5.3230, 2.9444, 4.6151, 0.0000, 3.0445, 0.0000, 0.0000,\n",
      "         0.0000, 5.7746, 3.6376, 0.6931, 6.0331, 1.6094, 5.2883, 0.0000, 0.0000,\n",
      "         5.7170, 4.7274, 0.0000, 4.9558, 0.0000, 0.0000, 4.9345, 4.6250, 0.0000,\n",
      "         0.0000, 5.3327, 0.0000, 0.0000, 0.0000, 1.0986, 0.6931, 0.0000, 1.0986,\n",
      "         0.6931, 0.0000, 0.0000, 5.6312, 4.6250, 5.7557, 0.0000, 0.0000, 1.0986,\n",
      "         5.7203, 4.7274, 6.2710, 0.0000, 0.6931, 0.0000, 4.9488, 4.8283, 0.0000,\n",
      "         0.0000, 0.0000, 4.6250]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_ordercount_xs\n",
      "tensor([[1.9459, 0.0000, 0.0000, 2.1972, 0.0000, 0.0000, 1.7918, 2.0794, 0.0000,\n",
      "         0.0000, 0.0000, 1.9459, 0.0000, 1.0986, 2.0794, 2.7081, 1.3863, 1.3863,\n",
      "         1.3863, 0.0000, 0.0000, 2.1972, 0.6931, 1.0986, 2.0794, 1.0986, 1.3863,\n",
      "         2.6391, 1.6094, 2.0794, 0.0000, 2.0794, 2.7081, 2.5649, 2.3979, 0.6931,\n",
      "         0.0000, 2.6391, 0.0000, 0.6931, 1.3863, 0.0000, 1.0986, 0.0000, 2.0794,\n",
      "         1.3863, 0.6931, 1.3863, 0.6931, 0.6931, 0.0000, 2.4849, 0.0000, 0.0000,\n",
      "         1.0986, 2.5649, 0.0000, 0.6931, 1.3863, 0.0000, 2.9444, 1.0986, 0.0000,\n",
      "         1.0986, 2.3979, 1.6094, 1.6094, 1.3863, 0.0000, 0.6931, 0.0000, 0.0000,\n",
      "         0.0000, 2.3979, 1.0986, 0.6931, 2.7081, 0.6931, 1.9459, 0.0000, 0.0000,\n",
      "         2.0794, 1.3863, 0.0000, 1.3863, 0.0000, 0.0000, 2.4849, 1.3863, 0.0000,\n",
      "         0.0000, 1.7918, 0.0000, 0.0000, 0.0000, 1.0986, 0.6931, 0.0000, 1.0986,\n",
      "         0.6931, 0.0000, 0.0000, 1.7918, 1.3863, 2.6391, 0.0000, 0.0000, 1.0986,\n",
      "         2.6391, 1.9459, 2.4849, 0.0000, 0.6931, 0.0000, 2.6391, 1.7918, 0.0000,\n",
      "         0.0000, 0.0000, 1.0986]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_money_turnover_xs\n",
      "tensor([[5.3024, 0.0000, 0.0000, 5.3366, 0.0000, 0.0000, 5.3220, 5.2083, 0.0000,\n",
      "         0.0000, 0.0000, 2.1962, 0.0000, 5.1404, 4.7865, 6.5181, 4.1423, 4.8193,\n",
      "         4.6623, 0.0000, 0.0000, 6.4617, 3.0432, 2.8319, 6.4662, 4.8344, 4.5930,\n",
      "         6.3626, 4.7936, 5.5152, 0.0000, 5.3543, 6.4542, 6.1178, 5.9057, 3.2557,\n",
      "         0.0000, 6.5547, 0.0000, 0.6922, 2.1954, 0.0000, 4.6614, 0.0000, 5.7242,\n",
      "         4.3147, 1.3841, 4.9807, 0.6917, 0.6917, 0.0000, 6.2886, 0.0000, 0.0000,\n",
      "         4.6121, 6.0084, 0.0000, 0.6918, 5.3008, 0.0000, 6.1462, 4.6230, 0.0000,\n",
      "         4.6133, 6.2206, 5.3210, 2.9424, 4.6129, 0.0000, 3.0423, 0.0000, 0.0000,\n",
      "         0.0000, 5.7721, 3.6353, 0.6920, 6.0309, 1.6078, 5.2861, 0.0000, 0.0000,\n",
      "         5.7150, 4.7253, 0.0000, 4.9536, 0.0000, 0.0000, 4.9322, 4.6229, 0.0000,\n",
      "         0.0000, 5.3307, 0.0000, 0.0000, 0.0000, 1.0970, 0.6920, 0.0000, 1.0971,\n",
      "         0.6921, 0.0000, 0.0000, 5.6288, 4.6224, 5.7532, 0.0000, 0.0000, 1.0969,\n",
      "         5.7175, 4.7245, 6.2677, 0.0000, 0.6913, 0.0000, 4.9453, 4.8249, 0.0000,\n",
      "         0.0000, 0.0000, 4.6213]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_money_turnover_per_order_xs\n",
      "tensor([[3.5353, 0.0000, 0.0000, 3.2903, 0.0000, 0.0000, 3.7319, 3.7785, 0.0000,\n",
      "         0.0000, 0.0000, 1.2229, 0.0000, 4.4531, 2.8894, 4.6450, 3.0749, 3.7367,\n",
      "         3.9878, 0.0000, 0.0000, 5.2162, 3.0432, 2.1959, 4.5295, 4.8344, 4.5930,\n",
      "         4.7957, 3.9296, 4.8843, 0.0000, 3.4363, 4.5166, 4.8029, 4.8006, 3.2557,\n",
      "         0.0000, 5.0585, 0.0000, 0.6922, 1.2978, 0.0000, 3.9776, 0.0000, 3.7977,\n",
      "         3.2425, 1.3841, 3.8957, 0.6917, 0.6917, 0.0000, 3.9091, 0.0000, 0.0000,\n",
      "         3.9288, 3.5501, 0.0000, 0.6918, 5.0114, 0.0000, 3.2916, 3.9396, 0.0000,\n",
      "         3.9300, 4.0547, 4.9082, 1.7030, 3.5339, 0.0000, 3.0423, 0.0000, 0.0000,\n",
      "         0.0000, 4.3371, 2.9681, 0.6920, 4.2836, 1.6078, 3.5194, 0.0000, 0.0000,\n",
      "         3.7886, 3.6443, 0.0000, 3.8690, 0.0000, 0.0000, 3.0954, 3.9492, 0.0000,\n",
      "         0.0000, 4.0278, 0.0000, 0.0000, 0.0000, 0.6919, 0.6920, 0.0000, 0.6920,\n",
      "         0.6921, 0.0000, 0.0000, 4.0336, 3.5432, 4.0198, 0.0000, 0.0000, 0.6919,\n",
      "         3.4135, 3.4472, 4.2308, 0.0000, 0.6913, 0.0000, 4.6442, 3.2470, 0.0000,\n",
      "         0.0000, 0.0000, 4.6213]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logret1_xs\n",
      "tensor([[-7.7680e-01, -7.7680e-01, -6.8623e-02,  6.7328e-01, -4.8532e-02,\n",
      "          3.7172e-01,  5.6857e-02, -2.5074e-01, -9.9375e-01,  1.7405e-01,\n",
      "          6.8433e-02,  7.0038e-01,  1.2492e-01, -1.3495e+00,  1.0666e+00,\n",
      "          5.1070e-01, -4.1751e-02, -6.4015e-03, -6.7136e-01, -2.6296e-02,\n",
      "          3.9817e-02, -9.8889e-01, -8.4128e-01, -1.1107e+00, -2.2903e+00,\n",
      "         -1.5481e+00, -1.5099e+00, -9.2775e-01, -7.4993e-01, -1.3067e+00,\n",
      "          1.8113e-01, -3.6812e-01, -2.8201e+00,  1.3158e-01,  1.6369e+00,\n",
      "          1.9151e-01,  3.7371e-01,  1.7664e+00,  1.1575e+00, -1.1125e-01,\n",
      "          7.9329e-02,  8.9109e-01, -3.4531e+00, -2.2108e+00, -1.5161e+00,\n",
      "         -5.4590e-01, -7.1532e-01,  1.3627e-01, -5.4038e-01,  9.0180e-01,\n",
      "         -7.0203e-01, -1.4106e+00, -1.5298e-01, -1.5299e-01, -1.0891e-03,\n",
      "          2.9099e+00, -2.2905e-01, -4.5718e-01,  2.1538e+00,  1.5167e-01,\n",
      "          4.0605e+00,  1.8936e-01,  5.3510e-01,  1.5238e+00,  4.5908e-01,\n",
      "         -1.1990e+00, -7.9428e-01, -1.3792e+00,  2.2514e-01, -9.8336e-01,\n",
      "          8.9667e-01, -5.0048e-01, -5.0050e-01, -9.5374e-01,  6.0001e-02,\n",
      "          2.2383e-01,  3.2025e-01,  1.1402e+00,  5.8908e-01, -1.5750e+00,\n",
      "         -3.1343e-02,  1.3332e+00,  5.6344e-01,  9.6391e-01,  3.1342e-01,\n",
      "         -3.0081e+00, -5.3374e-01, -6.4902e-01,  1.7861e+00, -1.9742e-01,\n",
      "          1.2865e+00, -3.0998e-01, -1.3622e+00,  5.5502e-01,  0.0000e+00,\n",
      "         -2.9586e+00,  1.2638e+00,  7.6171e-01,  6.1231e-01,  9.0020e-01,\n",
      "         -1.0147e-03, -1.3182e+00, -1.3183e+00, -7.5227e-01, -1.5973e+00,\n",
      "         -8.1991e-01,  1.1986e+00,  1.4711e-01, -2.0264e+00, -1.3291e+00,\n",
      "         -5.3050e+00, -2.0951e+00,  5.3387e-01, -7.8693e-02,  5.3789e-01,\n",
      "          1.9783e+00, -2.8291e+00,  1.2949e+00, -1.7899e-01, -4.8892e-02]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logret2_xs\n",
      "tensor([[ 0.9859,  0.9859, -0.1455, -1.2756,  1.5009, -0.0909, -0.0406, -0.5837,\n",
      "         -0.5796, -0.9822,  0.3893,  0.4608, -0.0628, -0.1517,  0.0359,  0.9919,\n",
      "         -0.5060,  0.0652, -1.9585, -0.5432,  0.4822, -0.3305, -0.2238,  0.0935,\n",
      "         -2.9143, -2.2891, -1.2099, -1.0210, -2.9866,  0.9696,  0.1230, -1.1505,\n",
      "         -1.7135,  0.2416,  1.8676, -0.2321,  0.1684,  1.8445, -0.0817, -1.3762,\n",
      "          1.8866,  1.9385, -2.1587, -1.1067, -4.4063, -0.5619, -0.2444, -0.4791,\n",
      "         -0.6453, -0.6869,  1.8901, -2.1611, -0.0178, -0.0178, -0.3498,  0.6256,\n",
      "          0.5493,  2.6553,  1.5744, -0.6500,  2.3321,  2.1582,  1.2279,  0.2242,\n",
      "         -0.4004, -0.2631, -2.0794, -0.5214,  0.0000,  1.0671, -0.2156, -0.0635,\n",
      "         -0.0635, -3.0902,  1.6110,  0.0915, -0.6223,  1.2058, -0.1832,  0.1907,\n",
      "         -0.1133,  1.5597,  0.1667, -1.8668, -0.6013, -0.3198,  1.0539, -1.2939,\n",
      "          2.3039,  1.6961,  0.4793, -3.0270, -1.4520,  1.6252, -0.0483, -1.6339,\n",
      "          0.6728,  1.1467, -0.3846, -0.1781,  1.4351, -0.7536, -0.7537, -1.5527,\n",
      "         -2.6237,  0.2717, -0.5195,  0.2717,  0.9765, -3.4961, -3.2493, -3.2187,\n",
      "         -0.4303,  0.1247,  1.2807,  0.0454, -2.5902,  1.0193,  0.6861, -0.4485]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_directional_volume1_xs\n",
      "tensor([[   4.2000,   70.0000,  106.3333,   34.7500,  245.0000,  212.0000,\n",
      "          303.2000,  -21.7500,  -24.3333,  -51.5000,  -70.2000,  -80.0000,\n",
      "         -127.0000,   -1.0000,   12.2500,   61.5000,   82.0000,   57.6667,\n",
      "         -217.0000, -191.0000, -199.0000, -127.8000, -184.5000, -257.2500,\n",
      "          -76.4000, -102.6000,  -63.4000,  -81.2500, -136.7500,   20.0000,\n",
      "          -77.7500, -219.5000,  -30.7500,   14.0000,   -4.5000,  -96.0000,\n",
      "           -4.0000,   64.7500,  -74.0000,  -94.6000,  -91.0000,  -99.0000,\n",
      "          -40.0000,   97.0000,   81.7500, -100.0000, -100.6667, -118.3333,\n",
      "          -33.0000,  -98.2500,  -50.0000, -121.7500,    0.0000,  -99.0000,\n",
      "          -99.0000,  -98.6000, -152.5000,    0.0000,   35.6000,  148.5000,\n",
      "          -74.5000,   76.7500,  157.0000, -159.2000, -155.4000,  -12.7500,\n",
      "          -58.2000,  -11.0000, -111.0000,  -21.6000, -104.3333,    0.0000,\n",
      "          -28.0000,  -46.2500,  -99.0000,  -98.5000,   26.7500,   -1.5000,\n",
      "          -19.2500,   88.7500,   80.5000,  111.4000,   -8.2500,  -66.0000,\n",
      "          -85.5000,  -19.6000,   -2.5000,   75.0000,  143.5000,  176.0000,\n",
      "          100.0000,  -32.2000,    2.0000,    7.0000,    7.0000,   10.0000,\n",
      "          -12.5000,  -23.0000,  -98.5000, -188.0000,  -99.0000,    0.0000,\n",
      "          -80.0000,  -24.7500,   -3.0000,    0.8000,    1.0000,    0.6667,\n",
      "           21.0000,  -49.5000,  -33.0000,  -44.0000, -137.4000, -115.0000,\n",
      "          -28.7500,  -14.6667,   11.3333,  -23.3333,  -93.2500, -123.7500]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_directional_volume2_xs\n",
      "tensor([[  17.8000,  -99.0000,  -99.0000,   26.2500,  -41.7500,    0.0000,\n",
      "           34.0000,    5.7500,  -95.6667,  -46.5000,  -89.8000,  -77.0000,\n",
      "          -44.6000,  -24.0000,  -44.0000,   51.5000,   33.0000,  -10.6667,\n",
      "           18.0000,   55.0000,   16.0000,   -8.0000, -165.7500, -162.7500,\n",
      "          -50.6000,  -72.4000, -159.6000,  -76.2500,   56.2500,   -7.5000,\n",
      "          -50.0000,   68.0000,  -71.2500,  -93.5000, -121.2500,  -68.4000,\n",
      "          -77.0000,   23.5000,  -19.5000,   78.2000,   15.6000,  -78.0000,\n",
      "         -126.6667, -101.0000,   50.5000,  -15.7500,  -48.3333,  -87.6667,\n",
      "           57.6667,  244.5000, -154.0000,  -52.7500,    0.0000,  -99.0000,\n",
      "          -59.0000,   61.6000,   38.0000, -100.0000,  -95.6000,   47.5000,\n",
      "           37.2500,  101.2500,  -39.0000,  -51.2000,    1.8000,   13.2500,\n",
      "           26.4000,    0.0000,    0.0000, -145.4000,  -91.3333,    0.0000,\n",
      "          -95.0000,   14.2500,  -60.0000,  -59.5000,  113.5000,  -10.2500,\n",
      "          105.7500,   -4.0000,   50.7500,   -2.4000,  -19.7500,  112.0000,\n",
      "          164.7500,   71.0000,   -0.5000,   55.2000,    3.2500,  -11.7500,\n",
      "          -20.0000,   78.8000,  178.0000,   -3.0000,   -2.0000,    0.0000,\n",
      "          -10.5000,  -22.0000,  -12.5000,  -13.5000, -157.0000,    0.0000,\n",
      "         -276.0000, -144.2500,   32.2500,    1.6000,   50.0000,    6.6667,\n",
      "          -96.4000,  -43.7500, -127.2500, -120.8000,  -86.8000,  -89.3333,\n",
      "          -79.7500,   15.3333,   35.3333,  -50.0000,  -13.7500,   -7.7500]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_price_spread1_xs\n",
      "tensor([[0.2467, 0.1835, 0.1762, 0.1817, 0.2643, 0.2423, 0.2291, 0.2313, 0.2644,\n",
      "         0.2313, 0.2115, 0.2159, 0.1939, 0.2644, 0.1982, 0.2863, 0.1982, 0.1909,\n",
      "         0.2129, 0.2203, 0.1982, 0.2379, 0.2754, 0.1598, 0.2645, 0.3086, 0.2337,\n",
      "         0.1764, 0.2701, 0.2371, 0.1433, 0.1709, 0.2592, 0.1930, 0.2812, 0.4852,\n",
      "         0.2315, 0.3583, 0.3032, 0.2161, 0.1852, 0.2095, 0.4852, 0.3639, 0.3033,\n",
      "         0.2869, 0.2648, 0.2059, 0.1839, 0.1986, 0.1986, 0.2428, 0.2042, 0.1655,\n",
      "         0.1765, 0.3839, 0.2923, 0.3089, 0.2912, 0.1985, 0.2426, 0.2150, 0.2425,\n",
      "         0.2160, 0.2028, 0.2976, 0.1940, 0.3308, 0.3308, 0.2823, 0.2646, 0.2536,\n",
      "         0.2426, 0.2812, 0.2647, 0.1765, 0.1599, 0.1102, 0.3583, 0.2646, 0.2316,\n",
      "         0.2249, 0.4355, 0.3528, 0.3307, 0.3573, 0.2095, 0.2250, 0.2316, 0.3087,\n",
      "         0.2205, 0.2602, 0.3307, 0.2867, 0.2867, 0.2206, 0.2206, 0.2867, 0.2867,\n",
      "         0.3308, 0.2425, 0.2536, 0.2647, 0.2702, 0.2702, 0.2603, 0.0662, 0.0882,\n",
      "         0.2162, 0.2152, 0.3423, 0.3048, 0.2739, 0.2576, 0.2484, 0.3165, 0.3239,\n",
      "         0.3092, 0.2485, 0.2264]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_price_spread2_xs\n",
      "tensor([[0.3083, 0.2569, 0.2423, 0.2863, 0.4240, 0.4185, 0.3788, 0.3689, 0.3672,\n",
      "         0.2864, 0.2732, 0.2908, 0.3172, 0.3084, 0.2808, 0.3359, 0.3480, 0.3230,\n",
      "         0.3451, 0.3304, 0.3084, 0.3305, 0.3690, 0.3250, 0.3482, 0.3880, 0.2954,\n",
      "         0.2977, 0.4190, 0.3749, 0.2977, 0.3088, 0.3584, 0.3253, 0.4631, 0.6175,\n",
      "         0.4742, 0.5071, 0.4631, 0.4145, 0.4101, 0.3748, 0.6028, 0.5072, 0.4964,\n",
      "         0.4523, 0.3751, 0.3163, 0.2795, 0.2538, 0.2647, 0.2979, 0.2704, 0.2428,\n",
      "         0.2560, 0.4457, 0.4192, 0.4853, 0.5559, 0.3639, 0.3473, 0.3693, 0.4079,\n",
      "         0.3439, 0.2998, 0.3858, 0.3704, 0.5292, 0.5292, 0.3837, 0.3528, 0.3418,\n",
      "         0.3308, 0.5128, 0.3419, 0.2647, 0.2812, 0.3252, 0.5568, 0.3749, 0.2977,\n",
      "         0.3043, 0.5181, 0.4410, 0.4576, 0.4367, 0.2702, 0.3484, 0.3969, 0.4355,\n",
      "         0.3748, 0.4587, 0.4852, 0.3970, 0.3970, 0.3970, 0.3529, 0.3970, 0.3970,\n",
      "         0.4631, 0.3749, 0.4301, 0.4853, 0.4026, 0.3971, 0.3618, 0.2868, 0.2941,\n",
      "         0.4192, 0.3531, 0.5354, 0.3755, 0.3357, 0.3018, 0.3698, 0.5152, 0.5889,\n",
      "         0.4417, 0.3147, 0.2871]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_bid_spread_xs\n",
      "tensor([[0.3964, 0.4404, 0.4404, 0.3303, 0.2752, 0.4404, 0.4404, 0.5507, 0.4405,\n",
      "         0.3304, 0.3969, 0.5289, 0.7491, 0.2202, 0.3306, 0.2206, 1.0133, 1.1015,\n",
      "         0.8815, 0.8815, 0.8815, 0.2203, 0.5510, 0.7161, 0.2203, 0.5292, 0.2204,\n",
      "         0.2204, 1.1580, 0.9374, 0.4414, 0.7718, 0.2205, 0.2757, 1.0479, 0.6619,\n",
      "         1.9852, 0.3310, 0.9923, 1.1028, 0.7498, 0.5510, 0.5145, 0.9926, 1.1034,\n",
      "         0.9933, 0.8093, 0.5885, 0.4412, 0.3309, 0.4412, 0.2758, 0.4138, 0.5519,\n",
      "         0.5742, 0.3976, 0.6624, 0.2206, 1.6769, 0.6618, 0.6062, 0.8820, 0.9924,\n",
      "         0.6174, 0.5732, 0.6612, 1.2788, 0.2205, 0.2205, 0.3969, 0.6614, 0.6614,\n",
      "         0.6614, 1.0478, 0.3308, 0.5512, 0.6614, 1.0477, 0.6066, 0.2205, 0.2205,\n",
      "         0.5732, 0.6064, 0.6617, 0.7722, 0.4854, 0.2211, 0.6616, 0.8821, 0.6062,\n",
      "         0.8823, 0.9704, 0.8824, 0.8824, 0.8824, 0.2205, 0.2205, 0.2205, 0.4416,\n",
      "         0.6615, 0.6620, 0.7724, 0.8827, 0.7172, 0.8826, 0.2650, 1.5443, 1.5443,\n",
      "         0.4854, 0.9381, 0.6624, 0.2208, 0.2208, 0.2208, 0.7176, 1.6193, 0.9574,\n",
      "         1.1046, 0.4415, 0.3864]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_ask_spread_xs\n",
      "tensor([[0.2201, 0.2937, 0.2201, 0.7155, 1.3213, 1.3213, 1.0569, 0.8260, 0.5873,\n",
      "         0.2202, 0.2202, 0.2202, 0.4847, 0.2202, 0.4958, 0.2752, 0.4844, 0.2203,\n",
      "         0.4405, 0.2202, 0.2202, 0.7049, 0.3855, 0.9361, 0.6170, 0.2644, 0.3966,\n",
      "         0.9920, 0.3306, 0.4410, 1.1027, 0.6063, 0.7722, 1.0477, 0.7716, 0.6612,\n",
      "         0.4412, 1.1571, 0.6063, 0.8821, 1.4989, 1.1017, 0.6612, 0.4408, 0.8271,\n",
      "         0.6616, 0.2944, 0.5150, 0.5150, 0.2211, 0.2205, 0.2757, 0.2482, 0.2206,\n",
      "         0.2206, 0.2205, 0.6066, 1.5440, 0.9702, 0.9923, 0.4408, 0.6612, 0.6616,\n",
      "         0.6614, 0.3967, 0.2205, 0.4848, 1.7637, 1.7637, 0.6173, 0.2204, 0.2204,\n",
      "         0.2204, 1.2680, 0.4409, 0.3307, 0.5515, 1.1026, 1.3777, 0.8820, 0.4409,\n",
      "         0.2204, 0.2203, 0.2204, 0.4960, 0.3088, 0.3857, 0.5732, 0.7713, 0.6611,\n",
      "         0.6611, 1.0142, 0.6617, 0.2204, 0.2204, 1.5439, 1.1028, 0.8823, 0.6616,\n",
      "         0.6618, 0.6618, 0.9925, 1.3233, 0.6068, 0.3860, 0.7503, 0.6621, 0.5149,\n",
      "         1.5444, 0.4411, 1.2692, 0.4860, 0.3977, 0.2211, 0.4967, 0.3677, 1.6927,\n",
      "         0.2207, 0.2207, 0.2207]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_total_volume_xs\n",
      "tensor([[7.0309, 6.2653, 6.4536, 6.9670, 7.5380, 6.1377, 7.9356, 6.8617, 5.9738,\n",
      "         6.4877, 6.7581, 6.9295, 7.1538, 4.0775, 6.9866, 8.0020, 8.0449, 7.4685,\n",
      "         6.9315, 6.6958, 6.8459, 7.1808, 7.6401, 7.7558, 6.8200, 6.8896, 7.6658,\n",
      "         6.9575, 7.2086, 7.2800, 7.4512, 7.1982, 7.6113, 7.3402, 7.2034, 7.4343,\n",
      "         5.5568, 7.4372, 6.7811, 7.0379, 7.0527, 6.1026, 6.8512, 6.1841, 6.9866,\n",
      "         6.2577, 6.9660, 6.9949, 7.1025, 7.5486, 6.0426, 7.0148, 0.0000, 6.0039,\n",
      "         7.0992, 7.2485, 7.2086, 6.2166, 7.6192, 6.4877, 7.1808, 7.4691, 6.8035,\n",
      "         7.3746, 7.6540, 6.5028, 6.6412, 4.0943, 5.0752, 6.9508, 6.5073, 0.0000,\n",
      "         6.3333, 6.6053, 5.7900, 5.8021, 7.5262, 6.8669, 7.0344, 6.6067, 6.8565,\n",
      "         7.4110, 7.0344, 6.7811, 7.0493, 6.3491, 5.3423, 7.0951, 6.8669, 6.8835,\n",
      "         5.7961, 6.9037, 5.4337, 5.4765, 5.4806, 4.7958, 4.6151, 3.9512, 5.4510,\n",
      "         6.1985, 5.7071, 0.0000, 6.1026, 7.0758, 6.1985, 5.6937, 4.7095, 3.5553,\n",
      "         7.0510, 6.7238, 6.7499, 6.7923, 7.1884, 6.5876, 6.9670, 6.7105, 6.3404,\n",
      "         6.2672, 6.3784, 6.6107]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_volume_imbalance_xs\n",
      "tensor([[4.6653, 3.4012, 2.1203, 4.1271, 5.3193, 5.3613, 5.8236, 5.0783, 4.7958,\n",
      "         4.5951, 5.0814, 5.0626, 5.1510, 3.2581, 3.8011, 4.9090, 5.0999, 4.1795,\n",
      "         5.2983, 4.9200, 5.2149, 4.9185, 5.8615, 6.0426, 4.8520, 5.1705, 5.4116,\n",
      "         5.2730, 4.4886, 4.6492, 4.8579, 5.1090, 4.6347, 5.0073, 4.8422, 5.1084,\n",
      "         4.4067, 4.9541, 4.5747, 2.8565, 4.3360, 5.1818, 5.1259, 4.6052, 4.8922,\n",
      "         4.7600, 5.0106, 5.3327, 4.7391, 4.9921, 5.3230, 5.1676, 0.0000, 5.2933,\n",
      "         5.0739, 3.6376, 4.7493, 4.6151, 4.9459, 5.2832, 4.0818, 5.1874, 4.7791,\n",
      "         5.3538, 5.0408, 3.8607, 4.2370, 2.4849, 4.7185, 5.1240, 5.2815, 0.0000,\n",
      "         4.8203, 3.9120, 5.0752, 5.0689, 4.9505, 3.3759, 4.8363, 4.4514, 4.8847,\n",
      "         4.7005, 3.4177, 4.1217, 4.5191, 4.1141, 3.5115, 4.8828, 4.9955, 5.1075,\n",
      "         4.3944, 4.7808, 5.1985, 1.6094, 1.7918, 2.4849, 3.1781, 3.8286, 4.7185,\n",
      "         5.3107, 5.5491, 0.0000, 5.8777, 5.1417, 3.4095, 3.8330, 3.9512, 2.1203,\n",
      "         4.4864, 4.5512, 5.0830, 5.1108, 5.4170, 5.3246, 4.8714, 2.1972, 4.5609,\n",
      "         4.3086, 4.6821, 4.8866]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_money_turnover1_xs\n",
      "tensor([[2.1963, 1.9449, 1.9449, 4.6624, 5.5824, 4.6141, 5.3171, 2.3016, 1.3854,\n",
      "         4.3808, 1.7908, 1.7909, 1.7909, 1.0978, 3.3663, 5.7858, 4.6624, 4.6434,\n",
      "         4.1261, 4.1421, 1.3855, 2.0784, 5.3458, 5.7607, 4.7690, 1.9444, 4.6520,\n",
      "         1.7901, 5.4909, 6.0569, 1.6077, 1.7899, 6.0472, 5.3156, 5.1156, 5.2235,\n",
      "         1.0971, 5.5032, 4.6423, 3.3303, 3.8267, 1.0973, 4.6323, 3.2165, 5.1621,\n",
      "         1.6073, 5.3153, 1.3842, 1.3842, 1.7895, 1.0968, 4.6414, 0.0000, 1.0967,\n",
      "         1.7893, 4.7069, 5.3105, 4.6124, 5.4137, 4.6610, 5.9969, 4.7938, 5.3499,\n",
      "         1.7902, 3.2942, 4.9180, 1.7901, 0.6921, 0.6921, 2.3006, 2.7706, 0.0000,\n",
      "         4.8419, 1.6076, 1.0971, 1.3846, 5.9917, 5.9767, 5.1337, 4.5304, 4.5304,\n",
      "         5.6038, 6.0190, 4.6519, 1.6079, 1.9440, 3.5532, 1.9439, 4.8421, 4.5725,\n",
      "         4.6131, 2.3960, 0.6920, 4.5517, 4.5517, 1.3844, 1.0970, 1.0971, 1.3846,\n",
      "         1.0972, 0.6921, 0.0000, 3.0888, 5.4091, 4.2879, 1.7895, 1.0969, 1.3844,\n",
      "         4.7423, 1.6071, 3.9855, 2.6357, 2.6357, 3.2154, 4.6600, 5.5181, 3.8676,\n",
      "         1.3837, 3.3288, 1.6066]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0023]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id\n",
      "['9-633']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_id\n",
      "tensor([9.])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds_in_bucket_xs\n",
      "tensor([[  5.,  10.,  15.,  20.,  25.,  30.,  35.,  40.,  45.,  50.,  55.,  60.,\n",
      "          65.,  70.,  75.,  80.,  85.,  90.,  95., 100., 105., 110., 115., 120.,\n",
      "         125., 130., 135., 140., 145., 150., 155., 160., 165., 170., 175., 180.,\n",
      "         185., 190., 195., 200., 205., 210., 215., 220., 225., 230., 235., 240.,\n",
      "         245., 250., 255., 260., 265., 270., 275., 280., 285., 290., 295., 300.,\n",
      "         305., 310., 315., 320., 325., 330., 335., 340., 345., 350., 355., 360.,\n",
      "         365., 370., 375., 380., 385., 390., 395., 400., 405., 410., 415., 420.,\n",
      "         425., 430., 435., 440., 445., 450., 455., 460., 465., 470., 475., 480.,\n",
      "         485., 490., 495., 500., 505., 510., 515., 520., 525., 530., 535., 540.,\n",
      "         545., 550., 555., 560., 565., 570., 575., 580., 585., 590., 595., 600.]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15084/2440942144.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         )\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "\n",
    "    \n",
    "dataloader_train = DataLoader(dataset, batch_size=1,\n",
    "                                shuffle=True, num_workers=0, pin_memory=False)#, collate_fn=optiver_custom_collate_func)\n",
    "\n",
    "\n",
    "#  encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "# >>> src = torch.rand(10, 32, 512)\n",
    "# >>> out = encoder_layer(src)   \n",
    "for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "    i += 1\n",
    "    for k,v in Feature_X.items():\n",
    "        print(k)\n",
    "        print(feature_transform(v,k))\n",
    "        input()\n",
    "    print(feature_y)\n",
    "    input()\n",
    "# batch = []\n",
    "# for idx in range(len(dataset)):\n",
    "#     batch.append(dataset[idx])\n",
    "#     if idx % 128 == 0:\n",
    "#         features_x = [x[0] for x in batch]\n",
    "#         features_y = [x[1] for x in batch]\n",
    "#         features_y = torch.tensor(features_y).reshape(-1,1)\n",
    "# #         print(features_y)\n",
    "# #         input()\n",
    "#         batch = []\n",
    "    \n",
    "#     y = feature_y.to(device) * output_scaling \n",
    "#     print(Feature_X['logret1_xs'].type())\n",
    "#     pred = model(Feature_X)\n",
    "#     print(pred.type())\n",
    "#     input()\n",
    "#     for stk in Feature_X['row_id']:\n",
    "        \n",
    "#         stockid.add(stk.split(\"-\")[0])\n",
    "# for i in range(len(dataset)-10):\n",
    "#     dataset[i]\n",
    "print(\"-->\", (time.time()-stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f03435-5cde-40bf-a86b-656ab4515ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce561c-5b64-44bf-882b-41f6250b1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated(device)/1024/1024/1024\n",
    "# model.to(\"cpu\")\n",
    "# torch.cuda.memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a966ee2c-8547-48be-a912-8d05bf48b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.init()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
