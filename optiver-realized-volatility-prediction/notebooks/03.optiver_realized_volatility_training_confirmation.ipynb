{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2f18e2-9894-4f0c-bd57-8b37afc02309",
   "metadata": {},
   "source": [
    "### Can our model predict current volatility?  (forget future; first it should be capable of predicting current one with given features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c4e895-fd66-490d-a8b8-d28b324d3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import types\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from optiver_features_handler import get_features_map_for_stock, get_row_id, realized_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750eb0e3-2860-490b-bc3c-2a512485a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\n",
    "OUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\n",
    "MODEL_OUTPUT_DIRECTORY = os.path.join(OUTPUT_DIRECTORY,\"models\")\n",
    "os.makedirs(OUTPUT_DIRECTORY,exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIRECTORY,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dfc95c8-8426-4ff7-9e05-319a2f0326cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_names len: 13\n",
      "sequence_feature_names = ['book_seconds_in_bucket_sum_xs', 'trade_seconds_in_bucket_sum_xs', 'logret_bid_price1_sum_xs', 'logret_ask_price1_sum_xs', 'logret_bid_price2_sum_xs', 'logret_ask_price2_sum_xs', 'logret_bid_size1_sum_xs', 'logret_ask_size1_sum_xs', 'logret_bid_size2_sum_xs', 'logret_ask_size2_sum_xs', 'logret_price_sum_xs', 'logret_size_sum_xs', 'logret_order_count_sum_xs']\n",
      "overview_feature_names len: 156\n",
      "overview_feature_names = ['book_seconds_in_bucket_sum_0', 'trade_seconds_in_bucket_sum_0', 'wap1_sum_0', 'wap1_std_0', 'wap2_sum_0', 'wap2_std_0', 'logret1_realized_volatility_0', 'logret2_realized_volatility_0', 'logret_price_realized_volatility_0', 'wap_balance_sum_0', 'wap_balance_max_0', 'price_spread1_sum_0', 'price_spread1_max_0', 'bid_spread_sum_0', 'bid_spread_max_0', 'ask_spread_sum_0', 'ask_spread_max_0', 'total_volume_sum_0', 'total_volume_max_0', 'volume_imbalance_sum_0', 'volume_imbalance_max_0', 'bid_ask_spread_sum_0', 'bid_ask_spread_max_0', 'size_sum_0', 'size_max_0', 'size_min_0', 'order_count_sum_0', 'order_count_max_0', 'size_tau_sum_0', 'size_tau_max_0', 'size_tau_min_0', 'order_count_tau_sum_0', 'order_count_tau_max_0', 'order_count_tau_min_0', 'trade_money_turnover_sum_0', 'trade_money_turnover_max_0', 'trade_money_turnover_min_0', 'directional_volume1_sum_0', 'directional_volume1_max_0', 'directional_volume1_min_0', 'directional_volume2_sum_0', 'directional_volume2_max_0', 'directional_volume2_min_0', 'logret_directional_volume1_sum_0', 'logret_directional_volume1_max_0', 'logret_directional_volume1_min_0', 'logret_directional_volume2_sum_0', 'logret_directional_volume2_max_0', 'logret_directional_volume2_min_0', 'trade_price_push_on_book_sum_0', 'trade_price_push_on_book_max_0', 'trade_price_push_on_book_min_0', 'book_seconds_in_bucket_sum_300', 'trade_seconds_in_bucket_sum_300', 'wap1_sum_300', 'wap1_std_300', 'wap2_sum_300', 'wap2_std_300', 'logret1_realized_volatility_300', 'logret2_realized_volatility_300', 'logret_price_realized_volatility_300', 'wap_balance_sum_300', 'wap_balance_max_300', 'price_spread1_sum_300', 'price_spread1_max_300', 'bid_spread_sum_300', 'bid_spread_max_300', 'ask_spread_sum_300', 'ask_spread_max_300', 'total_volume_sum_300', 'total_volume_max_300', 'volume_imbalance_sum_300', 'volume_imbalance_max_300', 'bid_ask_spread_sum_300', 'bid_ask_spread_max_300', 'size_sum_300', 'size_max_300', 'size_min_300', 'order_count_sum_300', 'order_count_max_300', 'size_tau_sum_300', 'size_tau_max_300', 'size_tau_min_300', 'order_count_tau_sum_300', 'order_count_tau_max_300', 'order_count_tau_min_300', 'trade_money_turnover_sum_300', 'trade_money_turnover_max_300', 'trade_money_turnover_min_300', 'directional_volume1_sum_300', 'directional_volume1_max_300', 'directional_volume1_min_300', 'directional_volume2_sum_300', 'directional_volume2_max_300', 'directional_volume2_min_300', 'logret_directional_volume1_sum_300', 'logret_directional_volume1_max_300', 'logret_directional_volume1_min_300', 'logret_directional_volume2_sum_300', 'logret_directional_volume2_max_300', 'logret_directional_volume2_min_300', 'trade_price_push_on_book_sum_300', 'trade_price_push_on_book_max_300', 'trade_price_push_on_book_min_300', 'book_seconds_in_bucket_sum_450', 'trade_seconds_in_bucket_sum_450', 'wap1_sum_450', 'wap1_std_450', 'wap2_sum_450', 'wap2_std_450', 'logret1_realized_volatility_450', 'logret2_realized_volatility_450', 'logret_price_realized_volatility_450', 'wap_balance_sum_450', 'wap_balance_max_450', 'price_spread1_sum_450', 'price_spread1_max_450', 'bid_spread_sum_450', 'bid_spread_max_450', 'ask_spread_sum_450', 'ask_spread_max_450', 'total_volume_sum_450', 'total_volume_max_450', 'volume_imbalance_sum_450', 'volume_imbalance_max_450', 'bid_ask_spread_sum_450', 'bid_ask_spread_max_450', 'size_sum_450', 'size_max_450', 'size_min_450', 'order_count_sum_450', 'order_count_max_450', 'size_tau_sum_450', 'size_tau_max_450', 'size_tau_min_450', 'order_count_tau_sum_450', 'order_count_tau_max_450', 'order_count_tau_min_450', 'trade_money_turnover_sum_450', 'trade_money_turnover_max_450', 'trade_money_turnover_min_450', 'directional_volume1_sum_450', 'directional_volume1_max_450', 'directional_volume1_min_450', 'directional_volume2_sum_450', 'directional_volume2_max_450', 'directional_volume2_min_450', 'logret_directional_volume1_sum_450', 'logret_directional_volume1_max_450', 'logret_directional_volume1_min_450', 'logret_directional_volume2_sum_450', 'logret_directional_volume2_max_450', 'logret_directional_volume2_min_450', 'trade_price_push_on_book_sum_450', 'trade_price_push_on_book_max_450', 'trade_price_push_on_book_min_450']\n"
     ]
    }
   ],
   "source": [
    "data_interval_seconds = 5\n",
    "data_intervals_count = int(600/data_interval_seconds)\n",
    "\n",
    "sequence_essential_feature_names = ['sequence_mask_xs','seconds_in_bucket_xs','has_trade_data_xs']\n",
    "\n",
    "fetandagglist = [\n",
    "            (['book_seconds_in_bucket','trade_seconds_in_bucket'],\n",
    "             ['sum']),\n",
    "            (['logret_bid_price1','logret_ask_price1','logret_bid_price2','logret_ask_price2','logret_bid_size1', 'logret_ask_size1','logret_bid_size2','logret_ask_size2','logret_price','logret_size','logret_order_count'],\n",
    "             ['sum']),\n",
    "        ]\n",
    "sequence_feature_names = []\n",
    "for fetlist, agglist in fetandagglist:\n",
    "    for feature_name in fetlist:\n",
    "        for agg in agglist:\n",
    "            sequence_feature_names.append(f'{feature_name}_{agg}_xs')\n",
    "print('feature_names len:',len(sequence_feature_names))\n",
    "print('sequence_feature_names','=',sequence_feature_names)\n",
    "\n",
    "\n",
    "overview_feature_names = []\n",
    "seconds_in_bucket_gropus = [0,300,450]\n",
    "overview_aggregations = {\n",
    "            'book_seconds_in_bucket': ['sum'],\n",
    "            'trade_seconds_in_bucket': ['sum'],\n",
    "            'wap1': ['sum', 'std'],\n",
    "            'wap2': ['sum', 'std'],\n",
    "            'logret1': [realized_volatility],\n",
    "            'logret2': [realized_volatility],\n",
    "            'logret_price': [realized_volatility],\n",
    "            'wap_balance': ['sum', 'max'],\n",
    "            'price_spread1': ['sum', 'max'],\n",
    "            'bid_spread': ['sum', 'max'],\n",
    "            'ask_spread': ['sum', 'max'],\n",
    "            'total_volume': ['sum', 'max'],\n",
    "            'volume_imbalance': ['sum', 'max'],\n",
    "            \"bid_ask_spread\": ['sum', 'max'],\n",
    "            'size':  ['sum', 'max','min'],\n",
    "            'order_count': ['sum', 'max'],\n",
    "            'size_tau':  ['sum', 'max','min'],\n",
    "            'order_count_tau':  ['sum', 'max','min'],\n",
    "            'trade_money_turnover': ['sum', 'max','min'],\n",
    "            'directional_volume1': ['sum','max','min'],\n",
    "            'directional_volume2': ['sum','max','min'],\n",
    "            'logret_directional_volume1': ['sum','max','min'],\n",
    "            'logret_directional_volume2': ['sum','max','min'],\n",
    "            'trade_price_push_on_book': ['sum','max','min']\n",
    "        }\n",
    "for seconds_in_bucket_group in seconds_in_bucket_gropus:     \n",
    "    for key, aggs in overview_aggregations.items():\n",
    "        for agg in aggs:\n",
    "            if isinstance(agg, types.FunctionType):\n",
    "                agg = agg.__name__\n",
    "            overview_feature_names.append(f'{key}_{agg}_{seconds_in_bucket_group}')\n",
    "print('overview_feature_names len:',len(overview_feature_names))\n",
    "print('overview_feature_names','=',overview_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e02947ab-aa1d-4af0-9d6e-7a51cff159ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OptiverRealizedVolatilityDataset(Dataset):\n",
    "    def __init__(self, data_directory, mode=\"train\", lazy_load=True):\n",
    "        \"\"\"initializes Optiver Competition dataset\n",
    "        `mode`: train|test\n",
    "        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n",
    "        \"\"\"\n",
    "        print(\"INIT: OptiverRealizedVolatilityDataset\")\n",
    "        if mode.lower() not in ['train','test']:\n",
    "            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n",
    "        self.data_directory = data_directory\n",
    "        self.mode = mode.lower()\n",
    "        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n",
    "#         if self.mode == 'train':\n",
    "#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n",
    "        if self.mode == 'test':\n",
    "            self.main_df['target'] = 0\n",
    "        \n",
    "        self.cache_stocks_done_set = set()\n",
    "        # this is our final features lookup where we park all our features which can be addressed by row_id\n",
    "        # which is individual train/test.csv row id using 'stock_id`-`time_id`\n",
    "        self.cache_rowid_feature_map = {}\n",
    "        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n",
    "        targets = self.main_df['target'].tolist()\n",
    "        self.stock_possible_timeids_list = {}\n",
    "        for idx, row_id in enumerate(row_id_series.tolist()):\n",
    "            stock_id = int(row_id.split('-')[0])\n",
    "            time_id = int(row_id.split('-')[1])\n",
    "            self.cache_rowid_feature_map[row_id] = {'target_realized_volatility':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n",
    "            \n",
    "            # below code is to make sure what timeids we expect from stock data extractor\n",
    "            # in case of missing parquet files we'll have to know the keys to fill default values into\n",
    "            if stock_id not in self.stock_possible_timeids_list:\n",
    "                self.stock_possible_timeids_list[stock_id] = []\n",
    "            self.stock_possible_timeids_list[stock_id].append(time_id)\n",
    "            \n",
    "        \n",
    "        if lazy_load == False:\n",
    "            worker_data = []\n",
    "            for gkey, gdf in self.main_df.groupby(['stock_id']):\n",
    "                worker_data.append((self.data_directory, self.mode, gkey))\n",
    "#             print(\"---------- CPU COUNG:\", multiprocessing.cpu_count())\n",
    "            # NOTE: this was hell of a hunt; this windows and pytorch and jupyter combination is too tedious\n",
    "            #       make sure the function that we distribute don't call pytorch\n",
    "            chunksize = multiprocessing.cpu_count() * 1\n",
    "            processed = 0\n",
    "            for worker_data_chunk in [worker_data[i * chunksize:(i + 1) * chunksize] for i in range((len(worker_data) + chunksize - 1) // chunksize )]:\n",
    "                with Pool(multiprocessing.cpu_count()) as p:\n",
    "                    \n",
    "                    feature_set_list = p.starmap(get_features_map_for_stock, worker_data_chunk)\n",
    "                    \n",
    "                    for feature_map in feature_set_list:\n",
    "                        for rowid, features_dict in feature_map.items():\n",
    "                            for fkey,fval in features_dict.items():\n",
    "                                self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                            self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                        # udpate the indications that we've already fetched this stock and the lazy loader code won't fetch this again\n",
    "                        self.cache_stocks_done_set.add(int(rowid.split('-')[0]))\n",
    "                    \n",
    "                    processed += chunksize\n",
    "                    print(f\"Processed and loaded {processed} stocks features.\")\n",
    "    \n",
    "    def __cache_generate_features(self, main_stock_id, main_time_id):\n",
    "            \n",
    "            main_row_id = get_row_id(main_stock_id, main_time_id)\n",
    "            if main_stock_id not in self.cache_stocks_done_set:\n",
    "#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))   \n",
    "                # we'll combine the featureset with the bigger feature set of all stocks\n",
    "                feature_map = get_features_map_for_stock(self.data_directory, self.mode, main_stock_id)\n",
    "                # NOTE: sometime we might now have parquet files in that case we'll have 3 entried in .csv while only 1 gets returned in feature map\n",
    "                # we need to cover for that disparity\n",
    "                for time_id in self.stock_possible_timeids_list[main_stock_id]:\n",
    "                    expected_row_id = get_row_id(main_stock_id, time_id)\n",
    "                    if expected_row_id not in feature_map:\n",
    "                        feature_map[expected_row_id] = {}\n",
    "                for rowid, features_dict in feature_map.items():\n",
    "                    for fkey,fval in features_dict.items():\n",
    "                        self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                    self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                self.cache_stocks_done_set.add(main_stock_id)\n",
    "#             print(self.cache_rowid_feature_map[main_row_id])\n",
    "#             print(torch.tensor([self.cache_rowid_feature_map[main_row_id].get('book_realized_volatility',0)]))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map[main_row_id].get('log_return1_2s', [0]*(int(600/2)))))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map.get('book_directional_volume1_2s', [0]*(int(600/2)))))\n",
    "            return self.cache_rowid_feature_map[main_row_id]\n",
    "        \n",
    "    @staticmethod\n",
    "    def transform_to_01_realized_volatility_linear_data(features_dict):\n",
    "        feature_x  = {\n",
    "                    'row_id':features_dict['row_id'],\n",
    "                    'stock_id':torch.tensor(features_dict['stock_id'], dtype=torch.float32),\n",
    "                    'sequence_mask_xs': torch.tensor(features_dict.get('sequence_mask_xs', [False]+[True]*(int(600/data_interval_seconds)-1)), dtype=torch.bool),\n",
    "                    'seconds_in_bucket_xs': torch.tensor(features_dict.get('seconds_in_bucket_xs', [(idx) for idx in range(0,int(data_intervals_count))]), dtype=torch.float32),\n",
    "                    'has_trade_data_xs': torch.tensor(features_dict.get('has_trade_data_xs', [0]*(int(600/data_interval_seconds))), dtype=torch.float32),\n",
    "                }\n",
    "\n",
    "        for feature_name in sequence_feature_names:\n",
    "            feature_x[feature_name] = torch.tensor(features_dict.get(feature_name, [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "\n",
    "\n",
    "        for feature_name in overview_feature_names:\n",
    "            feature_x[feature_name] = torch.tensor(features_dict.get(feature_name, -0.01), dtype=torch.float32)\n",
    "                \n",
    "\n",
    "\n",
    "        return (\n",
    "                feature_x,\n",
    "                {'target_realized_volatility':torch.tensor([features_dict['target_realized_volatility']])}\n",
    "        #                 [features_dict['target']]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: handle for num_workers more than 0\n",
    "        #      using https://pytorch.org/docs/stable/data.html\n",
    "        #      using torch.util.data.get_worker_info()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        stock_id = self.main_df.at[idx, 'stock_id']\n",
    "        time_id = self.main_df.at[idx, 'time_id']\n",
    "        x,y = self.__cache_generate_features(stock_id,time_id)\n",
    "#         x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efcee691-c9ce-481f-a4e8-dbf97b66b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT: OptiverRealizedVolatilityDataset\n",
      "Processed and loaded 16 stocks features.\n",
      "Processed and loaded 32 stocks features.\n",
      "Processed and loaded 48 stocks features.\n",
      "Processed and loaded 64 stocks features.\n",
      "Processed and loaded 80 stocks features.\n",
      "Processed and loaded 96 stocks features.\n",
      "Processed and loaded 112 stocks features.\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\", lazy_load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d085c-aaed-4a28-9d44-93eb8e2d4e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in range(0,9):\n",
    "#     print(dataset[x])\n",
    "\n",
    "# dataset[10000] #[0]['bidp1_1s']\n",
    "for key,val in dataset[10000][0].items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eaeb87d1-d4be-4351-b483-22ac0c168909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_idx in range(len(dataset)):\n",
    "#     dataset[data_idx][0]['book_realized_volatility'] = dataset[data_idx][0]['book_realized_volatility'].type(torch.float32).to('cpu')\n",
    "#     print(dataset[data_idx][0]['book_realized_volatility'])\n",
    "#     input()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ec3227-ffc6-4099-b51c-04a3bd2df633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stock_id': {'mean': 62.43794250488281, 'std': 37.12644958496094},\n",
       " 'seconds_in_bucket_xs': {'mean': 54.860660552978516,\n",
       "  'std': 36.77859115600586},\n",
       " 'has_trade_data_xs': {'mean': 0.9256505966186523, 'std': 0.26233869791030884},\n",
       " 'book_seconds_in_bucket_sum_xs': {'mean': 3.2410800457000732,\n",
       "  'std': 1.6307694911956787},\n",
       " 'trade_seconds_in_bucket_sum_xs': {'mean': 0.7438617944717407,\n",
       "  'std': 1.050882339477539},\n",
       " 'logret_bid_price1_sum_xs': {'mean': -1.6512319289674338e-09,\n",
       "  'std': 0.0005907019949518144},\n",
       " 'logret_ask_price1_sum_xs': {'mean': -2.367119167345777e-09,\n",
       "  'std': 0.000591019110288471},\n",
       " 'logret_bid_price2_sum_xs': {'mean': -1.6668815217002475e-09,\n",
       "  'std': 0.0006065901252441108},\n",
       " 'logret_ask_price2_sum_xs': {'mean': -2.3287849426623097e-09,\n",
       "  'std': 0.0006076738936826587},\n",
       " 'logret_bid_size1_sum_xs': {'mean': 2.0847835457971087e-06,\n",
       "  'std': 1.4804167747497559},\n",
       " 'logret_ask_size1_sum_xs': {'mean': 2.521141823308426e-06,\n",
       "  'std': 1.455901026725769},\n",
       " 'logret_bid_size2_sum_xs': {'mean': 1.7478965901318588e-06,\n",
       "  'std': 1.3515498638153076},\n",
       " 'logret_ask_size2_sum_xs': {'mean': 2.5673455184005434e-06,\n",
       "  'std': 1.3213202953338623},\n",
       " 'logret_price_sum_xs': {'mean': -2.3343620370042117e-09,\n",
       "  'std': 0.0005715438746847212},\n",
       " 'logret_size_sum_xs': {'mean': -2.419666316200164e-07,\n",
       "  'std': 1.587930679321289},\n",
       " 'logret_order_count_sum_xs': {'mean': -1.0728214192567975e-06,\n",
       "  'std': 0.7549796104431152},\n",
       " 'book_seconds_in_bucket_sum_0': {'mean': 388.9295959472656,\n",
       "  'std': 135.8336944580078},\n",
       " 'trade_seconds_in_bucket_sum_0': {'mean': 89.26342010498047,\n",
       "  'std': 82.71247100830078},\n",
       " 'wap1_sum_0': {'mean': 389.92791748046875, 'std': 135.835205078125},\n",
       " 'wap1_std_0': {'mean': 0.0011102678254246712, 'std': 0.0010516541078686714},\n",
       " 'wap2_sum_0': {'mean': 389.9277038574219, 'std': 135.83547973632812},\n",
       " 'wap2_std_0': {'mean': 0.0011489872122183442, 'std': 0.0010650980984792113},\n",
       " 'logret1_realized_volatility_0': {'mean': 0.005850940477102995,\n",
       "  'std': 0.004778958857059479},\n",
       " 'logret2_realized_volatility_0': {'mean': 0.007210279814898968,\n",
       "  'std': 0.005797804333269596},\n",
       " 'logret_price_realized_volatility_0': {'mean': 0.004610072355717421,\n",
       "  'std': 0.004089121241122484},\n",
       " 'wap_balance_sum_0': {'mean': 0.09234429150819778,\n",
       "  'std': 0.08365236222743988},\n",
       " 'wap_balance_max_0': {'mean': 0.0010880399495363235,\n",
       "  'std': 0.0012415354140102863},\n",
       " 'price_spread1_sum_0': {'mean': 0.2226376235485077,\n",
       "  'std': 0.20077848434448242},\n",
       " 'price_spread1_max_0': {'mean': 0.0014055456267669797,\n",
       "  'std': 0.001588768558576703},\n",
       " 'bid_spread_sum_0': {'mean': 0.07556163519620895, 'std': 0.06961646676063538},\n",
       " 'bid_spread_max_0': {'mean': 0.0007034957525320351,\n",
       "  'std': 0.0008316159946843982},\n",
       " 'ask_spread_sum_0': {'mean': 0.07635528594255447, 'std': 0.06991078704595566},\n",
       " 'ask_spread_max_0': {'mean': 0.0007134961779229343,\n",
       "  'std': 0.0008447545696981251},\n",
       " 'total_volume_sum_0': {'mean': 1629942.5, 'std': 9067553.0},\n",
       " 'total_volume_max_0': {'mean': 6443.4765625, 'std': 27511.509765625},\n",
       " 'volume_imbalance_sum_0': {'mean': 396575.46875, 'std': 2442117.5},\n",
       " 'volume_imbalance_max_0': {'mean': 3689.99755859375, 'std': 13807.1123046875},\n",
       " 'bid_ask_spread_sum_0': {'mean': 0.038354359567165375,\n",
       "  'std': 0.050127506256103516},\n",
       " 'bid_ask_spread_max_0': {'mean': 0.000663700804580003,\n",
       "  'std': 0.0009456104598939419},\n",
       " 'size_sum_0': {'mean': 31860.21484375, 'std': 70259.2109375},\n",
       " 'size_max_0': {'mean': 3035.258544921875, 'std': 8191.23974609375},\n",
       " 'size_min_0': {'mean': 6.389898777008057, 'std': 352.6045227050781},\n",
       " 'order_count_sum_0': {'mean': 373.43682861328125, 'std': 608.5436401367188},\n",
       " 'order_count_max_0': {'mean': 26.246469497680664, 'std': 50.72904968261719},\n",
       " 'size_tau_sum_0': {'mean': 7.7889628410339355, 'std': 6.480547904968262},\n",
       " 'size_tau_max_0': {'mean': 0.917707085609436, 'std': 0.23625372350215912},\n",
       " 'size_tau_min_0': {'mean': 0.001273703295737505, 'std': 0.004305623937398195},\n",
       " 'order_count_tau_sum_0': {'mean': 46.07876205444336,\n",
       "  'std': 37.05854415893555},\n",
       " 'order_count_tau_max_0': {'mean': 0.9990649223327637,\n",
       "  'std': 0.025059882551431656},\n",
       " 'order_count_tau_min_0': {'mean': 0.06630133837461472,\n",
       "  'std': 0.0399792343378067},\n",
       " 'trade_money_turnover_sum_0': {'mean': 31859.123046875, 'std': 70262.2265625},\n",
       " 'trade_money_turnover_max_0': {'mean': 3035.2578125, 'std': 8194.078125},\n",
       " 'trade_money_turnover_min_0': {'mean': 6.387502670288086,\n",
       "  'std': 352.42071533203125},\n",
       " 'directional_volume1_sum_0': {'mean': 2020.0023193359375, 'std': 1096721.0},\n",
       " 'directional_volume1_max_0': {'mean': 2131.2041015625,\n",
       "  'std': 9154.2041015625},\n",
       " 'directional_volume1_min_0': {'mean': -2176.4326171875,\n",
       "  'std': 8616.134765625},\n",
       " 'directional_volume2_sum_0': {'mean': 13685.599609375, 'std': 1445586.875},\n",
       " 'directional_volume2_max_0': {'mean': 1842.190673828125,\n",
       "  'std': 7931.50048828125},\n",
       " 'directional_volume2_min_0': {'mean': -1858.739013671875,\n",
       "  'std': 7222.1728515625},\n",
       " 'logret_directional_volume1_sum_0': {'mean': -1.8199473572622082e-07,\n",
       "  'std': 0.028246624395251274},\n",
       " 'logret_directional_volume1_max_0': {'mean': 0.04359145835042,\n",
       "  'std': 0.15943863987922668},\n",
       " 'logret_directional_volume1_min_0': {'mean': -0.04361077770590782,\n",
       "  'std': 0.1639694720506668},\n",
       " 'logret_directional_volume2_sum_0': {'mean': 1.47040069009563e-07,\n",
       "  'std': 0.03417246416211128},\n",
       " 'logret_directional_volume2_max_0': {'mean': 0.034739550203084946,\n",
       "  'std': 0.1626690775156021},\n",
       " 'logret_directional_volume2_min_0': {'mean': -0.0347997285425663,\n",
       "  'std': 0.16696001589298248},\n",
       " 'trade_price_push_on_book_sum_0': {'mean': -1.7589563867659308e-05,\n",
       "  'std': 0.0011976135428994894},\n",
       " 'trade_price_push_on_book_max_0': {'mean': 0.00011602201266214252,\n",
       "  'std': 0.0001294823014177382},\n",
       " 'trade_price_push_on_book_min_0': {'mean': -0.00011783508671214804,\n",
       "  'std': 0.0001304793986491859},\n",
       " 'book_seconds_in_bucket_sum_300': {'mean': 192.7294464111328,\n",
       "  'std': 69.91691589355469},\n",
       " 'trade_seconds_in_bucket_sum_300': {'mean': 44.1748161315918,\n",
       "  'std': 41.352291107177734},\n",
       " 'wap1_sum_300': {'mean': 192.72744750976562, 'std': 69.91730499267578},\n",
       " 'wap1_std_300': {'mean': 0.0007740700384601951, 'std': 0.0007044852827675641},\n",
       " 'wap2_sum_300': {'mean': 192.72735595703125, 'std': 69.91746520996094},\n",
       " 'wap2_std_300': {'mean': 0.0008196851122193038, 'std': 0.0007237914251163602},\n",
       " 'logret1_realized_volatility_300': {'mean': 0.0028540410567075014,\n",
       "  'std': 0.0023584188893437386},\n",
       " 'logret2_realized_volatility_300': {'mean': 0.003916623070836067,\n",
       "  'std': 0.003286789869889617},\n",
       " 'logret_price_realized_volatility_300': {'mean': 0.0018293253378942609,\n",
       "  'std': 0.0014226160710677505},\n",
       " 'wap_balance_sum_300': {'mean': 0.04406936839222908,\n",
       "  'std': 0.0395335778594017},\n",
       " 'wap_balance_max_300': {'mean': 0.0009255553013645113,\n",
       "  'std': 0.0009595266892574728},\n",
       " 'price_spread1_sum_300': {'mean': 0.10654828697443008,\n",
       "  'std': 0.09531298279762268},\n",
       " 'price_spread1_max_300': {'mean': 0.001214938354678452,\n",
       "  'std': 0.001255062990821898},\n",
       " 'bid_spread_sum_300': {'mean': 0.036845091730356216,\n",
       "  'std': 0.034686002880334854},\n",
       " 'bid_spread_max_300': {'mean': 0.0005883006961084902,\n",
       "  'std': 0.0006275468622334301},\n",
       " 'ask_spread_sum_300': {'mean': 0.037220779806375504,\n",
       "  'std': 0.03482009097933769},\n",
       " 'ask_spread_max_300': {'mean': 0.0005957503453828394,\n",
       "  'std': 0.0006389436312019825},\n",
       " 'total_volume_sum_300': {'mean': 820102.0625, 'std': 4566595.0},\n",
       " 'total_volume_max_300': {'mean': 5717.91162109375, 'std': 25948.44140625},\n",
       " 'volume_imbalance_sum_300': {'mean': 198661.75, 'std': 1222840.625},\n",
       " 'volume_imbalance_max_300': {'mean': 2998.133544921875,\n",
       "  'std': 11909.1279296875},\n",
       " 'bid_ask_spread_sum_300': {'mean': 0.01812143065035343,\n",
       "  'std': 0.02387070842087269},\n",
       " 'bid_ask_spread_max_300': {'mean': 0.0005402453825809062,\n",
       "  'std': 0.0007164308335632086},\n",
       " 'size_sum_300': {'mean': 15430.2470703125, 'std': 34412.78125},\n",
       " 'size_max_300': {'mean': 2164.879638671875, 'std': 6209.435546875},\n",
       " 'size_min_300': {'mean': 14.42845630645752, 'std': 500.7431640625},\n",
       " 'order_count_sum_300': {'mean': 181.84652709960938, 'std': 298.0603332519531},\n",
       " 'order_count_max_300': {'mean': 19.513988494873047, 'std': 36.19590759277344},\n",
       " 'size_tau_sum_300': {'mean': 3.894740343093872, 'std': 3.503800868988037},\n",
       " 'size_tau_max_300': {'mean': 0.8218374848365784, 'std': 0.3298044204711914},\n",
       " 'size_tau_min_300': {'mean': 0.002340120729058981,\n",
       "  'std': 0.019352799281477928},\n",
       " 'order_count_tau_sum_300': {'mean': 22.86751365661621,\n",
       "  'std': 18.809871673583984},\n",
       " 'order_count_tau_max_300': {'mean': 0.9939700365066528,\n",
       "  'std': 0.06370208412408829},\n",
       " 'order_count_tau_min_300': {'mean': 0.0875999927520752,\n",
       "  'std': 0.06473039090633392},\n",
       " 'trade_money_turnover_sum_300': {'mean': 15429.447265625,\n",
       "  'std': 34409.23828125},\n",
       " 'trade_money_turnover_max_300': {'mean': 2164.802734375, 'std': 6210.3203125},\n",
       " 'trade_money_turnover_min_300': {'mean': 14.426054954528809,\n",
       "  'std': 500.55487060546875},\n",
       " 'directional_volume1_sum_300': {'mean': 1529.6800537109375,\n",
       "  'std': 628742.8125},\n",
       " 'directional_volume1_max_300': {'mean': 1727.3031005859375,\n",
       "  'std': 7824.5888671875},\n",
       " 'directional_volume1_min_300': {'mean': -1741.162353515625,\n",
       "  'std': 7332.9716796875},\n",
       " 'directional_volume2_sum_300': {'mean': 6554.21142578125, 'std': 745032.0},\n",
       " 'directional_volume2_max_300': {'mean': 1405.59716796875,\n",
       "  'std': 6610.2470703125},\n",
       " 'directional_volume2_min_300': {'mean': -1386.0255126953125,\n",
       "  'std': 5760.64990234375},\n",
       " 'logret_directional_volume1_sum_300': {'mean': 4.345632260083221e-05,\n",
       "  'std': 0.028431734070181847},\n",
       " 'logret_directional_volume1_max_300': {'mean': 0.03356919065117836,\n",
       "  'std': 0.1125909835100174},\n",
       " 'logret_directional_volume1_min_300': {'mean': -0.033594489097595215,\n",
       "  'std': 0.11414666473865509},\n",
       " 'logret_directional_volume2_sum_300': {'mean': 1.1709575119311921e-05,\n",
       "  'std': 0.039300017058849335},\n",
       " 'logret_directional_volume2_max_300': {'mean': 0.025390710681676865,\n",
       "  'std': 0.10197589546442032},\n",
       " 'logret_directional_volume2_min_300': {'mean': -0.025465641170740128,\n",
       "  'std': 0.10425708442926407},\n",
       " 'trade_price_push_on_book_sum_300': {'mean': -7.437845852109604e-06,\n",
       "  'std': 0.0006747114821337163},\n",
       " 'trade_price_push_on_book_max_300': {'mean': 8.800736395642161e-05,\n",
       "  'std': 0.000252813333645463},\n",
       " 'trade_price_push_on_book_min_300': {'mean': -9.999539906857535e-05,\n",
       "  'std': 0.0002493471256457269},\n",
       " 'book_seconds_in_bucket_sum_450': {'mean': 95.99272918701172,\n",
       "  'std': 35.80431365966797},\n",
       " 'trade_seconds_in_bucket_sum_450': {'mean': 22.018674850463867,\n",
       "  'std': 20.96971893310547},\n",
       " 'wap1_sum_450': {'mean': 95.99224090576172, 'std': 35.80501937866211},\n",
       " 'wap1_std_450': {'mean': 0.0005554135423153639, 'std': 0.0004982542595826089},\n",
       " 'wap2_sum_450': {'mean': 95.99219512939453, 'std': 35.80509948730469},\n",
       " 'wap2_std_450': {'mean': 0.0006081690080463886, 'std': 0.0005254594143480062},\n",
       " 'logret1_realized_volatility_450': {'mean': 0.0019678790122270584,\n",
       "  'std': 0.0016546284314244986},\n",
       " 'logret2_realized_volatility_450': {'mean': 0.0027031346689909697,\n",
       "  'std': 0.002307959133759141},\n",
       " 'logret_price_realized_volatility_450': {'mean': 0.0012547716032713652,\n",
       "  'std': 0.0010070931166410446},\n",
       " 'wap_balance_sum_450': {'mean': 0.021708102896809578,\n",
       "  'std': 0.020036635920405388},\n",
       " 'wap_balance_max_450': {'mean': 0.0008153317030519247,\n",
       "  'std': 0.00083634298061952},\n",
       " 'price_spread1_sum_450': {'mean': 0.0524955578148365,\n",
       "  'std': 0.04763847589492798},\n",
       " 'price_spread1_max_450': {'mean': 0.0010924044763669372,\n",
       "  'std': 0.0011063138954341412},\n",
       " 'bid_spread_sum_450': {'mean': 0.01827302947640419,\n",
       "  'std': 0.017582140862941742},\n",
       " 'bid_spread_max_450': {'mean': 0.0005060372641310096,\n",
       "  'std': 0.0005326357786543667},\n",
       " 'ask_spread_sum_450': {'mean': 0.018451424315571785,\n",
       "  'std': 0.017645424231886864},\n",
       " 'ask_spread_max_450': {'mean': 0.0005125325405970216,\n",
       "  'std': 0.000542079156730324},\n",
       " 'total_volume_sum_450': {'mean': 411367.03125, 'std': 2298820.0},\n",
       " 'total_volume_max_450': {'mean': 5173.892578125, 'std': 24339.939453125},\n",
       " 'volume_imbalance_sum_450': {'mean': 99233.9140625, 'std': 633497.625},\n",
       " 'volume_imbalance_max_450': {'mean': 2485.0400390625, 'std': 10063.650390625},\n",
       " 'bid_ask_spread_sum_450': {'mean': 0.008893482387065887,\n",
       "  'std': 0.012128329835832119},\n",
       " 'bid_ask_spread_max_450': {'mean': 0.00045698389294557273,\n",
       "  'std': 0.0006128445384092629},\n",
       " 'size_sum_450': {'mean': 7675.55859375, 'std': 17927.212890625},\n",
       " 'size_max_450': {'mean': 1556.7783203125, 'std': 4357.44921875},\n",
       " 'size_min_450': {'mean': 29.608509063720703, 'std': 650.6060791015625},\n",
       " 'order_count_sum_450': {'mean': 90.46131134033203, 'std': 153.04953002929688},\n",
       " 'order_count_max_450': {'mean': 14.780510902404785,\n",
       "  'std': 27.617895126342773},\n",
       " 'size_tau_sum_450': {'mean': 1.9405014514923096, 'std': 1.965571403503418},\n",
       " 'size_tau_max_450': {'mean': 0.6700259447097778, 'std': 0.4062366783618927},\n",
       " 'size_tau_min_450': {'mean': 0.0067564076744019985,\n",
       "  'std': 0.054960206151008606},\n",
       " 'order_count_tau_sum_450': {'mean': 11.408973693847656,\n",
       "  'std': 9.678533554077148},\n",
       " 'order_count_tau_max_450': {'mean': 0.9722397923469543,\n",
       "  'std': 0.13647033274173737},\n",
       " 'order_count_tau_min_450': {'mean': 0.12197491526603699,\n",
       "  'std': 0.11613699793815613},\n",
       " 'trade_money_turnover_sum_450': {'mean': 7675.16796875,\n",
       "  'std': 17920.162109375},\n",
       " 'trade_money_turnover_max_450': {'mean': 1556.71240234375,\n",
       "  'std': 4356.255859375},\n",
       " 'trade_money_turnover_min_450': {'mean': 29.608877182006836,\n",
       "  'std': 650.7915649414062},\n",
       " 'directional_volume1_sum_450': {'mean': 1027.3812255859375,\n",
       "  'std': 348880.09375},\n",
       " 'directional_volume1_max_450': {'mean': 1402.8546142578125,\n",
       "  'std': 6641.55615234375},\n",
       " 'directional_volume1_min_450': {'mean': -1400.69482421875,\n",
       "  'std': 6144.20263671875},\n",
       " 'directional_volume2_sum_450': {'mean': 2842.36474609375, 'std': 395188.5625},\n",
       " 'directional_volume2_max_450': {'mean': 1073.40625, 'std': 5436.3525390625},\n",
       " 'directional_volume2_min_450': {'mean': -1044.613037109375,\n",
       "  'std': 5055.80078125},\n",
       " 'logret_directional_volume1_sum_450': {'mean': 5.574337410507724e-05,\n",
       "  'std': 0.03156222403049469},\n",
       " 'logret_directional_volume1_max_450': {'mean': 0.026738177984952927,\n",
       "  'std': 0.08597720414400101},\n",
       " 'logret_directional_volume1_min_450': {'mean': -0.02673548273742199,\n",
       "  'std': 0.08504117280244827},\n",
       " 'logret_directional_volume2_sum_450': {'mean': -3.8485297409351915e-05,\n",
       "  'std': 0.027410544455051422},\n",
       " 'logret_directional_volume2_max_450': {'mean': 0.01956695318222046,\n",
       "  'std': 0.06811542809009552},\n",
       " 'logret_directional_volume2_min_450': {'mean': -0.019588645547628403,\n",
       "  'std': 0.06911838799715042},\n",
       " 'trade_price_push_on_book_sum_450': {'mean': -2.9896987143729348e-06,\n",
       "  'std': 0.00039390556048601866},\n",
       " 'trade_price_push_on_book_max_450': {'mean': 3.4425011108396575e-05,\n",
       "  'std': 0.000637091463431716},\n",
       " 'trade_price_push_on_book_min_450': {'mean': -0.00011452393664512783,\n",
       "  'std': 0.0006280417437665164}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feature_dict = {key:[] for key in dataset[0][0].keys()}\n",
    "for data_idx in range(len(dataset)):\n",
    "    for feature_key,feature_val in dataset[data_idx][0].items():\n",
    "        all_feature_dict[feature_key].append(feature_val)\n",
    "        \n",
    "standard_scaling_feature_map = {}\n",
    "for key,val in all_feature_dict.items():\n",
    "    if type(val[0]) is not list and type(val[0]) is not str and val[0].type() is not str:\n",
    "        if len(val[0].size())>0 and \"bool\" not in str(val[0].type()).lower():\n",
    "#             print(val[0].type())\n",
    "            mean = torch.mean(torch.cat(val).reshape(-1)).item()\n",
    "            std = torch.std(torch.cat(val).reshape(-1)).item()\n",
    "            standard_scaling_feature_map[key] = {'mean':mean,'std':std}\n",
    "        elif \"bool\" not in str(val[0].type()).lower():\n",
    "            mean = torch.mean(torch.stack(val).reshape(-1)).item()\n",
    "            std = torch.std(torch.stack(val).reshape(-1)).item()\n",
    "            standard_scaling_feature_map[key] = {'mean':mean,'std':std}\n",
    "#             print(key, 'mean', , 'std', )\n",
    "standard_scaling_feature_map       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a72c9b-fab8-4536-b125-abc46cc800b6",
   "metadata": {},
   "source": [
    "### Learnings about model CNN input\n",
    "- it's better to use multiple channel for logreturn1 and logreturn2 than stacking it and using as one channel\n",
    "- 2 channels input for CNN is better than stacking it(dim 2, which is logret1_t1, logret2_t1, logret1_t2, logret2_t2...) and using it as one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4b98ff-b328-465e-a04e-c25eef8ea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = None\n",
    "\n",
    "\n",
    "def loss_fn_mse(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)))\n",
    "\n",
    "def loss_fn_mspe(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)/y))\n",
    "\n",
    "def loss_fn_ape(y, pred):\n",
    "    return torch.mean(torch.abs((y-pred)/y))\n",
    "\n",
    "def loss_fn_orig(y, pred):\n",
    "    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a6a979-c114-4525-953f-f5c368fe26d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_stocks = dataset.main_df['stock_id'].max()\n",
    "number_of_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7de51c7-de13-45ff-8513-5d890d691352",
   "metadata": {},
   "outputs": [],
   "source": [
    "realize_volatility_scale_factor = 1000\n",
    "def scale_optiver_feature(feature_name, feature_tensor):\n",
    "    standard_scaling_feature_map = {'stock_id': {'mean': 62.43794250488281, 'std': 37.12644958496094},\n",
    " 'seconds_in_bucket_xs': {'mean': 54.860660552978516,\n",
    "  'std': 36.77859115600586},\n",
    " 'has_trade_data_xs': {'mean': 0.9256505966186523, 'std': 0.26233869791030884},\n",
    " 'book_seconds_in_bucket_sum_xs': {'mean': 3.2410800457000732,\n",
    "  'std': 1.6307694911956787},\n",
    " 'trade_seconds_in_bucket_sum_xs': {'mean': 0.7438617944717407,\n",
    "  'std': 1.050882339477539},\n",
    " 'logret_bid_price1_sum_xs': {'mean': -1.6512319289674338e-09,\n",
    "  'std': 0.0005907019949518144},\n",
    " 'logret_ask_price1_sum_xs': {'mean': -2.367119167345777e-09,\n",
    "  'std': 0.000591019110288471},\n",
    " 'logret_bid_price2_sum_xs': {'mean': -1.6668815217002475e-09,\n",
    "  'std': 0.0006065901252441108},\n",
    " 'logret_ask_price2_sum_xs': {'mean': -2.3287849426623097e-09,\n",
    "  'std': 0.0006076738936826587},\n",
    " 'logret_bid_size1_sum_xs': {'mean': 2.0847835457971087e-06,\n",
    "  'std': 1.4804167747497559},\n",
    " 'logret_ask_size1_sum_xs': {'mean': 2.521141823308426e-06,\n",
    "  'std': 1.455901026725769},\n",
    " 'logret_bid_size2_sum_xs': {'mean': 1.7478965901318588e-06,\n",
    "  'std': 1.3515498638153076},\n",
    " 'logret_ask_size2_sum_xs': {'mean': 2.5673455184005434e-06,\n",
    "  'std': 1.3213202953338623},\n",
    " 'logret_price_sum_xs': {'mean': -2.3343620370042117e-09,\n",
    "  'std': 0.0005715438746847212},\n",
    " 'logret_size_sum_xs': {'mean': -2.419666316200164e-07,\n",
    "  'std': 1.587930679321289},\n",
    " 'logret_order_count_sum_xs': {'mean': -1.0728214192567975e-06,\n",
    "  'std': 0.7549796104431152},\n",
    " 'book_seconds_in_bucket_sum_0': {'mean': 388.9295959472656,\n",
    "  'std': 135.8336944580078},\n",
    " 'trade_seconds_in_bucket_sum_0': {'mean': 89.26342010498047,\n",
    "  'std': 82.71247100830078},\n",
    " 'wap1_sum_0': {'mean': 389.92791748046875, 'std': 135.835205078125},\n",
    " 'wap1_std_0': {'mean': 0.0011102678254246712, 'std': 0.0010516541078686714},\n",
    " 'wap2_sum_0': {'mean': 389.9277038574219, 'std': 135.83547973632812},\n",
    " 'wap2_std_0': {'mean': 0.0011489872122183442, 'std': 0.0010650980984792113},\n",
    " 'logret1_realized_volatility_0': {'mean': 0.005850940477102995,\n",
    "  'std': 0.004778958857059479},\n",
    " 'logret2_realized_volatility_0': {'mean': 0.007210279814898968,\n",
    "  'std': 0.005797804333269596},\n",
    " 'logret_price_realized_volatility_0': {'mean': 0.004610072355717421,\n",
    "  'std': 0.004089121241122484},\n",
    " 'wap_balance_sum_0': {'mean': 0.09234429150819778,\n",
    "  'std': 0.08365236222743988},\n",
    " 'wap_balance_max_0': {'mean': 0.0010880399495363235,\n",
    "  'std': 0.0012415354140102863},\n",
    " 'price_spread1_sum_0': {'mean': 0.2226376235485077,\n",
    "  'std': 0.20077848434448242},\n",
    " 'price_spread1_max_0': {'mean': 0.0014055456267669797,\n",
    "  'std': 0.001588768558576703},\n",
    " 'bid_spread_sum_0': {'mean': 0.07556163519620895, 'std': 0.06961646676063538},\n",
    " 'bid_spread_max_0': {'mean': 0.0007034957525320351,\n",
    "  'std': 0.0008316159946843982},\n",
    " 'ask_spread_sum_0': {'mean': 0.07635528594255447, 'std': 0.06991078704595566},\n",
    " 'ask_spread_max_0': {'mean': 0.0007134961779229343,\n",
    "  'std': 0.0008447545696981251},\n",
    " 'total_volume_sum_0': {'mean': 1629942.5, 'std': 9067553.0},\n",
    " 'total_volume_max_0': {'mean': 6443.4765625, 'std': 27511.509765625},\n",
    " 'volume_imbalance_sum_0': {'mean': 396575.46875, 'std': 2442117.5},\n",
    " 'volume_imbalance_max_0': {'mean': 3689.99755859375, 'std': 13807.1123046875},\n",
    " 'bid_ask_spread_sum_0': {'mean': 0.038354359567165375,\n",
    "  'std': 0.050127506256103516},\n",
    " 'bid_ask_spread_max_0': {'mean': 0.000663700804580003,\n",
    "  'std': 0.0009456104598939419},\n",
    " 'size_sum_0': {'mean': 31860.21484375, 'std': 70259.2109375},\n",
    " 'size_max_0': {'mean': 3035.258544921875, 'std': 8191.23974609375},\n",
    " 'size_min_0': {'mean': 6.389898777008057, 'std': 352.6045227050781},\n",
    " 'order_count_sum_0': {'mean': 373.43682861328125, 'std': 608.5436401367188},\n",
    " 'order_count_max_0': {'mean': 26.246469497680664, 'std': 50.72904968261719},\n",
    " 'size_tau_sum_0': {'mean': 7.7889628410339355, 'std': 6.480547904968262},\n",
    " 'size_tau_max_0': {'mean': 0.917707085609436, 'std': 0.23625372350215912},\n",
    " 'size_tau_min_0': {'mean': 0.001273703295737505, 'std': 0.004305623937398195},\n",
    " 'order_count_tau_sum_0': {'mean': 46.07876205444336,\n",
    "  'std': 37.05854415893555},\n",
    " 'order_count_tau_max_0': {'mean': 0.9990649223327637,\n",
    "  'std': 0.025059882551431656},\n",
    " 'order_count_tau_min_0': {'mean': 0.06630133837461472,\n",
    "  'std': 0.0399792343378067},\n",
    " 'trade_money_turnover_sum_0': {'mean': 31859.123046875, 'std': 70262.2265625},\n",
    " 'trade_money_turnover_max_0': {'mean': 3035.2578125, 'std': 8194.078125},\n",
    " 'trade_money_turnover_min_0': {'mean': 6.387502670288086,\n",
    "  'std': 352.42071533203125},\n",
    " 'directional_volume1_sum_0': {'mean': 2020.0023193359375, 'std': 1096721.0},\n",
    " 'directional_volume1_max_0': {'mean': 2131.2041015625,\n",
    "  'std': 9154.2041015625},\n",
    " 'directional_volume1_min_0': {'mean': -2176.4326171875,\n",
    "  'std': 8616.134765625},\n",
    " 'directional_volume2_sum_0': {'mean': 13685.599609375, 'std': 1445586.875},\n",
    " 'directional_volume2_max_0': {'mean': 1842.190673828125,\n",
    "  'std': 7931.50048828125},\n",
    " 'directional_volume2_min_0': {'mean': -1858.739013671875,\n",
    "  'std': 7222.1728515625},\n",
    " 'logret_directional_volume1_sum_0': {'mean': -1.8199473572622082e-07,\n",
    "  'std': 0.028246624395251274},\n",
    " 'logret_directional_volume1_max_0': {'mean': 0.04359145835042,\n",
    "  'std': 0.15943863987922668},\n",
    " 'logret_directional_volume1_min_0': {'mean': -0.04361077770590782,\n",
    "  'std': 0.1639694720506668},\n",
    " 'logret_directional_volume2_sum_0': {'mean': 1.47040069009563e-07,\n",
    "  'std': 0.03417246416211128},\n",
    " 'logret_directional_volume2_max_0': {'mean': 0.034739550203084946,\n",
    "  'std': 0.1626690775156021},\n",
    " 'logret_directional_volume2_min_0': {'mean': -0.0347997285425663,\n",
    "  'std': 0.16696001589298248},\n",
    " 'trade_price_push_on_book_sum_0': {'mean': -1.7589563867659308e-05,\n",
    "  'std': 0.0011976135428994894},\n",
    " 'trade_price_push_on_book_max_0': {'mean': 0.00011602201266214252,\n",
    "  'std': 0.0001294823014177382},\n",
    " 'trade_price_push_on_book_min_0': {'mean': -0.00011783508671214804,\n",
    "  'std': 0.0001304793986491859},\n",
    " 'book_seconds_in_bucket_sum_300': {'mean': 192.7294464111328,\n",
    "  'std': 69.91691589355469},\n",
    " 'trade_seconds_in_bucket_sum_300': {'mean': 44.1748161315918,\n",
    "  'std': 41.352291107177734},\n",
    " 'wap1_sum_300': {'mean': 192.72744750976562, 'std': 69.91730499267578},\n",
    " 'wap1_std_300': {'mean': 0.0007740700384601951, 'std': 0.0007044852827675641},\n",
    " 'wap2_sum_300': {'mean': 192.72735595703125, 'std': 69.91746520996094},\n",
    " 'wap2_std_300': {'mean': 0.0008196851122193038, 'std': 0.0007237914251163602},\n",
    " 'logret1_realized_volatility_300': {'mean': 0.0028540410567075014,\n",
    "  'std': 0.0023584188893437386},\n",
    " 'logret2_realized_volatility_300': {'mean': 0.003916623070836067,\n",
    "  'std': 0.003286789869889617},\n",
    " 'logret_price_realized_volatility_300': {'mean': 0.0018293253378942609,\n",
    "  'std': 0.0014226160710677505},\n",
    " 'wap_balance_sum_300': {'mean': 0.04406936839222908,\n",
    "  'std': 0.0395335778594017},\n",
    " 'wap_balance_max_300': {'mean': 0.0009255553013645113,\n",
    "  'std': 0.0009595266892574728},\n",
    " 'price_spread1_sum_300': {'mean': 0.10654828697443008,\n",
    "  'std': 0.09531298279762268},\n",
    " 'price_spread1_max_300': {'mean': 0.001214938354678452,\n",
    "  'std': 0.001255062990821898},\n",
    " 'bid_spread_sum_300': {'mean': 0.036845091730356216,\n",
    "  'std': 0.034686002880334854},\n",
    " 'bid_spread_max_300': {'mean': 0.0005883006961084902,\n",
    "  'std': 0.0006275468622334301},\n",
    " 'ask_spread_sum_300': {'mean': 0.037220779806375504,\n",
    "  'std': 0.03482009097933769},\n",
    " 'ask_spread_max_300': {'mean': 0.0005957503453828394,\n",
    "  'std': 0.0006389436312019825},\n",
    " 'total_volume_sum_300': {'mean': 820102.0625, 'std': 4566595.0},\n",
    " 'total_volume_max_300': {'mean': 5717.91162109375, 'std': 25948.44140625},\n",
    " 'volume_imbalance_sum_300': {'mean': 198661.75, 'std': 1222840.625},\n",
    " 'volume_imbalance_max_300': {'mean': 2998.133544921875,\n",
    "  'std': 11909.1279296875},\n",
    " 'bid_ask_spread_sum_300': {'mean': 0.01812143065035343,\n",
    "  'std': 0.02387070842087269},\n",
    " 'bid_ask_spread_max_300': {'mean': 0.0005402453825809062,\n",
    "  'std': 0.0007164308335632086},\n",
    " 'size_sum_300': {'mean': 15430.2470703125, 'std': 34412.78125},\n",
    " 'size_max_300': {'mean': 2164.879638671875, 'std': 6209.435546875},\n",
    " 'size_min_300': {'mean': 14.42845630645752, 'std': 500.7431640625},\n",
    " 'order_count_sum_300': {'mean': 181.84652709960938, 'std': 298.0603332519531},\n",
    " 'order_count_max_300': {'mean': 19.513988494873047, 'std': 36.19590759277344},\n",
    " 'size_tau_sum_300': {'mean': 3.894740343093872, 'std': 3.503800868988037},\n",
    " 'size_tau_max_300': {'mean': 0.8218374848365784, 'std': 0.3298044204711914},\n",
    " 'size_tau_min_300': {'mean': 0.002340120729058981,\n",
    "  'std': 0.019352799281477928},\n",
    " 'order_count_tau_sum_300': {'mean': 22.86751365661621,\n",
    "  'std': 18.809871673583984},\n",
    " 'order_count_tau_max_300': {'mean': 0.9939700365066528,\n",
    "  'std': 0.06370208412408829},\n",
    " 'order_count_tau_min_300': {'mean': 0.0875999927520752,\n",
    "  'std': 0.06473039090633392},\n",
    " 'trade_money_turnover_sum_300': {'mean': 15429.447265625,\n",
    "  'std': 34409.23828125},\n",
    " 'trade_money_turnover_max_300': {'mean': 2164.802734375, 'std': 6210.3203125},\n",
    " 'trade_money_turnover_min_300': {'mean': 14.426054954528809,\n",
    "  'std': 500.55487060546875},\n",
    " 'directional_volume1_sum_300': {'mean': 1529.6800537109375,\n",
    "  'std': 628742.8125},\n",
    " 'directional_volume1_max_300': {'mean': 1727.3031005859375,\n",
    "  'std': 7824.5888671875},\n",
    " 'directional_volume1_min_300': {'mean': -1741.162353515625,\n",
    "  'std': 7332.9716796875},\n",
    " 'directional_volume2_sum_300': {'mean': 6554.21142578125, 'std': 745032.0},\n",
    " 'directional_volume2_max_300': {'mean': 1405.59716796875,\n",
    "  'std': 6610.2470703125},\n",
    " 'directional_volume2_min_300': {'mean': -1386.0255126953125,\n",
    "  'std': 5760.64990234375},\n",
    " 'logret_directional_volume1_sum_300': {'mean': 4.345632260083221e-05,\n",
    "  'std': 0.028431734070181847},\n",
    " 'logret_directional_volume1_max_300': {'mean': 0.03356919065117836,\n",
    "  'std': 0.1125909835100174},\n",
    " 'logret_directional_volume1_min_300': {'mean': -0.033594489097595215,\n",
    "  'std': 0.11414666473865509},\n",
    " 'logret_directional_volume2_sum_300': {'mean': 1.1709575119311921e-05,\n",
    "  'std': 0.039300017058849335},\n",
    " 'logret_directional_volume2_max_300': {'mean': 0.025390710681676865,\n",
    "  'std': 0.10197589546442032},\n",
    " 'logret_directional_volume2_min_300': {'mean': -0.025465641170740128,\n",
    "  'std': 0.10425708442926407},\n",
    " 'trade_price_push_on_book_sum_300': {'mean': -7.437845852109604e-06,\n",
    "  'std': 0.0006747114821337163},\n",
    " 'trade_price_push_on_book_max_300': {'mean': 8.800736395642161e-05,\n",
    "  'std': 0.000252813333645463},\n",
    " 'trade_price_push_on_book_min_300': {'mean': -9.999539906857535e-05,\n",
    "  'std': 0.0002493471256457269},\n",
    " 'book_seconds_in_bucket_sum_450': {'mean': 95.99272918701172,\n",
    "  'std': 35.80431365966797},\n",
    " 'trade_seconds_in_bucket_sum_450': {'mean': 22.018674850463867,\n",
    "  'std': 20.96971893310547},\n",
    " 'wap1_sum_450': {'mean': 95.99224090576172, 'std': 35.80501937866211},\n",
    " 'wap1_std_450': {'mean': 0.0005554135423153639, 'std': 0.0004982542595826089},\n",
    " 'wap2_sum_450': {'mean': 95.99219512939453, 'std': 35.80509948730469},\n",
    " 'wap2_std_450': {'mean': 0.0006081690080463886, 'std': 0.0005254594143480062},\n",
    " 'logret1_realized_volatility_450': {'mean': 0.0019678790122270584,\n",
    "  'std': 0.0016546284314244986},\n",
    " 'logret2_realized_volatility_450': {'mean': 0.0027031346689909697,\n",
    "  'std': 0.002307959133759141},\n",
    " 'logret_price_realized_volatility_450': {'mean': 0.0012547716032713652,\n",
    "  'std': 0.0010070931166410446},\n",
    " 'wap_balance_sum_450': {'mean': 0.021708102896809578,\n",
    "  'std': 0.020036635920405388},\n",
    " 'wap_balance_max_450': {'mean': 0.0008153317030519247,\n",
    "  'std': 0.00083634298061952},\n",
    " 'price_spread1_sum_450': {'mean': 0.0524955578148365,\n",
    "  'std': 0.04763847589492798},\n",
    " 'price_spread1_max_450': {'mean': 0.0010924044763669372,\n",
    "  'std': 0.0011063138954341412},\n",
    " 'bid_spread_sum_450': {'mean': 0.01827302947640419,\n",
    "  'std': 0.017582140862941742},\n",
    " 'bid_spread_max_450': {'mean': 0.0005060372641310096,\n",
    "  'std': 0.0005326357786543667},\n",
    " 'ask_spread_sum_450': {'mean': 0.018451424315571785,\n",
    "  'std': 0.017645424231886864},\n",
    " 'ask_spread_max_450': {'mean': 0.0005125325405970216,\n",
    "  'std': 0.000542079156730324},\n",
    " 'total_volume_sum_450': {'mean': 411367.03125, 'std': 2298820.0},\n",
    " 'total_volume_max_450': {'mean': 5173.892578125, 'std': 24339.939453125},\n",
    " 'volume_imbalance_sum_450': {'mean': 99233.9140625, 'std': 633497.625},\n",
    " 'volume_imbalance_max_450': {'mean': 2485.0400390625, 'std': 10063.650390625},\n",
    " 'bid_ask_spread_sum_450': {'mean': 0.008893482387065887,\n",
    "  'std': 0.012128329835832119},\n",
    " 'bid_ask_spread_max_450': {'mean': 0.00045698389294557273,\n",
    "  'std': 0.0006128445384092629},\n",
    " 'size_sum_450': {'mean': 7675.55859375, 'std': 17927.212890625},\n",
    " 'size_max_450': {'mean': 1556.7783203125, 'std': 4357.44921875},\n",
    " 'size_min_450': {'mean': 29.608509063720703, 'std': 650.6060791015625},\n",
    " 'order_count_sum_450': {'mean': 90.46131134033203, 'std': 153.04953002929688},\n",
    " 'order_count_max_450': {'mean': 14.780510902404785,\n",
    "  'std': 27.617895126342773},\n",
    " 'size_tau_sum_450': {'mean': 1.9405014514923096, 'std': 1.965571403503418},\n",
    " 'size_tau_max_450': {'mean': 0.6700259447097778, 'std': 0.4062366783618927},\n",
    " 'size_tau_min_450': {'mean': 0.0067564076744019985,\n",
    "  'std': 0.054960206151008606},\n",
    " 'order_count_tau_sum_450': {'mean': 11.408973693847656,\n",
    "  'std': 9.678533554077148},\n",
    " 'order_count_tau_max_450': {'mean': 0.9722397923469543,\n",
    "  'std': 0.13647033274173737},\n",
    " 'order_count_tau_min_450': {'mean': 0.12197491526603699,\n",
    "  'std': 0.11613699793815613},\n",
    " 'trade_money_turnover_sum_450': {'mean': 7675.16796875,\n",
    "  'std': 17920.162109375},\n",
    " 'trade_money_turnover_max_450': {'mean': 1556.71240234375,\n",
    "  'std': 4356.255859375},\n",
    " 'trade_money_turnover_min_450': {'mean': 29.608877182006836,\n",
    "  'std': 650.7915649414062},\n",
    " 'directional_volume1_sum_450': {'mean': 1027.3812255859375,\n",
    "  'std': 348880.09375},\n",
    " 'directional_volume1_max_450': {'mean': 1402.8546142578125,\n",
    "  'std': 6641.55615234375},\n",
    " 'directional_volume1_min_450': {'mean': -1400.69482421875,\n",
    "  'std': 6144.20263671875},\n",
    " 'directional_volume2_sum_450': {'mean': 2842.36474609375, 'std': 395188.5625},\n",
    " 'directional_volume2_max_450': {'mean': 1073.40625, 'std': 5436.3525390625},\n",
    " 'directional_volume2_min_450': {'mean': -1044.613037109375,\n",
    "  'std': 5055.80078125},\n",
    " 'logret_directional_volume1_sum_450': {'mean': 5.574337410507724e-05,\n",
    "  'std': 0.03156222403049469},\n",
    " 'logret_directional_volume1_max_450': {'mean': 0.026738177984952927,\n",
    "  'std': 0.08597720414400101},\n",
    " 'logret_directional_volume1_min_450': {'mean': -0.02673548273742199,\n",
    "  'std': 0.08504117280244827},\n",
    " 'logret_directional_volume2_sum_450': {'mean': -3.8485297409351915e-05,\n",
    "  'std': 0.027410544455051422},\n",
    " 'logret_directional_volume2_max_450': {'mean': 0.01956695318222046,\n",
    "  'std': 0.06811542809009552},\n",
    " 'logret_directional_volume2_min_450': {'mean': -0.019588645547628403,\n",
    "  'std': 0.06911838799715042},\n",
    " 'trade_price_push_on_book_sum_450': {'mean': -2.9896987143729348e-06,\n",
    "  'std': 0.00039390556048601866},\n",
    " 'trade_price_push_on_book_max_450': {'mean': 3.4425011108396575e-05,\n",
    "  'std': 0.000637091463431716},\n",
    " 'trade_price_push_on_book_min_450': {'mean': -0.00011452393664512783,\n",
    "  'std': 0.0006280417437665164}}\n",
    "#     print(feature_name, feature_tensor.size())\n",
    "    \n",
    "#     if feature_name in ['book_realized_volatility_xs','trade_realized_volatility_xs']:\n",
    "#         # we expect feature_tensor to be log returns tensor\n",
    "#         feature_tensor = feature_tensor ** 2\n",
    "# #         print(feature_tensor)\n",
    "#         feature_tensor = torch.cumsum(feature_tensor,1)\n",
    "#         # scale it to make each step realize volatility extrapolatable to 10 min window\n",
    "# #         feature_tensor = feature_tensor * torch.tensor([data_intervals_count/idx for idx in range(1,data_intervals_count+1,1)])\n",
    "#         feature_tensor = torch.sqrt(feature_tensor) * realize_volatility_scale_factor\n",
    "        \n",
    "    if feature_name == 'sequence_mask_xs':\n",
    "        feature_tensor = feature_tensor.type(torch.float32)\n",
    "        return feature_tensor\n",
    "    if feature_name == 'has_trade_data_xs':\n",
    "        #TODO: we'll pre convert it so directly reutrn feature_tensor without converting\n",
    "        return feature_tensor.type(torch.float32)\n",
    "    if feature_name == 'seconds_in_bucket_xs':\n",
    "        return feature_tensor / standard_scaling_feature_map[feature_name]['std']/100\n",
    "    if feature_name in standard_scaling_feature_map:\n",
    "        return (feature_tensor - standard_scaling_feature_map[feature_name]['mean'])/standard_scaling_feature_map[feature_name]['std']\n",
    "#         return feature_tensor/standard_scaling_feature_map[feature_name]['std']/2\n",
    "    if feature_name in ['trade_price_local_standardized_xs','book_wap1_local_standardized_xs']:\n",
    "        #TODO: the kaggle version of pytorch dont have nan_to_num, do something here!\n",
    "        feature_tensor = torch.masked_fill(feature_tensor, torch.isinf(feature_tensor),0)\n",
    "#         feature_tensor = torch.nan_to_num(feature_tensor,nan=0, posinf=0, neginf=0)\n",
    "#     print(feature_tensor)\n",
    "#     print(torch.any(torch.isnan(feature_tensor)))\n",
    "#     input()\n",
    "    return feature_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b110429-4ffd-4923-8f5e-392e119ee6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_id\n",
      "tensor([[-1.6279]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_mask_xs\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds_in_bucket_xs\n",
      "tensor([[0.0000, 0.0003, 0.0007, 0.0010, 0.0013, 0.0017, 0.0020, 0.0023, 0.0026,\n",
      "         0.0030, 0.0033, 0.0036, 0.0040, 0.0043, 0.0046, 0.0050, 0.0053, 0.0056,\n",
      "         0.0059, 0.0063, 0.0066, 0.0069, 0.0073, 0.0076, 0.0079, 0.0083, 0.0086,\n",
      "         0.0089, 0.0093, 0.0096, 0.0099, 0.0102, 0.0106, 0.0109, 0.0112, 0.0116,\n",
      "         0.0119, 0.0122, 0.0126, 0.0129, 0.0132, 0.0135, 0.0139, 0.0142, 0.0145,\n",
      "         0.0149, 0.0152, 0.0155, 0.0159, 0.0162, 0.0165, 0.0168, 0.0172, 0.0175,\n",
      "         0.0178, 0.0182, 0.0185, 0.0188, 0.0192, 0.0195, 0.0198, 0.0202, 0.0205,\n",
      "         0.0208, 0.0211, 0.0215, 0.0218, 0.0221, 0.0225, 0.0228, 0.0231, 0.0235,\n",
      "         0.0238, 0.0241, 0.0244, 0.0248, 0.0251, 0.0254, 0.0258, 0.0261, 0.0264,\n",
      "         0.0268, 0.0271, 0.0274, 0.0278, 0.0281, 0.0284, 0.0287, 0.0291, 0.0294,\n",
      "         0.0297, 0.0301, 0.0304, 0.0307, 0.0311, 0.0314, 0.0317, 0.0320, 0.0324,\n",
      "         0.0327]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_trade_data_xs\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logret1_realized_volatility\n",
      "tensor([[-0.3818]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logret2_realized_volatility\n",
      "tensor([[-0.5276]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logret_price_realized_volatility\n",
      "tensor([[-0.2682]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23728/195287288.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'row_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale_optiver_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         )\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "for key,val in dataset[10000][0].items():\n",
    "    print(key)\n",
    "#     print(val)\n",
    "    if key not in ['row_id']:\n",
    "        print(scale_optiver_feature(key, val.reshape(1,-1)))\n",
    "    input()\n",
    "    \n",
    "    \n",
    "# for idx in range(len(dataset)):\n",
    "#     feature_x = dataset[idx][0]\n",
    "#     test = scale_optiver_feature('logret1_xs', feature_x['logret1_xs'].reshape(1,-1))\n",
    "#     print(feature_x['logret1_xs'].tolist())\n",
    "#     print(test.tolist())\n",
    "#     input()\n",
    "#     realized_volatility_xs = scale_optiver_feature('realized_volatility_xs', feature_x['logret1_xs'].reshape(1,-1))\n",
    "#     print(feature_x['book_realized_volatility'])\n",
    "# #     print(feature_x['logret1_xs'])\n",
    "#     print(realized_volatility_xs)\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15e35973-37f1-408a-b47f-3a8fa6c9879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockIdEmbedding(nn.Module):\n",
    "    def __init__(self,number_of_stock_embeddings=126+10, number_of_stock_embedding_dimention=2, mode='train'):\n",
    "        super(StockIdEmbedding, self).__init__()\n",
    "        \n",
    "        self.number_of_stock_embeddings = number_of_stock_embeddings\n",
    "        self.number_of_stock_embedding_dimention = number_of_stock_embedding_dimention\n",
    "        self.stock_embedding = nn.Embedding(self.number_of_stock_embeddings, self.number_of_stock_embedding_dimention)\n",
    "        self.mode = mode\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(self.number_of_stock_embedding_dimention, 32),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return []\n",
    "    \n",
    "    def set_mode(self,mode):\n",
    "        self.mode = mode\n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "        \n",
    "        stock_id_clamped = torch.clamp(feature_dict['stock_id'],0,self.number_of_stock_embeddings-1)\n",
    "        stock_id_clamped = stock_id_clamped.type(torch.int64)\n",
    "        stock_id_clamped = stock_id_clamped.to(device).reshape(-1,1)\n",
    "        embedding_logits = self.stock_embedding(stock_id_clamped)\n",
    "        embedding_logits = embedding_logits.reshape(-1,self.number_of_stock_embedding_dimention)\n",
    "        \n",
    "        if self.mode == 'stock_id_embedding':\n",
    "            return embedding_logits\n",
    "\n",
    "            \n",
    "        logits = self.linear_stack(embedding_logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad1ae512-810e-410f-a177-127168b56106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "#         super().__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "#         position = torch.arange(max_len).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "#         pe = torch.zeros(max_len,1, d_model)\n",
    "# #         print(pe.size())\n",
    "#         pe[:,0, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:,0, 1::2] = torch.cos(position * div_term)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "#         \"\"\"\n",
    "# #         print(\"x\",x.size())\n",
    "# #         print(\"PE\",self.pe[:,:x.size(1)].size())\n",
    "#         x = x + self.pe[:x.size(0)]\n",
    "#         return self.dropout(x)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class MultiFetTransformer(nn.Module):\n",
    "    def __init__(self, mask_feature_name='sequence_mask_xs',sequence_feature_name='seconds_in_bucket_xs', mode=\"train\"):\n",
    "        \"\"\"single feature, feature learner\n",
    "        `mode`: train|feature_generator\n",
    "        \"\"\"\n",
    "        super(MultiFetTransformer,self).__init__()\n",
    "        self.sequence_feature_names = ['seconds_in_bucket_xs','has_trade_data_xs'] + ['book_seconds_in_bucket_sum_xs', 'trade_seconds_in_bucket_sum_xs', 'logret_bid_price1_sum_xs', 'logret_ask_price1_sum_xs', 'logret_bid_price2_sum_xs', 'logret_ask_price2_sum_xs', 'logret_bid_size1_sum_xs', 'logret_ask_size1_sum_xs', 'logret_bid_size2_sum_xs', 'logret_ask_size2_sum_xs', 'logret_price_sum_xs', 'logret_size_sum_xs', 'logret_order_count_sum_xs']\n",
    "#         self.overview_feature_names len: 156\n",
    "        self.overview_feature_names = ['book_seconds_in_bucket_sum_0', 'trade_seconds_in_bucket_sum_0', 'wap1_sum_0', 'wap1_std_0', 'wap2_sum_0', 'wap2_std_0', 'logret1_realized_volatility_0', 'logret2_realized_volatility_0', 'logret_price_realized_volatility_0', 'wap_balance_sum_0', 'wap_balance_max_0', 'price_spread1_sum_0', 'price_spread1_max_0', 'bid_spread_sum_0', 'bid_spread_max_0', 'ask_spread_sum_0', 'ask_spread_max_0', 'total_volume_sum_0', 'total_volume_max_0', 'volume_imbalance_sum_0', 'volume_imbalance_max_0', 'bid_ask_spread_sum_0', 'bid_ask_spread_max_0', 'size_sum_0', 'size_max_0', 'size_min_0', 'order_count_sum_0', 'order_count_max_0', 'size_tau_sum_0', 'size_tau_max_0', 'size_tau_min_0', 'order_count_tau_sum_0', 'order_count_tau_max_0', 'order_count_tau_min_0', 'trade_money_turnover_sum_0', 'trade_money_turnover_max_0', 'trade_money_turnover_min_0', 'directional_volume1_sum_0', 'directional_volume1_max_0', 'directional_volume1_min_0', 'directional_volume2_sum_0', 'directional_volume2_max_0', 'directional_volume2_min_0', 'logret_directional_volume1_sum_0', 'logret_directional_volume1_max_0', 'logret_directional_volume1_min_0', 'logret_directional_volume2_sum_0', 'logret_directional_volume2_max_0', 'logret_directional_volume2_min_0', 'trade_price_push_on_book_sum_0', 'trade_price_push_on_book_max_0', 'trade_price_push_on_book_min_0', 'book_seconds_in_bucket_sum_300', 'trade_seconds_in_bucket_sum_300', 'wap1_sum_300', 'wap1_std_300', 'wap2_sum_300', 'wap2_std_300', 'logret1_realized_volatility_300', 'logret2_realized_volatility_300', 'logret_price_realized_volatility_300', 'wap_balance_sum_300', 'wap_balance_max_300', 'price_spread1_sum_300', 'price_spread1_max_300', 'bid_spread_sum_300', 'bid_spread_max_300', 'ask_spread_sum_300', 'ask_spread_max_300', 'total_volume_sum_300', 'total_volume_max_300', 'volume_imbalance_sum_300', 'volume_imbalance_max_300', 'bid_ask_spread_sum_300', 'bid_ask_spread_max_300', 'size_sum_300', 'size_max_300', 'size_min_300', 'order_count_sum_300', 'order_count_max_300', 'size_tau_sum_300', 'size_tau_max_300', 'size_tau_min_300', 'order_count_tau_sum_300', 'order_count_tau_max_300', 'order_count_tau_min_300', 'trade_money_turnover_sum_300', 'trade_money_turnover_max_300', 'trade_money_turnover_min_300', 'directional_volume1_sum_300', 'directional_volume1_max_300', 'directional_volume1_min_300', 'directional_volume2_sum_300', 'directional_volume2_max_300', 'directional_volume2_min_300', 'logret_directional_volume1_sum_300', 'logret_directional_volume1_max_300', 'logret_directional_volume1_min_300', 'logret_directional_volume2_sum_300', 'logret_directional_volume2_max_300', 'logret_directional_volume2_min_300', 'trade_price_push_on_book_sum_300', 'trade_price_push_on_book_max_300', 'trade_price_push_on_book_min_300', 'book_seconds_in_bucket_sum_450', 'trade_seconds_in_bucket_sum_450', 'wap1_sum_450', 'wap1_std_450', 'wap2_sum_450', 'wap2_std_450', 'logret1_realized_volatility_450', 'logret2_realized_volatility_450', 'logret_price_realized_volatility_450', 'wap_balance_sum_450', 'wap_balance_max_450', 'price_spread1_sum_450', 'price_spread1_max_450', 'bid_spread_sum_450', 'bid_spread_max_450', 'ask_spread_sum_450', 'ask_spread_max_450', 'total_volume_sum_450', 'total_volume_max_450', 'volume_imbalance_sum_450', 'volume_imbalance_max_450', 'bid_ask_spread_sum_450', 'bid_ask_spread_max_450', 'size_sum_450', 'size_max_450', 'size_min_450', 'order_count_sum_450', 'order_count_max_450', 'size_tau_sum_450', 'size_tau_max_450', 'size_tau_min_450', 'order_count_tau_sum_450', 'order_count_tau_max_450', 'order_count_tau_min_450', 'trade_money_turnover_sum_450', 'trade_money_turnover_max_450', 'trade_money_turnover_min_450', 'directional_volume1_sum_450', 'directional_volume1_max_450', 'directional_volume1_min_450', 'directional_volume2_sum_450', 'directional_volume2_max_450', 'directional_volume2_min_450', 'logret_directional_volume1_sum_450', 'logret_directional_volume1_max_450', 'logret_directional_volume1_min_450', 'logret_directional_volume2_sum_450', 'logret_directional_volume2_max_450', 'logret_directional_volume2_min_450', 'trade_price_push_on_book_sum_450', 'trade_price_push_on_book_max_450', 'trade_price_push_on_book_min_450']\n",
    "        self.use_stock_embedding = True\n",
    "        self.use_overview_features = True\n",
    "        self.stock_id_embedding_dimension = 3\n",
    "        self.stock_id_embedding = StockIdEmbedding(number_of_stock_embedding_dimention=self.stock_id_embedding_dimension, mode='stock_id_embedding')\n",
    "        \n",
    "        \n",
    "        self.mask_feature_name = mask_feature_name\n",
    "        self.sequence_feature_name = sequence_feature_name\n",
    "        self.features_count = len(self.sequence_feature_names)\n",
    "\n",
    "        \n",
    "        self.output_dimensions = 128\n",
    "        self.transformer_input_dimension = 18\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.transformer_input_dimension)\n",
    "        self.mode = mode\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.transformer_input_dimension, nhead=6, dropout=0.1, activation='gelu')\n",
    "        self.encoder_stack = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        \n",
    "        self.feature_to_feature_embedding = nn.Sequential(\n",
    "            nn.Linear(self.features_count, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, self.transformer_input_dimension)\n",
    "        )\n",
    "        \n",
    "        self.transformer_output_feature = nn.Sequential(\n",
    "#             nn.Conv1d(self.transformer_input_dimension, 32, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(32, 24, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Flatten(),\n",
    "            nn.Linear(data_intervals_count*self.transformer_input_dimension,256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256,self.output_dimensions),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, self.features_out),\n",
    "        )\n",
    "        \n",
    "        self.overviewff_output_feature = nn.Sequential(\n",
    "#             nn.Conv1d(self.transformer_input_dimension, 32, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(32, 24, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Flatten(),\n",
    "            nn.Linear(len(self.overview_feature_names)+self.stock_id_embedding_dimension,256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256,self.output_dimensions),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, self.features_out),\n",
    "        )\n",
    "        \n",
    "        self.transformer_train = nn.Sequential(\n",
    "            nn.Linear(self.output_dimensions, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),   \n",
    "        )\n",
    "        self.overviewff_train = nn.Sequential(\n",
    "            nn.Linear(self.output_dimensions, 256),\n",
    "            nn.GELU(),\n",
    "#             nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),   \n",
    "        )\n",
    "       \n",
    "        \n",
    "        self.hybrid_stack = nn.Sequential(\n",
    "\n",
    "            nn.Linear(self.output_dimensions*2, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.05),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.Hardswish(),\n",
    "            nn.Linear(64, 1),   \n",
    "        )\n",
    "    \n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def get_possible_modes(self):\n",
    "        return ['transformer_train','overviewff_train','hybrid','full_train']\n",
    "    \n",
    "    def parameters(self):\n",
    "        generator_sources_map = {\n",
    "            'transformer_train': [self.feature_to_feature_embedding, self.encoder_stack, self.transformer_output_feature, self.transformer_train],\n",
    "            'overviewff_train': [self.stock_id_embedding, self.overviewff_output_feature, self.overviewff_train],\n",
    "            'hybrid': [self.hybrid_stack],\n",
    "            'full_train': [self.feature_to_feature_embedding, self.encoder_stack, self.transformer_output_feature, self.transformer_train] + [self.stock_id_embedding, self.overviewff_output_feature, self.overviewff_train] + [self.hybrid_stack]\n",
    "        }\n",
    "        params = []\n",
    "        if self.mode in generator_sources_map:\n",
    "            for generator_source in generator_sources_map[self.mode]:\n",
    "                for param in generator_source.parameters():\n",
    "                    params.append(param)\n",
    "            return params\n",
    "        \n",
    "        \n",
    "    def forward(self, feature_dict, h0_tensor=None):  \n",
    "        out_ = []\n",
    "        if self.mode in ['transformer_train','hybrid','full_train']:\n",
    "            feature_x = []     \n",
    "            for idx,feature_name in enumerate(self.sequence_feature_names):\n",
    "                feature_tensor = scale_optiver_feature(feature_name, feature_dict[feature_name]).to(device)\n",
    "                feature_tensor = feature_tensor.reshape(-1,feature_tensor.size(1),1)\n",
    "                feature_x.append(feature_tensor)\n",
    "            \n",
    "        \n",
    "            #combine all the features activated tensors\n",
    "            feature_x = torch.cat(feature_x,dim=2) #.reshape(-1, data_intervals_count, self.input_size_) #+[stock_embedding_logits]\n",
    "            \n",
    "            feature_x = self.feature_to_feature_embedding(feature_x)\n",
    "            \n",
    "            #positional encoding\n",
    "#             position_encodings = []\n",
    "#             for idx,feature_name in enumerate(self.sequence_feature_names):\n",
    "            position_encodings = feature_dict[self.sequence_feature_name]\n",
    "            position_encodings = (position_encodings.to(device)+1)/601\n",
    "            position_encodings = position_encodings.reshape(-1,feature_x.size(1),1)\n",
    "            \n",
    "            divterm = torch.exp(torch.arange(0, feature_x.size(2), 2) * (-np.log(10000.0) / feature_x.size(2)))\n",
    "            positional_encodings = position_encodings * divterm.to(device)\n",
    "            \n",
    "            feature_x = torch.add(feature_x, position_encodings)\n",
    "\n",
    "            #Mask prepare\n",
    "            mask = feature_dict[self.mask_feature_name].to(device)\n",
    "\n",
    "            \n",
    "            # make them sequence first!!\n",
    "            # RANT: all of this due to Kaggle using pytorch 1.7.0\n",
    "            feature_x = torch.stack(feature_x.unbind(0),dim=1)\n",
    "            \n",
    "#             feature_x = self.positional_encoding(feature_x)\n",
    "            \n",
    "            transformer_features = self.encoder_stack(feature_x, src_key_padding_mask=mask)\n",
    "            # back to batch first!\n",
    "            transformer_features = torch.stack(transformer_features.unbind(0),dim=1)\n",
    "            transformer_features = transformer_features.reshape(-1, self.transformer_input_dimension*data_intervals_count)\n",
    "            \n",
    "\n",
    "            transformer_features = self.transformer_output_feature(transformer_features)\n",
    "            if self.mode == 'transformer_train':\n",
    "                return self.transformer_train(transformer_features)\n",
    "            out_.append(transformer_features)\n",
    "        # we add overview features here CAT them; that's why \n",
    "#         print(out_.size(),stock_embedding_logits.size())\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.mode in ['overviewff_train','hybrid','full_train']:\n",
    "        \n",
    "            stock_embedding_logits = self.stock_id_embedding(feature_dict)\n",
    "            \n",
    "        \n",
    "        \n",
    "            feature_x = []\n",
    "            for idx,feature_name in enumerate(self.overview_feature_names):\n",
    "                feature_tensor = scale_optiver_feature(feature_name, feature_dict[feature_name]).to(device)\n",
    "    #             print(feature_tensor.size())\n",
    "                feature_tensor = feature_tensor.reshape(-1,1)\n",
    "\n",
    "                feature_x.append(feature_tensor)\n",
    "            \n",
    "            \n",
    "            feature_x = torch.cat([stock_embedding_logits]+feature_x,dim=1)\n",
    "#             print(feature_x.size())\n",
    "            overview_features = self.overviewff_output_feature(feature_x)\n",
    "#             print(overview_features.size())\n",
    "            if self.mode == 'overviewff_train':\n",
    "                return self.overviewff_train(overview_features)\n",
    "            \n",
    "#             out_.append(stock_embedding_logits)\n",
    "            out_.append(overview_features)\n",
    "            \n",
    "#         print(feature_x.size())\n",
    "#         print(out_.size())\n",
    "        out_ = torch.cat(out_,dim=1)\n",
    "        \n",
    "        out_ = self.hybrid_stack(out_)\n",
    "\n",
    "        return out_\n",
    "\n",
    "# model = MultiFetTransformer()\n",
    "# model.to(device)\n",
    "# dataloader_train = DataLoader(dataset, batch_size=2,\n",
    "#                                 shuffle=True, num_workers=0, pin_memory=False)#, collate_fn=optiver_custom_collate_func)\n",
    "\n",
    "\n",
    " \n",
    "# >>> src = torch.rand(10, 32, 512)\n",
    "# >>> out = encoder_layer(src)   \n",
    "# for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "#     model(Feature_X)\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58e3006a-19df-4243-b389-07e09236408e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1f24f-c586-4e1b-aad6-fb4671c87232",
   "metadata": {},
   "source": [
    "#### analyze the initial weights (or change them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "931890cf-e329-4a15-b0d0-352dc27f985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @torch.no_grad()\n",
    "# def init_weights(m):\n",
    "# #     print(m)\n",
    "#     if type(m) == nn.Linear:\n",
    "# #         m.weight.fill_(1.0)\n",
    "#         torch.nn.init.xavier_uniform_(m.weight,gain=10)\n",
    "#         m.bias.data.uniform_(-1,1)\n",
    "# #     elif type(m) == nn.ReLU:\n",
    "# #         print(m.data)\n",
    "#     else:\n",
    "#         print(type(m))\n",
    "# #         print(m.weight)\n",
    "# model.apply(init_weights)\n",
    "# # for param in model.parameters():\n",
    "# # #     print(param)\n",
    "# #       print(param.data.size(), param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c79a7d-9e08-4433-9c77-1af049f349d6",
   "metadata": {},
   "source": [
    "### LEarning rate: our base line is 0.34 loss as that's what the optiver guys have when they use current 10 min realize vol and use it as target (copy to prediction). We create simplest neural network and work with learning rates to figure out what's best and when we see something in range of 0.35 then we've found good Learning rate\n",
    "- #### SGD: 1e-7 works best\n",
    "- #### ADAM: 1e-5, (NOTE: 1e-3 makes it behave dumb where some deep local minima gets stuck and produces constant output!)\n",
    "- TODO: analyze that constant output phenomenon more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cb3871f-5215-4afb-92fc-47ae2d385fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-4\n",
    "# batch_size = 4096\n",
    "# epochs = 100\n",
    "\n",
    "# input_scaling = 1\n",
    "# output_scaling = 1\n",
    "\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "# strategyname = \"ret1_n_ret2\"\n",
    "# summary_writer = SummaryWriter(f'../output/training_tensorboard/{strategyname}_scaleIn{input_scaling}Out{output_scaling}_{learning_rate}_{batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9aadc54b-6009-4af6-bfb6-f6619c6acd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a7ed1-80f8-4a57-90b4-223345dde76b",
   "metadata": {},
   "source": [
    "### Learnings about training\n",
    "- (non scaling)logreturns input and volatility output; non scaled makes the model predict constant output with no variety(close to 0 std dev)\n",
    "- scaling input rids of variety issue, \n",
    "- scaling output makes the model start with low rmse initially so there's less ground to cover and we can iterate over ideas rapidly due to less epochs needed to achieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef34a31-7317-47c0-a94f-9bebe2fd2e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n",
      "Current epochs: 28 LR : 0.001 batch_size: 256\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.2512805697633259 [34048/343145] recent lr: 3.4059328807066636e-06\n",
      "train: 0.25107915399234687 [68352/343145] recent lr: 7.2984276015142785e-06\n",
      "train: 0.2497316624468832 [102656/343145] recent lr: 1.1677484162422846e-05\n",
      "train: 0.2501068323183416 [136960/343145] recent lr: 1.556997888323046e-05\n",
      "train: 0.24464837005778925 [171264/343145] recent lr: 1.9462473604038075e-05\n",
      "train: 0.24882652543818773 [205568/343145] recent lr: 2.3841530164946643e-05\n",
      "train: 0.24645610164795348 [239872/343145] recent lr: 2.773402488575426e-05\n",
      "train: 0.2462864279079793 [274176/343145] recent lr: 3.2113081446662825e-05\n",
      "train: 0.24422500304766556 [308480/343145] recent lr: 3.600557616747044e-05\n",
      "train: 0.24484667526697046 [342784/343145] recent lr: 3.989807088827805e-05\n",
      "train: 0.22207579016685486 test: 0.2438870013381044 [343040/343145] recent lr: 3.989807088827805e-05\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.24519519408543905 [34048/343145] recent lr: 4.4277127449186624e-05\n",
      "train: 0.24550389976643805 [68352/343145] recent lr: 4.8169622169994234e-05\n",
      "train: 0.2459731576825256 [102656/343145] recent lr: 5.2548678730902806e-05\n",
      "train: 0.24513632114698639 [136960/343145] recent lr: 5.644117345171042e-05\n",
      "train: 0.2442584721677339 [171264/343145] recent lr: 6.033366817251803e-05\n",
      "train: 0.24356360444382055 [205568/343145] recent lr: 6.47127247334266e-05\n",
      "train: 0.24559031393545777 [239872/343145] recent lr: 6.860521945423422e-05\n",
      "train: 0.24479704995208712 [274176/343145] recent lr: 7.249771417504184e-05\n",
      "train: 0.24534242222113395 [308480/343145] recent lr: 7.68767707359504e-05\n",
      "train: 0.24568886120817554 [342784/343145] recent lr: 8.076926545675801e-05\n",
      "train: 0.25408801436424255 test: 0.24316499941051006 [343040/343145] recent lr: 8.076926545675801e-05\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.2449484610999072 [34048/343145] recent lr: 8.514832201766658e-05\n",
      "train: 0.244351558943293 [68352/343145] recent lr: 8.90408167384742e-05\n",
      "train: 0.2460154276063193 [102656/343145] recent lr: 9.293331145928181e-05\n",
      "train: 0.24432264613126642 [136960/343145] recent lr: 9.731236802019038e-05\n",
      "train: 0.24420105932808633 [171264/343145] recent lr: 0.00010120486274099799\n",
      "train: 0.24323279822050636 [205568/343145] recent lr: 0.00010509735746180561\n",
      "train: 0.24279642627755207 [239872/343145] recent lr: 0.00010947641402271418\n",
      "train: 0.2433469100896992 [274176/343145] recent lr: 0.00011336890874352179\n",
      "train: 0.24682150936838407 [308480/343145] recent lr: 0.00011774796530443035\n",
      "train: 0.2431887539465036 [342784/343145] recent lr: 0.00012164046002523799\n",
      "train: 0.2664320170879364 test: 0.24193704341139113 [343040/343145] recent lr: 0.00012164046002523799\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.2455232669909795 [34048/343145] recent lr: 0.00012553295474604557\n",
      "train: 0.242597596636459 [68352/343145] recent lr: 0.00012991201130695416\n",
      "train: 0.24340811015954658 [102656/343145] recent lr: 0.00013380450602776178\n",
      "train: 0.2452135489725355 [136960/343145] recent lr: 0.00013818356258867034\n",
      "train: 0.2448653395273792 [171264/343145] recent lr: 0.00014207605730947798\n",
      "train: 0.24401901072975415 [205568/343145] recent lr: 0.00014596855203028556\n",
      "train: 0.24262435611949038 [239872/343145] recent lr: 0.00015034760859119413\n",
      "train: 0.2433959043292857 [274176/343145] recent lr: 0.00015424010331200176\n",
      "train: 0.24168984447397404 [308480/343145] recent lr: 0.00015813259803280937\n",
      "train: 0.24306946848310643 [342784/343145] recent lr: 0.00016251165459371794\n",
      "train: 0.26412975788116455 test: 0.24246601547513688 [343040/343145] recent lr: 0.00016251165459371794\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.24476103870956986 [34048/343145] recent lr: 0.00016640414931452555\n",
      "train: 0.242305728370574 [68352/343145] recent lr: 0.0001707832058754341\n",
      "train: 0.2431259290940726 [102656/343145] recent lr: 0.00017467570059624172\n",
      "train: 0.24354424750182166 [136960/343145] recent lr: 0.00017856819531704936\n",
      "train: 0.24659194986322033 [171264/343145] recent lr: 0.00018294725187795793\n",
      "train: 0.24308770127705673 [205568/343145] recent lr: 0.00018683974659876554\n",
      "train: 0.2427878366477454 [239872/343145] recent lr: 0.00019073224131957315\n",
      "train: 0.24143175580608311 [274176/343145] recent lr: 0.0001951112978804817\n",
      "train: 0.2452601018681455 [308480/343145] recent lr: 0.00019900379260128932\n",
      "train: 0.2422230891994576 [342784/343145] recent lr: 0.0002033828491621979\n",
      "train: 0.23020966351032257 test: 0.24679597613534757 [343040/343145] recent lr: 0.0002033828491621979\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.24324184711332675 [34048/343145] recent lr: 0.00020727534388300552\n",
      "train: 0.2439269499102635 [68352/343145] recent lr: 0.0002111678386038131\n",
      "train: 0.24456694499770207 [102656/343145] recent lr: 0.0002155468951647217\n",
      "train: 0.24360245577434994 [136960/343145] recent lr: 0.0002194393898855293\n",
      "train: 0.24264174432896857 [171264/343145] recent lr: 0.0002238184464464379\n",
      "train: 0.24305857373262518 [205568/343145] recent lr: 0.0002277109411672455\n",
      "train: 0.24407873113653553 [239872/343145] recent lr: 0.0002316034358880531\n",
      "train: 0.24419235905159764 [274176/343145] recent lr: 0.00023598249244896169\n",
      "train: 0.24124430231193997 [308480/343145] recent lr: 0.0002398749871697693\n",
      "train: 0.2409740955527149 [342784/343145] recent lr: 0.00024376748189057688\n",
      "train: 0.25039345026016235 test: 0.24651305936276913 [343040/343145] recent lr: 0.00024376748189057688\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.24143541179321432 [34048/343145] recent lr: 0.00024814653845148547\n",
      "train: 0.24415276744472447 [68352/343145] recent lr: 0.00025203903317229305\n",
      "train: 0.24272572593902475 [102656/343145] recent lr: 0.0002564180897332017\n",
      "train: 0.2431929823177964 [136960/343145] recent lr: 0.00026031058445400926\n",
      "train: 0.24304969008289165 [171264/343145] recent lr: 0.0002642030791748169\n",
      "train: 0.24325418627973813 [205568/343145] recent lr: 0.00026858213573572546\n",
      "train: 0.2433735456929278 [239872/343145] recent lr: 0.0002724746304565331\n",
      "train: 0.2415411315524756 [274176/343145] recent lr: 0.0002763671251773407\n",
      "train: 0.24249593624428137 [308480/343145] recent lr: 0.00028074618173824924\n",
      "train: 0.2421221359452205 [342784/343145] recent lr: 0.00028463867645905683\n",
      "train: 0.2479804903268814 test: 0.24550136209776005 [343040/343145] recent lr: 0.00028463867645905683\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.24209784953682512 [34048/343145] recent lr: 0.0002890177330199654\n",
      "train: 0.240418992269395 [68352/343145] recent lr: 0.00029291022774077303\n",
      "train: 0.24339349445567202 [102656/343145] recent lr: 0.00029680272246158067\n",
      "train: 0.2414677923739846 [136960/343145] recent lr: 0.00030118177902248923\n",
      "train: 0.24319889329707445 [171264/343145] recent lr: 0.00030507427374329687\n",
      "train: 0.2418453139600469 [205568/343145] recent lr: 0.00030945333030420543\n",
      "train: 0.24115172506713156 [239872/343145] recent lr: 0.000313345825025013\n",
      "train: 0.24242233651787487 [274176/343145] recent lr: 0.00031723831974582066\n",
      "train: 0.24364860622740503 [308480/343145] recent lr: 0.0003216173763067292\n",
      "train: 0.2447626753084695 [342784/343145] recent lr: 0.0003255098710275368\n",
      "train: 0.2546539604663849 test: 0.2407218696815627 [343040/343145] recent lr: 0.0003255098710275368\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.2399703648355272 [34048/343145] recent lr: 0.00032940236574834444\n",
      "train: 0.241611510515213 [68352/343145] recent lr: 0.000333781422309253\n",
      "train: 0.24044055625129102 [102656/343145] recent lr: 0.0003376739170300606\n",
      "train: 0.24043325623914377 [136960/343145] recent lr: 0.0003420529735909692\n",
      "train: 0.23921581090831046 [171264/343145] recent lr: 0.0003459454683117768\n",
      "train: 0.24155720245482318 [205568/343145] recent lr: 0.00034983796303258443\n",
      "train: 0.24371903200647724 [239872/343145] recent lr: 0.000354217019593493\n",
      "train: 0.24279731178461617 [274176/343145] recent lr: 0.00035810951431430063\n",
      "train: 0.24280571047939473 [308480/343145] recent lr: 0.0003620020090351082\n",
      "train: 0.2421135803434386 [342784/343145] recent lr: 0.0003663810655960168\n",
      "train: 0.26521992683410645 test: 0.24113519786901416 [343040/343145] recent lr: 0.0003663810655960168\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.24037855521396354 [34048/343145] recent lr: 0.00037027356031682436\n",
      "train: 0.24038604179869838 [68352/343145] recent lr: 0.000374652616877733\n",
      "train: 0.24212525829450407 [102656/343145] recent lr: 0.00037854511159854056\n",
      "train: 0.24523515154176684 [136960/343145] recent lr: 0.0003824376063193482\n",
      "train: 0.24296069712336385 [171264/343145] recent lr: 0.00038681666288025677\n",
      "train: 0.2403143979052999 [205568/343145] recent lr: 0.0003907091576010644\n",
      "train: 0.24045594147781826 [239872/343145] recent lr: 0.00039508821416197297\n",
      "train: 0.24320623389820553 [274176/343145] recent lr: 0.0003989807088827806\n",
      "train: 0.24209881985365456 [308480/343145] recent lr: 0.0004028732036035882\n",
      "train: 0.23988709020525661 [342784/343145] recent lr: 0.0004072522601644967\n",
      "train: 0.2413008213043213 test: 0.2412885111268787 [343040/343145] recent lr: 0.0004072522601644967\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.23767945611918415 [34048/343145] recent lr: 0.00041114475488530434\n",
      "train: 0.2394588415080042 [68352/343145] recent lr: 0.000415037249606112\n",
      "train: 0.24221327133587936 [102656/343145] recent lr: 0.00041941630616702054\n",
      "train: 0.2380484989774761 [136960/343145] recent lr: 0.0004233088008878282\n",
      "train: 0.23981496469298405 [171264/343145] recent lr: 0.00042768785744873674\n",
      "train: 0.2404096903863238 [205568/343145] recent lr: 0.0004315803521695443\n",
      "train: 0.24023325352081612 [239872/343145] recent lr: 0.00043547284689035196\n",
      "train: 0.2417290971350314 [274176/343145] recent lr: 0.0004398519034512605\n",
      "train: 0.24034653770834652 [308480/343145] recent lr: 0.00044374439817206816\n",
      "train: 0.24391260053684463 [342784/343145] recent lr: 0.0004476368928928758\n",
      "train: 0.2746584117412567 test: 0.2532518855517819 [343040/343145] recent lr: 0.0004481234547329767\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.23993391460842556 [34048/343145] recent lr: 0.0004520159494537843\n",
      "train: 0.24126337424143038 [68352/343145] recent lr: 0.0004559084441745919\n",
      "train: 0.23777727171111462 [102656/343145] recent lr: 0.0004602875007355005\n",
      "train: 0.2377779410401387 [136960/343145] recent lr: 0.0004641799954563081\n",
      "train: 0.2401808971789346 [171264/343145] recent lr: 0.00046807249017711574\n",
      "train: 0.23881332420591098 [205568/343145] recent lr: 0.0004724515467380243\n",
      "train: 0.24349793080073684 [239872/343145] recent lr: 0.00047634404145883194\n",
      "train: 0.24232254599902167 [274176/343145] recent lr: 0.0004807230980197405\n",
      "train: 0.2407673855326069 [308480/343145] recent lr: 0.00048461559274054814\n",
      "train: 0.24039240492813624 [342784/343145] recent lr: 0.0004885080874613557\n",
      "train: 0.2695449888706207 test: 0.24274664159332002 [343040/343145] recent lr: 0.0004885080874613557\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.23978649918679837 [34048/343145] recent lr: 0.0004928871440222643\n",
      "train: 0.23642843839392733 [68352/343145] recent lr: 0.0004967796387430719\n",
      "train: 0.2399210939878848 [102656/343145] recent lr: 0.0005006721334638795\n",
      "train: 0.23976194291417277 [136960/343145] recent lr: 0.0005050511900247881\n",
      "train: 0.24217335779720278 [171264/343145] recent lr: 0.0005089436847455957\n",
      "train: 0.23890256648188207 [205568/343145] recent lr: 0.0005133227413065042\n",
      "train: 0.2376279290487517 [239872/343145] recent lr: 0.0005172152360273119\n",
      "train: 0.2423552755099624 [274176/343145] recent lr: 0.0005211077307481195\n",
      "train: 0.2415948902715498 [308480/343145] recent lr: 0.000525486787309028\n",
      "train: 0.23931169420925538 [342784/343145] recent lr: 0.0005293792820298356\n",
      "train: 0.2175167053937912 test: 0.2487118834895747 [343040/343145] recent lr: 0.0005293792820298356\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.23636559314197964 [34048/343145] recent lr: 0.0005337583385907443\n",
      "train: 0.23463843173500318 [68352/343145] recent lr: 0.0005376508333115518\n",
      "train: 0.2373495078576145 [102656/343145] recent lr: 0.0005415433280323595\n",
      "train: 0.24205177433010358 [136960/343145] recent lr: 0.000545922384593268\n",
      "train: 0.2393877986651748 [171264/343145] recent lr: 0.0005498148793140757\n",
      "train: 0.23943892143555542 [205568/343145] recent lr: 0.0005537073740348833\n",
      "train: 0.2382638278736997 [239872/343145] recent lr: 0.0005580864305957918\n",
      "train: 0.23942952747665233 [274176/343145] recent lr: 0.0005619789253165995\n",
      "train: 0.23785006221550614 [308480/343145] recent lr: 0.0005663579818775081\n",
      "train: 0.24060293186956377 [342784/343145] recent lr: 0.0005702504765983156\n",
      "train: 0.24043706059455872 test: 0.24671495807844968 [343040/343145] recent lr: 0.0005702504765983156\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.23625222389344816 [34048/343145] recent lr: 0.0005741429713191233\n",
      "train: 0.23729814755827633 [68352/343145] recent lr: 0.0005785220278800319\n",
      "train: 0.23828491368400517 [102656/343145] recent lr: 0.0005824145226008394\n",
      "train: 0.23720249578134336 [136960/343145] recent lr: 0.000586307017321647\n",
      "train: 0.23841371042514914 [171264/343145] recent lr: 0.0005906860738825557\n",
      "train: 0.2369898874590646 [205568/343145] recent lr: 0.0005945785686033632\n",
      "train: 0.23890887378756678 [239872/343145] recent lr: 0.0005989576251642718\n",
      "train: 0.2401681723656939 [274176/343145] recent lr: 0.0006028501198850793\n",
      "train: 0.24406018786465944 [308480/343145] recent lr: 0.0006067426146058871\n",
      "train: 0.2371973695594873 [342784/343145] recent lr: 0.0006111216711667956\n",
      "train: 0.20984067022800446 test: 0.24763650163298562 [343040/343145] recent lr: 0.0006111216711667956\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.23644578059514362 [34048/343145] recent lr: 0.0006150141658876032\n",
      "train: 0.2342931679380474 [68352/343145] recent lr: 0.0006193932224485117\n",
      "train: 0.23621027716504994 [102656/343145] recent lr: 0.0006232857171693194\n",
      "train: 0.2352095605499709 [136960/343145] recent lr: 0.000627178211890127\n",
      "train: 0.23878794085623614 [171264/343145] recent lr: 0.0006315572684510355\n",
      "train: 0.24207701522912553 [205568/343145] recent lr: 0.0006354497631718433\n",
      "train: 0.24440237801911227 [239872/343145] recent lr: 0.0006393422578926508\n",
      "train: 0.24146524160655577 [274176/343145] recent lr: 0.0006437213144535594\n",
      "train: 0.23903562606715445 [308480/343145] recent lr: 0.000647613809174367\n",
      "train: 0.23983559557306233 [342784/343145] recent lr: 0.0006519928657352756\n",
      "train: 0.23161594569683075 test: 0.2547413028687948 [343040/343145] recent lr: 0.0006519928657352756\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.23639503849877253 [34048/343145] recent lr: 0.0006558853604560832\n",
      "train: 0.237789135926695 [68352/343145] recent lr: 0.0006597778551768907\n",
      "train: 0.23720824907519925 [102656/343145] recent lr: 0.0006641569117377994\n",
      "train: 0.23460939715602505 [136960/343145] recent lr: 0.000668049406458607\n",
      "train: 0.2349192225443783 [171264/343145] recent lr: 0.0006719419011794145\n",
      "train: 0.23937762206170096 [205568/343145] recent lr: 0.0006763209577403231\n",
      "train: 0.24036062041770168 [239872/343145] recent lr: 0.0006802134524611308\n",
      "train: 0.2358724453778409 [274176/343145] recent lr: 0.0006845925090220393\n",
      "train: 0.23600636278070622 [308480/343145] recent lr: 0.0006884850037428469\n",
      "train: 0.2356775031605763 [342784/343145] recent lr: 0.0006923774984636546\n",
      "train: 0.2299492210149765 test: 0.2487398481794766 [343040/343145] recent lr: 0.0006923774984636546\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.23288904792732662 [34048/343145] recent lr: 0.0006967565550245631\n",
      "train: 0.23416848569663604 [68352/343145] recent lr: 0.0007006490497453708\n",
      "train: 0.23377581274331505 [102656/343145] recent lr: 0.0007050281063062793\n",
      "train: 0.23512435804552106 [136960/343145] recent lr: 0.0007089206010270869\n",
      "train: 0.23975306104368238 [171264/343145] recent lr: 0.0007128130957478946\n",
      "train: 0.2369379556890744 [205568/343145] recent lr: 0.0007171921523088031\n",
      "train: 0.23836873249331517 [239872/343145] recent lr: 0.0007210846470296108\n",
      "train: 0.24060219259404425 [274176/343145] recent lr: 0.0007249771417504184\n",
      "train: 0.23725262473323452 [308480/343145] recent lr: 0.0007293561983113269\n",
      "train: 0.23488884352481187 [342784/343145] recent lr: 0.0007332486930321345\n",
      "train: 0.23735499382019043 test: 0.2441043851098844 [343040/343145] recent lr: 0.0007332486930321345\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.23260065339229724 [34048/343145] recent lr: 0.0007376277495930432\n",
      "train: 0.23203345165768666 [68352/343145] recent lr: 0.0007415202443138507\n",
      "train: 0.2303650727245345 [102656/343145] recent lr: 0.0007454127390346583\n",
      "train: 0.23325435105544418 [136960/343145] recent lr: 0.0007497917955955669\n",
      "train: 0.24012869585361055 [171264/343145] recent lr: 0.0007536842903163745\n",
      "train: 0.23751381228664029 [205568/343145] recent lr: 0.0007575767850371821\n",
      "train: 0.23445518214755984 [239872/343145] recent lr: 0.0007619558415980907\n",
      "train: 0.23486527934003232 [274176/343145] recent lr: 0.0007658483363188983\n",
      "train: 0.23671578404618732 [308480/343145] recent lr: 0.0007702273928798069\n",
      "train: 0.23573932151741056 [342784/343145] recent lr: 0.0007741198876006144\n",
      "train: 0.2153083235025406 test: 0.2433570388349749 [343040/343145] recent lr: 0.0007741198876006144\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.23070709892997035 [34048/343145] recent lr: 0.0007780123823214222\n",
      "train: 0.23259527580951578 [68352/343145] recent lr: 0.0007823914388823307\n",
      "train: 0.23173054252097855 [102656/343145] recent lr: 0.0007862839336031383\n",
      "train: 0.2311747208682459 [136960/343145] recent lr: 0.0007906629901640468\n",
      "train: 0.23388744045549364 [171264/343145] recent lr: 0.0007945554848848544\n",
      "train: 0.2456991331123594 [205568/343145] recent lr: 0.0007984479796056621\n",
      "train: 0.24143416334444018 [239872/343145] recent lr: 0.0008028270361665706\n",
      "train: 0.23582149886373263 [274176/343145] recent lr: 0.0008067195308873783\n",
      "train: 0.234658250279391 [308480/343145] recent lr: 0.0008106120256081859\n",
      "train: 0.23406593646131343 [342784/343145] recent lr: 0.0008149910821690945\n",
      "train: 0.24636974930763245 test: 0.25048186873928424 [343040/343145] recent lr: 0.0008149910821690945\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.23164636779714515 [34048/343145] recent lr: 0.000818883576889902\n",
      "train: 0.2302930836579693 [68352/343145] recent lr: 0.0008232626334508106\n",
      "train: 0.22813889150744054 [102656/343145] recent lr: 0.0008271551281716183\n",
      "train: 0.22834834245158664 [136960/343145] recent lr: 0.0008310476228924258\n",
      "train: 0.23407045251397945 [171264/343145] recent lr: 0.0008354266794533344\n",
      "train: 0.23424059185963958 [205568/343145] recent lr: 0.0008393191741741421\n",
      "train: 0.23849350291846402 [239872/343145] recent lr: 0.0008432116688949496\n",
      "train: 0.2373989916114665 [274176/343145] recent lr: 0.0008475907254558582\n",
      "train: 0.23168267899039965 [308480/343145] recent lr: 0.0008514832201766657\n",
      "train: 0.23231820684315554 [342784/343145] recent lr: 0.0008558622767375744\n",
      "train: 0.22215281426906586 test: 0.24908737501218206 [343040/343145] recent lr: 0.0008558622767375744\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.22861922030095702 [34048/343145] recent lr: 0.000859754771458382\n",
      "train: 0.22784177569755867 [68352/343145] recent lr: 0.0008636472661791896\n",
      "train: 0.2252058422387536 [102656/343145] recent lr: 0.0008680263227400981\n",
      "train: 0.22959165097172582 [136960/343145] recent lr: 0.0008719188174609059\n",
      "train: 0.23452107185748086 [171264/343145] recent lr: 0.0008762978740218144\n",
      "train: 0.23222072900676016 [205568/343145] recent lr: 0.000880190368742622\n",
      "train: 0.23286824139641293 [239872/343145] recent lr: 0.0008840828634634297\n",
      "train: 0.23800355796493702 [274176/343145] recent lr: 0.0008884619200243382\n",
      "train: 0.23998851729417914 [308480/343145] recent lr: 0.0008923544147451458\n",
      "train: 0.2325800164644398 [342784/343145] recent lr: 0.0008962469094659534\n",
      "train: 0.23992350697517395 test: 0.2515843252518347 [343040/343145] recent lr: 0.0008962469094659534\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.22693102646757055 [34048/343145] recent lr: 0.000900625966026862\n",
      "train: 0.22580191648718137 [68352/343145] recent lr: 0.0009045184607476696\n",
      "train: 0.2241540127503338 [102656/343145] recent lr: 0.0009088975173085782\n",
      "train: 0.22586954818732702 [136960/343145] recent lr: 0.0009127900120293858\n",
      "train: 0.22987307919495142 [171264/343145] recent lr: 0.0009166825067501934\n",
      "train: 0.23059175485995279 [205568/343145] recent lr: 0.000921061563311102\n",
      "train: 0.22930606465731093 [239872/343145] recent lr: 0.0009249540580319095\n",
      "train: 0.2383142429501263 [274176/343145] recent lr: 0.0009288465527527173\n",
      "train: 0.23907727703674517 [308480/343145] recent lr: 0.0009332256093136258\n",
      "train: 0.23244326595050185 [342784/343145] recent lr: 0.0009371181040344333\n",
      "train: 0.2309906780719757 test: 0.2504390289208719 [343040/343145] recent lr: 0.0009371181040344333\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "---------- cnt16acc_BSPos_ULTsmol_8Atten_6Layer_0.001_256 transformer_train ----------\n",
      "train: 0.22549484416290566 [34048/343145] recent lr: 0.0009414971605953419\n",
      "train: 0.23072037656805408 [68352/343145] recent lr: 0.0009453896553161495\n"
     ]
    }
   ],
   "source": [
    "# model = None\n",
    "strategyname = \"cnt16acc_BSPos_ULTsmol_8Atten_6Layer\"\n",
    "\n",
    "\n",
    "print(\"DEVICE:\", device)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "training_configs = []\n",
    "learning_rates_to_try = [1e-3]# 1e-4]\n",
    "batch_sizes_to_try = [256]#, 512]#,10000, 128]\n",
    "accumulation_steps = 16\n",
    "# input_scalings_to_try = [1000]\n",
    "# output_scalings_to_try = [10000]\n",
    "\n",
    "for learning_rate in learning_rates_to_try:\n",
    "    for batch_size in batch_sizes_to_try:\n",
    "        for use_stock_id in [False]:\n",
    "            training_configs.append({\n",
    "                'learning_rate':learning_rate,\n",
    "                'batch_size':batch_size,\n",
    "                'use_stock_id': use_stock_id\n",
    "            })\n",
    "\n",
    "epochs = 200\n",
    "for training_config in training_configs:\n",
    "    \n",
    "    learning_rate = training_config['learning_rate']\n",
    "    batch_size = training_config['batch_size']\n",
    "    use_stock_id = training_config['use_stock_id']\n",
    "    # TRAINING SETUP\n",
    "    \n",
    "    #refresh the model\n",
    "    \n",
    "    STRATEGY_NAME_WITH_ATTRS = f\"{strategyname}_{learning_rate}_{batch_size}\"\n",
    "    summary_writer = SummaryWriter(f'../output/training_tensorboard/{STRATEGY_NAME_WITH_ATTRS}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "#     model = VolatilityBSModel()\n",
    "#MultiFetGRU\n",
    "#MultiFetTransformer\n",
    "    model = MultiFetTransformer()\n",
    "    #, hidden_size=128,layers=8,dropout=0.2,features_out=128)\n",
    "# model.to(device)\n",
    "#     model = NeuralNetwork()\n",
    "#     model = StockIdEmbedding(number_of_stock_embedding_dimention=3)\n",
    "#     model\n",
    "    model.to(device)\n",
    "    model.use_stock_embedding = False\n",
    "    model.use_overview_features = False\n",
    "    optimizer_for_modes = {}\n",
    "    optimizer_scheduler_for_modes = {}\n",
    "    recent_lr = learning_rate\n",
    "    feature_modes = ['logrett_xs','trade_volume_xs','trade_ordercount_xs','trade_money_turnover_per_order_xs',\n",
    "                             'logret1_xs',\n",
    "                             'book_price_spread1_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "                             'book_total_volume_xs','book_volume_imbalance_xs']\n",
    "#     feature_modes = model.get_feature_gen_train_modes()\n",
    "    done_epochs = -1\n",
    "    for modeidx, mode in enumerate(['transformer_train']):#+['transformer_train','overviewff_train','hybrid','full_train']):#model.get_feature_gen_train_modes()+['hidden_generator']+['hybrid']+['ultimate']):#+model.get_feature_gen_train_modes()+['hybrid']):#'train_stock_id_embedding','train']):#+['hidden_generator']*2 + feature_modes + ['hybrid']*2):\n",
    "        model.set_mode(mode)\n",
    "        \n",
    "#         batch_size\n",
    "        if mode == 'transformer_train':\n",
    "            epochs = 28\n",
    "#             learning_rate = 1e-3\n",
    "#             batch_size = 1024\n",
    "        if mode in ['overviewff_train']:\n",
    "            epochs = 8\n",
    "        if mode in ['hybrid']:\n",
    "            epochs = 8\n",
    "        if mode in ['full_train']:\n",
    "            epochs = 80\n",
    "        \n",
    "#             learning_rate = 1e-3\n",
    "#             batch_size = 256\n",
    "#         if mode in model.get_feature_gen_train_modes()+['hidden_generator']:\n",
    "            \n",
    "#             epochs = 10\n",
    "#             learning_rate = 1e-3\n",
    "#             batch_size = 64\n",
    "            \n",
    "        print(f\"Current epochs: {epochs} LR : {learning_rate} batch_size: {batch_size}\")\n",
    "#             batch_size = 2\n",
    "#             learning_rate = 1e-5\n",
    "#             batch_size = 16\n",
    "#             learning_rate=1e-4\n",
    "        \n",
    "        \n",
    "#         print(model.parameters())\n",
    "#         input()\n",
    "#         continue\n",
    "        \n",
    "        if str(mode) not in optimizer_for_modes:\n",
    "#             optimizer_for_modes[str(mode)] = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "            optimizer_for_modes[str(mode)] = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "#             optimizer_scheduler_for_modes[str(mode)] = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_for_modes[str(mode)], factor=0.1, patience=10, threshold=0.0001)\n",
    "#             optimizer_for_modes[str(mode)] = torch.optim.RMSprop(model.parameters())\n",
    "#             if mode in model.get_feature_gen_train_modes():\n",
    "#             optimizer_for_modes[str(mode)] = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        optimizer = optimizer_for_modes[str(mode)]\n",
    "#         optimizer_scheduler = optimizer_scheduler_for_modes[str(mode)]\n",
    "        \n",
    "        # TRAINING SETUP DONE\n",
    "\n",
    "        \n",
    "\n",
    "        data_ohlc_sample_len = 1 # 1 for each of open high low close\n",
    "        losses_train = []\n",
    "        step_num = 0\n",
    "        high_step_num = 0\n",
    "        for t in range(epochs):\n",
    "            done_epochs += 1\n",
    "            \n",
    "            if done_epochs == 11:\n",
    "                model.use_stock_embedding = True\n",
    "                model.use_overview_features = True\n",
    "            \n",
    "            print(f\"Epoch {done_epochs+1}\\n-------------------------------\")\n",
    "            print(\"----------\", STRATEGY_NAME_WITH_ATTRS, mode,\"----------\")\n",
    "\n",
    "            dataloader_train = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                shuffle=True, num_workers=0, pin_memory=True)\n",
    "            model.train()\n",
    "\n",
    "            for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):                \n",
    "                step_num += 1\n",
    "                y = feature_y['target_realized_volatility'].to(device) * realize_volatility_scale_factor\n",
    "                pred = model(Feature_X)\n",
    "\n",
    "\n",
    "                loss_orig = loss_fn_orig(y, pred)\n",
    "                \n",
    "                loss = loss_orig / accumulation_steps                # Normalize our loss (if averaged)\n",
    "                loss.backward()                                 # Backward pass\n",
    "                \n",
    "                \n",
    "                if (step_num+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        effective_step = max(1,step_num//accumulation_steps)\n",
    "                        param_group['lr'] = 66**-0.5 * min(effective_step**-0.5, effective_step*4000**-1.5)\n",
    "                        recent_lr = param_group['lr']\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "    \n",
    "\n",
    "                losses_train.append(loss_orig.item())\n",
    "\n",
    "                if (t*int(train_size/batch_size) + train_batch_idx + 1) % int(train_size/10/batch_size) == 0:\n",
    "\n",
    "                    # NOTE: real loss is same as upscaled normalized loss as it's percentage loss (rmspe)\n",
    "                    prediction_variety = np.std((pred/realize_volatility_scale_factor).reshape(-1).tolist()) * 100\n",
    "                    #NOTE: prediction variety is important as model sometimes predits a constant value! regardless of the input, then per batch variety is lowest(0 std dev)\n",
    "\n",
    "\n",
    "                    summary_writer.add_scalar(\"Prediction Variety\", prediction_variety, done_epochs*(train_size) + (train_batch_idx*batch_size))\n",
    "                    summary_writer.add_scalar(\"Training Loss\", np.mean(losses_train), done_epochs*(train_size) + (train_batch_idx*batch_size))\n",
    "\n",
    "                    print(\"train:\", np.mean(losses_train), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\", f\"recent lr: {recent_lr}\")\n",
    "                    losses_train = []\n",
    "            \n",
    "            dataloader_test = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                    shuffle=True, num_workers=0, pin_memory=True)\n",
    "            dataset_size = len(dataloader_test.dataset)\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            losses_test = []\n",
    "            for _, (Feature_X, feature_y) in enumerate(dataloader_test):\n",
    "                with torch.no_grad():\n",
    "#                     y = scale_optiver_feature('book_realized_volatility',feature_y['target_realized_volatility']).to(device) # * realize_volatility_scale_factor\n",
    "                    y = feature_y['target_realized_volatility'].to(device) * realize_volatility_scale_factor\n",
    "                    pred = model(Feature_X)\n",
    "                    loss = loss_fn_orig(y, pred)\n",
    "                    \n",
    "                    losses_test.append(loss.item())\n",
    "#             optimizer_scheduler.step(np.mean(losses_test))\n",
    "\n",
    "    #                 summary_writer.add_scalar(\"Epoch Training Loss\", np.mean(losses_train), (t+1)*train_size)\n",
    "            summary_writer.add_scalar(\"Test Loss\", np.mean(losses_test), done_epochs*(train_size) + (train_batch_idx*batch_size))\n",
    "            print(\"train:\", np.mean(losses_train), \"test:\", np.mean(losses_test), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\", f\"recent lr: {recent_lr}\")\n",
    "            losses_test = []\n",
    "            if (t+1)%30==0:\n",
    "#                 torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{loss:.4f}.pth\"))\n",
    "                # torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"13_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))\n",
    "                model_statedict = {'base':model.state_dict()}\n",
    "#                 for k,v in model.feature_gen_models.items():\n",
    "#                     model_statedict[k] = v.state_dict()\n",
    "\n",
    "                torch.save(model_statedict, os.path.join(MODEL_OUTPUT_DIRECTORY,f\"16_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "93bc88db-cae6-4c72-847f-890e0a68d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"13_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))\n",
    "model_statedict = {'base':model.state_dict()}\n",
    "# for k,v in model.feature_gen_models.items():\n",
    "#     model_statedict[k] = v.state_dict()\n",
    "\n",
    "torch.save(model_statedict, os.path.join(MODEL_OUTPUT_DIRECTORY,f\"19_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c40a983-cebc-400b-9b8b-95c2e4874894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del model\n",
    "# del loss\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f978d-06d8-408d-9d32-8d0c971f2c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c865e507-c49e-401a-b561-c0c1cba9e80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>stock_id</th>\n",
       "      <th>70</th>\n",
       "      <th>75</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.811194</td>\n",
       "      <td>0.705222</td>\n",
       "      <td>0.579442</td>\n",
       "      <td>0.449084</td>\n",
       "      <td>0.813176</td>\n",
       "      <td>0.743353</td>\n",
       "      <td>0.573379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.744891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.589083</td>\n",
       "      <td>0.469144</td>\n",
       "      <td>0.732183</td>\n",
       "      <td>0.739383</td>\n",
       "      <td>0.533038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.808020</td>\n",
       "      <td>0.754178</td>\n",
       "      <td>0.580484</td>\n",
       "      <td>0.473728</td>\n",
       "      <td>0.749465</td>\n",
       "      <td>0.756734</td>\n",
       "      <td>0.525109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.760358</td>\n",
       "      <td>0.660425</td>\n",
       "      <td>0.516831</td>\n",
       "      <td>0.285894</td>\n",
       "      <td>0.710278</td>\n",
       "      <td>0.650910</td>\n",
       "      <td>0.364603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.770228</td>\n",
       "      <td>0.753502</td>\n",
       "      <td>0.611220</td>\n",
       "      <td>0.576678</td>\n",
       "      <td>0.745659</td>\n",
       "      <td>0.778682</td>\n",
       "      <td>0.663163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.576149</td>\n",
       "      <td>0.589083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474793</td>\n",
       "      <td>0.560188</td>\n",
       "      <td>0.614096</td>\n",
       "      <td>0.499395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.340020</td>\n",
       "      <td>0.469144</td>\n",
       "      <td>0.474793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.376075</td>\n",
       "      <td>0.555407</td>\n",
       "      <td>0.634929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.844946</td>\n",
       "      <td>0.732183</td>\n",
       "      <td>0.560188</td>\n",
       "      <td>0.376075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758198</td>\n",
       "      <td>0.506475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.765444</td>\n",
       "      <td>0.739383</td>\n",
       "      <td>0.614096</td>\n",
       "      <td>0.555407</td>\n",
       "      <td>0.758198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.622576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.754608</td>\n",
       "      <td>0.721407</td>\n",
       "      <td>0.589519</td>\n",
       "      <td>0.621860</td>\n",
       "      <td>0.706099</td>\n",
       "      <td>0.779024</td>\n",
       "      <td>0.685978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.870479</td>\n",
       "      <td>0.735199</td>\n",
       "      <td>0.600032</td>\n",
       "      <td>0.445861</td>\n",
       "      <td>0.825894</td>\n",
       "      <td>0.803625</td>\n",
       "      <td>0.532197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.740288</td>\n",
       "      <td>0.692165</td>\n",
       "      <td>0.533340</td>\n",
       "      <td>0.539652</td>\n",
       "      <td>0.685939</td>\n",
       "      <td>0.747160</td>\n",
       "      <td>0.595898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.820951</td>\n",
       "      <td>0.752925</td>\n",
       "      <td>0.626719</td>\n",
       "      <td>0.525911</td>\n",
       "      <td>0.791776</td>\n",
       "      <td>0.792502</td>\n",
       "      <td>0.716967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.816374</td>\n",
       "      <td>0.731109</td>\n",
       "      <td>0.570766</td>\n",
       "      <td>0.402027</td>\n",
       "      <td>0.779568</td>\n",
       "      <td>0.763643</td>\n",
       "      <td>0.542926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.745952</td>\n",
       "      <td>0.719873</td>\n",
       "      <td>0.629644</td>\n",
       "      <td>0.498651</td>\n",
       "      <td>0.741755</td>\n",
       "      <td>0.753600</td>\n",
       "      <td>0.632786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.763163</td>\n",
       "      <td>0.714092</td>\n",
       "      <td>0.562613</td>\n",
       "      <td>0.474837</td>\n",
       "      <td>0.735885</td>\n",
       "      <td>0.754644</td>\n",
       "      <td>0.622048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.911003</td>\n",
       "      <td>0.725142</td>\n",
       "      <td>0.549348</td>\n",
       "      <td>0.298335</td>\n",
       "      <td>0.832846</td>\n",
       "      <td>0.761384</td>\n",
       "      <td>0.454619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.854270</td>\n",
       "      <td>0.744462</td>\n",
       "      <td>0.602939</td>\n",
       "      <td>0.477151</td>\n",
       "      <td>0.841559</td>\n",
       "      <td>0.802465</td>\n",
       "      <td>0.571472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.880381</td>\n",
       "      <td>0.746706</td>\n",
       "      <td>0.591059</td>\n",
       "      <td>0.443440</td>\n",
       "      <td>0.810087</td>\n",
       "      <td>0.780505</td>\n",
       "      <td>0.619799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.781451</td>\n",
       "      <td>0.753629</td>\n",
       "      <td>0.601444</td>\n",
       "      <td>0.494994</td>\n",
       "      <td>0.720945</td>\n",
       "      <td>0.736164</td>\n",
       "      <td>0.565407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "stock_id        70        75        80        81        82        83        8 \n",
       "stock_id                                                                      \n",
       "74        0.811194  0.705222  0.579442  0.449084  0.813176  0.743353  0.573379\n",
       "75        0.744891  1.000000  0.589083  0.469144  0.732183  0.739383  0.533038\n",
       "76        0.808020  0.754178  0.580484  0.473728  0.749465  0.756734  0.525109\n",
       "77        0.760358  0.660425  0.516831  0.285894  0.710278  0.650910  0.364603\n",
       "78        0.770228  0.753502  0.611220  0.576678  0.745659  0.778682  0.663163\n",
       "80        0.576149  0.589083  1.000000  0.474793  0.560188  0.614096  0.499395\n",
       "81        0.340020  0.469144  0.474793  1.000000  0.376075  0.555407  0.634929\n",
       "82        0.844946  0.732183  0.560188  0.376075  1.000000  0.758198  0.506475\n",
       "83        0.765444  0.739383  0.614096  0.555407  0.758198  1.000000  0.622576\n",
       "84        0.754608  0.721407  0.589519  0.621860  0.706099  0.779024  0.685978\n",
       "85        0.870479  0.735199  0.600032  0.445861  0.825894  0.803625  0.532197\n",
       "86        0.740288  0.692165  0.533340  0.539652  0.685939  0.747160  0.595898\n",
       "87        0.820951  0.752925  0.626719  0.525911  0.791776  0.792502  0.716967\n",
       "88        0.816374  0.731109  0.570766  0.402027  0.779568  0.763643  0.542926\n",
       "89        0.745952  0.719873  0.629644  0.498651  0.741755  0.753600  0.632786\n",
       "90        0.763163  0.714092  0.562613  0.474837  0.735885  0.754644  0.622048\n",
       "93        0.911003  0.725142  0.549348  0.298335  0.832846  0.761384  0.454619\n",
       "94        0.854270  0.744462  0.602939  0.477151  0.841559  0.802465  0.571472\n",
       "95        0.880381  0.746706  0.591059  0.443440  0.810087  0.780505  0.619799\n",
       "96        0.781451  0.753629  0.601444  0.494994  0.720945  0.736164  0.565407"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p = dataset.main_df.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "corr[[70,75,80,81,82,83,8]][65:85]\n",
    "# corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "28b838c3-3aea-416a-8b20-cf2eacddb2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Learned ----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Default ---------------------\n",
      "------- set mode : hybrid -----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"----------- Learned ----------\")\n",
    "stock_ids = torch.tensor(dataset.main_df['stock_id'].unique())\n",
    "# print(stock_ids)\n",
    "\n",
    "embedding_predictor = model.hidden_generator_network.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':stock_ids})\n",
    "datapoints = pred.tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([75,70])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([76,78,82,85,87,88,94,95])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "embedding_predictor = model.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':stock_ids})\n",
    "datapoints = pred.tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([75,70])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([76,78,82,85,87,88,94,95])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()\n",
    "\n",
    "print(\"------------- Default ---------------------\")\n",
    "default_model = VolatilityBSModel().to(device)\n",
    "embedding_predictor = default_model.hidden_generator_network.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':stock_ids})\n",
    "datapoints = pred.tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([75,70])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([76,78,82,85,87,88,94,95])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "embedding_predictor = default_model.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':stock_ids})\n",
    "datapoints = pred.tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([75,70])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([76,78,82,85,87,88,94,95])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "99ec365f-201b-44f7-826c-2017b8bbc55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "50cf1a35-5d02-43e8-b242-66d082ac237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a1f5760-8926-4064-8bf2-bddd62c80bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74227704-6e5f-41fd-b9a2-242d0b8e9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optiver_custom_collate_func(batch):\n",
    "    output_x = {}\n",
    "    for k,v in batch[0][0].items():\n",
    "        output_x[k] = []\n",
    "    \n",
    "    for x_dict in [x[0] for x in batch]:\n",
    "        for k,v in x_dict.items():\n",
    "            output_x[k].append(v)\n",
    "    \n",
    "    for k,v in batch[0][0].items():\n",
    "        if type(output_x[k][0]) != str:\n",
    "            output_x[k] = torch.stack(output_x[k])\n",
    "        \n",
    "    output_y = []\n",
    "    for y in [x[1] for x in batch]:\n",
    "        output_y.append(y)\n",
    "    output_y = torch.stack(output_y)\n",
    "    \n",
    "    return (output_x, output_y)\n",
    "#     input()\n",
    "#     print(batch)\n",
    "# #     return batch\n",
    "#     input()\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82320bc2-fca3-4696-8609-6034f371e2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "17cea68b-9cf2-4a6c-8fab-d0e6b8f7654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id\n",
      "['96-13771']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_id\n",
      "tensor([96.])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds_in_bucket_xs\n",
      "tensor([[  5.,  10.,  15.,  20.,  25.,  30.,  35.,  40.,  45.,  50.,  55.,  60.,\n",
      "          65.,  70.,  75.,  80.,  85.,  90.,  95., 100., 105., 110., 115., 120.,\n",
      "         125., 130., 135., 140., 145., 150., 155., 160., 165., 170., 175., 180.,\n",
      "         185., 190., 195., 200., 205., 210., 215., 220., 225., 230., 235., 240.,\n",
      "         245., 250., 255., 260., 265., 270., 275., 280., 285., 290., 295., 300.,\n",
      "         305., 310., 315., 320., 325., 330., 335., 340., 345., 350., 355., 360.,\n",
      "         365., 370., 375., 380., 385., 390., 395., 400., 405., 410., 415., 420.,\n",
      "         425., 430., 435., 440., 445., 450., 455., 460., 465., 470., 475., 480.,\n",
      "         485., 490., 495., 500., 505., 510., 515., 520., 525., 530., 535., 540.,\n",
      "         545., 550., 555., 560., 565., 570., 575., 580., 585., 590., 595., 600.]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logrett_xs\n",
      "tensor([[-0.2583, -0.2583, -0.2583, -0.2583, -0.2565, -0.2560, -0.2566, -1.2919,\n",
      "         -0.0621, -0.0615, -0.0621, -0.0621, -0.2757, -0.2751,  2.5545, -0.3759,\n",
      "          1.5375, -1.2732, -1.4517, -0.6660, -0.6666, -0.6661, -0.0943, -0.8810,\n",
      "         -0.8745, -3.8643, -2.0211, -0.0137, -1.3876, -0.1028, -0.2772, -0.2766,\n",
      "         -2.2825, -0.0269,  1.6510, -1.3265,  2.1354,  2.1350,  0.5107,  0.5112,\n",
      "         -0.6612, -0.1995, -0.1995, -2.8131, -2.8139, -1.4733, -1.5446,  0.1518,\n",
      "          1.3928, -0.8829, -0.2122, -0.2122, -0.4472, -0.4466, -0.4472,  1.6626,\n",
      "          1.2654,  1.2647,  1.1127,  1.5446,  1.5443,  2.4143,  0.7716,  0.7715,\n",
      "         -0.4849, -0.8379, -1.7267, -0.9193, -0.5514, -0.5508, -0.2246, -0.2252,\n",
      "         -0.2252, -0.2247,  0.4583,  0.6620,  1.0186,  1.1863, -0.5597,  0.2849,\n",
      "          0.2855,  0.2849, -0.0747, -0.8219, -0.8220, -0.0287, -0.0287, -0.0287,\n",
      "          2.1707, -0.2222, -0.2222, -0.2222, -0.7705, -0.7706, -0.7707, -0.7707,\n",
      "          0.2205,  0.6614,  0.6619,  0.8823, -0.8029, -0.8023, -0.8030, -2.0030,\n",
      "         -0.0884,  0.2498,  0.2504,  0.2498, -1.9602, -2.1202, -5.4583, -0.6376,\n",
      "         -0.6383,  0.7615,  0.7620,  0.0221, -0.6903, -0.6897, -0.6903, -0.6904]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_volume_xs\n",
      "tensor([[5.3033, 0.0000, 0.0000, 5.3375, 0.0000, 0.0000, 5.3230, 5.2095, 0.0000,\n",
      "         0.0000, 0.0000, 2.1972, 0.0000, 5.1417, 4.7875, 6.5191, 4.1431, 4.8203,\n",
      "         4.6634, 0.0000, 0.0000, 6.4630, 3.0445, 2.8332, 6.4677, 4.8363, 4.5951,\n",
      "         6.3648, 4.7958, 5.5175, 0.0000, 5.3566, 6.4568, 6.1203, 5.9081, 3.2581,\n",
      "         0.0000, 6.5568, 0.0000, 0.6931, 2.1972, 0.0000, 4.6634, 0.0000, 5.7268,\n",
      "         4.3175, 1.3863, 4.9836, 0.6931, 0.6931, 0.0000, 6.2916, 0.0000, 0.0000,\n",
      "         4.6151, 6.0113, 0.0000, 0.6931, 5.3033, 0.0000, 6.1485, 4.6250, 0.0000,\n",
      "         4.6151, 6.2226, 5.3230, 2.9444, 4.6151, 0.0000, 3.0445, 0.0000, 0.0000,\n",
      "         0.0000, 5.7746, 3.6376, 0.6931, 6.0331, 1.6094, 5.2883, 0.0000, 0.0000,\n",
      "         5.7170, 4.7274, 0.0000, 4.9558, 0.0000, 0.0000, 4.9345, 4.6250, 0.0000,\n",
      "         0.0000, 5.3327, 0.0000, 0.0000, 0.0000, 1.0986, 0.6931, 0.0000, 1.0986,\n",
      "         0.6931, 0.0000, 0.0000, 5.6312, 4.6250, 5.7557, 0.0000, 0.0000, 1.0986,\n",
      "         5.7203, 4.7274, 6.2710, 0.0000, 0.6931, 0.0000, 4.9488, 4.8283, 0.0000,\n",
      "         0.0000, 0.0000, 4.6250]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_ordercount_xs\n",
      "tensor([[1.9459, 0.0000, 0.0000, 2.1972, 0.0000, 0.0000, 1.7918, 2.0794, 0.0000,\n",
      "         0.0000, 0.0000, 1.9459, 0.0000, 1.0986, 2.0794, 2.7081, 1.3863, 1.3863,\n",
      "         1.3863, 0.0000, 0.0000, 2.1972, 0.6931, 1.0986, 2.0794, 1.0986, 1.3863,\n",
      "         2.6391, 1.6094, 2.0794, 0.0000, 2.0794, 2.7081, 2.5649, 2.3979, 0.6931,\n",
      "         0.0000, 2.6391, 0.0000, 0.6931, 1.3863, 0.0000, 1.0986, 0.0000, 2.0794,\n",
      "         1.3863, 0.6931, 1.3863, 0.6931, 0.6931, 0.0000, 2.4849, 0.0000, 0.0000,\n",
      "         1.0986, 2.5649, 0.0000, 0.6931, 1.3863, 0.0000, 2.9444, 1.0986, 0.0000,\n",
      "         1.0986, 2.3979, 1.6094, 1.6094, 1.3863, 0.0000, 0.6931, 0.0000, 0.0000,\n",
      "         0.0000, 2.3979, 1.0986, 0.6931, 2.7081, 0.6931, 1.9459, 0.0000, 0.0000,\n",
      "         2.0794, 1.3863, 0.0000, 1.3863, 0.0000, 0.0000, 2.4849, 1.3863, 0.0000,\n",
      "         0.0000, 1.7918, 0.0000, 0.0000, 0.0000, 1.0986, 0.6931, 0.0000, 1.0986,\n",
      "         0.6931, 0.0000, 0.0000, 1.7918, 1.3863, 2.6391, 0.0000, 0.0000, 1.0986,\n",
      "         2.6391, 1.9459, 2.4849, 0.0000, 0.6931, 0.0000, 2.6391, 1.7918, 0.0000,\n",
      "         0.0000, 0.0000, 1.0986]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_money_turnover_xs\n",
      "tensor([[5.3024, 0.0000, 0.0000, 5.3366, 0.0000, 0.0000, 5.3220, 5.2083, 0.0000,\n",
      "         0.0000, 0.0000, 2.1962, 0.0000, 5.1404, 4.7865, 6.5181, 4.1423, 4.8193,\n",
      "         4.6623, 0.0000, 0.0000, 6.4617, 3.0432, 2.8319, 6.4662, 4.8344, 4.5930,\n",
      "         6.3626, 4.7936, 5.5152, 0.0000, 5.3543, 6.4542, 6.1178, 5.9057, 3.2557,\n",
      "         0.0000, 6.5547, 0.0000, 0.6922, 2.1954, 0.0000, 4.6614, 0.0000, 5.7242,\n",
      "         4.3147, 1.3841, 4.9807, 0.6917, 0.6917, 0.0000, 6.2886, 0.0000, 0.0000,\n",
      "         4.6121, 6.0084, 0.0000, 0.6918, 5.3008, 0.0000, 6.1462, 4.6230, 0.0000,\n",
      "         4.6133, 6.2206, 5.3210, 2.9424, 4.6129, 0.0000, 3.0423, 0.0000, 0.0000,\n",
      "         0.0000, 5.7721, 3.6353, 0.6920, 6.0309, 1.6078, 5.2861, 0.0000, 0.0000,\n",
      "         5.7150, 4.7253, 0.0000, 4.9536, 0.0000, 0.0000, 4.9322, 4.6229, 0.0000,\n",
      "         0.0000, 5.3307, 0.0000, 0.0000, 0.0000, 1.0970, 0.6920, 0.0000, 1.0971,\n",
      "         0.6921, 0.0000, 0.0000, 5.6288, 4.6224, 5.7532, 0.0000, 0.0000, 1.0969,\n",
      "         5.7175, 4.7245, 6.2677, 0.0000, 0.6913, 0.0000, 4.9453, 4.8249, 0.0000,\n",
      "         0.0000, 0.0000, 4.6213]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_money_turnover_per_order_xs\n",
      "tensor([[3.5353, 0.0000, 0.0000, 3.2903, 0.0000, 0.0000, 3.7319, 3.7785, 0.0000,\n",
      "         0.0000, 0.0000, 1.2229, 0.0000, 4.4531, 2.8894, 4.6450, 3.0749, 3.7367,\n",
      "         3.9878, 0.0000, 0.0000, 5.2162, 3.0432, 2.1959, 4.5295, 4.8344, 4.5930,\n",
      "         4.7957, 3.9296, 4.8843, 0.0000, 3.4363, 4.5166, 4.8029, 4.8006, 3.2557,\n",
      "         0.0000, 5.0585, 0.0000, 0.6922, 1.2978, 0.0000, 3.9776, 0.0000, 3.7977,\n",
      "         3.2425, 1.3841, 3.8957, 0.6917, 0.6917, 0.0000, 3.9091, 0.0000, 0.0000,\n",
      "         3.9288, 3.5501, 0.0000, 0.6918, 5.0114, 0.0000, 3.2916, 3.9396, 0.0000,\n",
      "         3.9300, 4.0547, 4.9082, 1.7030, 3.5339, 0.0000, 3.0423, 0.0000, 0.0000,\n",
      "         0.0000, 4.3371, 2.9681, 0.6920, 4.2836, 1.6078, 3.5194, 0.0000, 0.0000,\n",
      "         3.7886, 3.6443, 0.0000, 3.8690, 0.0000, 0.0000, 3.0954, 3.9492, 0.0000,\n",
      "         0.0000, 4.0278, 0.0000, 0.0000, 0.0000, 0.6919, 0.6920, 0.0000, 0.6920,\n",
      "         0.6921, 0.0000, 0.0000, 4.0336, 3.5432, 4.0198, 0.0000, 0.0000, 0.6919,\n",
      "         3.4135, 3.4472, 4.2308, 0.0000, 0.6913, 0.0000, 4.6442, 3.2470, 0.0000,\n",
      "         0.0000, 0.0000, 4.6213]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logret1_xs\n",
      "tensor([[-7.7680e-01, -7.7680e-01, -6.8623e-02,  6.7328e-01, -4.8532e-02,\n",
      "          3.7172e-01,  5.6857e-02, -2.5074e-01, -9.9375e-01,  1.7405e-01,\n",
      "          6.8433e-02,  7.0038e-01,  1.2492e-01, -1.3495e+00,  1.0666e+00,\n",
      "          5.1070e-01, -4.1751e-02, -6.4015e-03, -6.7136e-01, -2.6296e-02,\n",
      "          3.9817e-02, -9.8889e-01, -8.4128e-01, -1.1107e+00, -2.2903e+00,\n",
      "         -1.5481e+00, -1.5099e+00, -9.2775e-01, -7.4993e-01, -1.3067e+00,\n",
      "          1.8113e-01, -3.6812e-01, -2.8201e+00,  1.3158e-01,  1.6369e+00,\n",
      "          1.9151e-01,  3.7371e-01,  1.7664e+00,  1.1575e+00, -1.1125e-01,\n",
      "          7.9329e-02,  8.9109e-01, -3.4531e+00, -2.2108e+00, -1.5161e+00,\n",
      "         -5.4590e-01, -7.1532e-01,  1.3627e-01, -5.4038e-01,  9.0180e-01,\n",
      "         -7.0203e-01, -1.4106e+00, -1.5298e-01, -1.5299e-01, -1.0891e-03,\n",
      "          2.9099e+00, -2.2905e-01, -4.5718e-01,  2.1538e+00,  1.5167e-01,\n",
      "          4.0605e+00,  1.8936e-01,  5.3510e-01,  1.5238e+00,  4.5908e-01,\n",
      "         -1.1990e+00, -7.9428e-01, -1.3792e+00,  2.2514e-01, -9.8336e-01,\n",
      "          8.9667e-01, -5.0048e-01, -5.0050e-01, -9.5374e-01,  6.0001e-02,\n",
      "          2.2383e-01,  3.2025e-01,  1.1402e+00,  5.8908e-01, -1.5750e+00,\n",
      "         -3.1343e-02,  1.3332e+00,  5.6344e-01,  9.6391e-01,  3.1342e-01,\n",
      "         -3.0081e+00, -5.3374e-01, -6.4902e-01,  1.7861e+00, -1.9742e-01,\n",
      "          1.2865e+00, -3.0998e-01, -1.3622e+00,  5.5502e-01,  0.0000e+00,\n",
      "         -2.9586e+00,  1.2638e+00,  7.6171e-01,  6.1231e-01,  9.0020e-01,\n",
      "         -1.0147e-03, -1.3182e+00, -1.3183e+00, -7.5227e-01, -1.5973e+00,\n",
      "         -8.1991e-01,  1.1986e+00,  1.4711e-01, -2.0264e+00, -1.3291e+00,\n",
      "         -5.3050e+00, -2.0951e+00,  5.3387e-01, -7.8693e-02,  5.3789e-01,\n",
      "          1.9783e+00, -2.8291e+00,  1.2949e+00, -1.7899e-01, -4.8892e-02]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logret2_xs\n",
      "tensor([[ 0.9859,  0.9859, -0.1455, -1.2756,  1.5009, -0.0909, -0.0406, -0.5837,\n",
      "         -0.5796, -0.9822,  0.3893,  0.4608, -0.0628, -0.1517,  0.0359,  0.9919,\n",
      "         -0.5060,  0.0652, -1.9585, -0.5432,  0.4822, -0.3305, -0.2238,  0.0935,\n",
      "         -2.9143, -2.2891, -1.2099, -1.0210, -2.9866,  0.9696,  0.1230, -1.1505,\n",
      "         -1.7135,  0.2416,  1.8676, -0.2321,  0.1684,  1.8445, -0.0817, -1.3762,\n",
      "          1.8866,  1.9385, -2.1587, -1.1067, -4.4063, -0.5619, -0.2444, -0.4791,\n",
      "         -0.6453, -0.6869,  1.8901, -2.1611, -0.0178, -0.0178, -0.3498,  0.6256,\n",
      "          0.5493,  2.6553,  1.5744, -0.6500,  2.3321,  2.1582,  1.2279,  0.2242,\n",
      "         -0.4004, -0.2631, -2.0794, -0.5214,  0.0000,  1.0671, -0.2156, -0.0635,\n",
      "         -0.0635, -3.0902,  1.6110,  0.0915, -0.6223,  1.2058, -0.1832,  0.1907,\n",
      "         -0.1133,  1.5597,  0.1667, -1.8668, -0.6013, -0.3198,  1.0539, -1.2939,\n",
      "          2.3039,  1.6961,  0.4793, -3.0270, -1.4520,  1.6252, -0.0483, -1.6339,\n",
      "          0.6728,  1.1467, -0.3846, -0.1781,  1.4351, -0.7536, -0.7537, -1.5527,\n",
      "         -2.6237,  0.2717, -0.5195,  0.2717,  0.9765, -3.4961, -3.2493, -3.2187,\n",
      "         -0.4303,  0.1247,  1.2807,  0.0454, -2.5902,  1.0193,  0.6861, -0.4485]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_directional_volume1_xs\n",
      "tensor([[   4.2000,   70.0000,  106.3333,   34.7500,  245.0000,  212.0000,\n",
      "          303.2000,  -21.7500,  -24.3333,  -51.5000,  -70.2000,  -80.0000,\n",
      "         -127.0000,   -1.0000,   12.2500,   61.5000,   82.0000,   57.6667,\n",
      "         -217.0000, -191.0000, -199.0000, -127.8000, -184.5000, -257.2500,\n",
      "          -76.4000, -102.6000,  -63.4000,  -81.2500, -136.7500,   20.0000,\n",
      "          -77.7500, -219.5000,  -30.7500,   14.0000,   -4.5000,  -96.0000,\n",
      "           -4.0000,   64.7500,  -74.0000,  -94.6000,  -91.0000,  -99.0000,\n",
      "          -40.0000,   97.0000,   81.7500, -100.0000, -100.6667, -118.3333,\n",
      "          -33.0000,  -98.2500,  -50.0000, -121.7500,    0.0000,  -99.0000,\n",
      "          -99.0000,  -98.6000, -152.5000,    0.0000,   35.6000,  148.5000,\n",
      "          -74.5000,   76.7500,  157.0000, -159.2000, -155.4000,  -12.7500,\n",
      "          -58.2000,  -11.0000, -111.0000,  -21.6000, -104.3333,    0.0000,\n",
      "          -28.0000,  -46.2500,  -99.0000,  -98.5000,   26.7500,   -1.5000,\n",
      "          -19.2500,   88.7500,   80.5000,  111.4000,   -8.2500,  -66.0000,\n",
      "          -85.5000,  -19.6000,   -2.5000,   75.0000,  143.5000,  176.0000,\n",
      "          100.0000,  -32.2000,    2.0000,    7.0000,    7.0000,   10.0000,\n",
      "          -12.5000,  -23.0000,  -98.5000, -188.0000,  -99.0000,    0.0000,\n",
      "          -80.0000,  -24.7500,   -3.0000,    0.8000,    1.0000,    0.6667,\n",
      "           21.0000,  -49.5000,  -33.0000,  -44.0000, -137.4000, -115.0000,\n",
      "          -28.7500,  -14.6667,   11.3333,  -23.3333,  -93.2500, -123.7500]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_directional_volume2_xs\n",
      "tensor([[  17.8000,  -99.0000,  -99.0000,   26.2500,  -41.7500,    0.0000,\n",
      "           34.0000,    5.7500,  -95.6667,  -46.5000,  -89.8000,  -77.0000,\n",
      "          -44.6000,  -24.0000,  -44.0000,   51.5000,   33.0000,  -10.6667,\n",
      "           18.0000,   55.0000,   16.0000,   -8.0000, -165.7500, -162.7500,\n",
      "          -50.6000,  -72.4000, -159.6000,  -76.2500,   56.2500,   -7.5000,\n",
      "          -50.0000,   68.0000,  -71.2500,  -93.5000, -121.2500,  -68.4000,\n",
      "          -77.0000,   23.5000,  -19.5000,   78.2000,   15.6000,  -78.0000,\n",
      "         -126.6667, -101.0000,   50.5000,  -15.7500,  -48.3333,  -87.6667,\n",
      "           57.6667,  244.5000, -154.0000,  -52.7500,    0.0000,  -99.0000,\n",
      "          -59.0000,   61.6000,   38.0000, -100.0000,  -95.6000,   47.5000,\n",
      "           37.2500,  101.2500,  -39.0000,  -51.2000,    1.8000,   13.2500,\n",
      "           26.4000,    0.0000,    0.0000, -145.4000,  -91.3333,    0.0000,\n",
      "          -95.0000,   14.2500,  -60.0000,  -59.5000,  113.5000,  -10.2500,\n",
      "          105.7500,   -4.0000,   50.7500,   -2.4000,  -19.7500,  112.0000,\n",
      "          164.7500,   71.0000,   -0.5000,   55.2000,    3.2500,  -11.7500,\n",
      "          -20.0000,   78.8000,  178.0000,   -3.0000,   -2.0000,    0.0000,\n",
      "          -10.5000,  -22.0000,  -12.5000,  -13.5000, -157.0000,    0.0000,\n",
      "         -276.0000, -144.2500,   32.2500,    1.6000,   50.0000,    6.6667,\n",
      "          -96.4000,  -43.7500, -127.2500, -120.8000,  -86.8000,  -89.3333,\n",
      "          -79.7500,   15.3333,   35.3333,  -50.0000,  -13.7500,   -7.7500]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_price_spread1_xs\n",
      "tensor([[0.2467, 0.1835, 0.1762, 0.1817, 0.2643, 0.2423, 0.2291, 0.2313, 0.2644,\n",
      "         0.2313, 0.2115, 0.2159, 0.1939, 0.2644, 0.1982, 0.2863, 0.1982, 0.1909,\n",
      "         0.2129, 0.2203, 0.1982, 0.2379, 0.2754, 0.1598, 0.2645, 0.3086, 0.2337,\n",
      "         0.1764, 0.2701, 0.2371, 0.1433, 0.1709, 0.2592, 0.1930, 0.2812, 0.4852,\n",
      "         0.2315, 0.3583, 0.3032, 0.2161, 0.1852, 0.2095, 0.4852, 0.3639, 0.3033,\n",
      "         0.2869, 0.2648, 0.2059, 0.1839, 0.1986, 0.1986, 0.2428, 0.2042, 0.1655,\n",
      "         0.1765, 0.3839, 0.2923, 0.3089, 0.2912, 0.1985, 0.2426, 0.2150, 0.2425,\n",
      "         0.2160, 0.2028, 0.2976, 0.1940, 0.3308, 0.3308, 0.2823, 0.2646, 0.2536,\n",
      "         0.2426, 0.2812, 0.2647, 0.1765, 0.1599, 0.1102, 0.3583, 0.2646, 0.2316,\n",
      "         0.2249, 0.4355, 0.3528, 0.3307, 0.3573, 0.2095, 0.2250, 0.2316, 0.3087,\n",
      "         0.2205, 0.2602, 0.3307, 0.2867, 0.2867, 0.2206, 0.2206, 0.2867, 0.2867,\n",
      "         0.3308, 0.2425, 0.2536, 0.2647, 0.2702, 0.2702, 0.2603, 0.0662, 0.0882,\n",
      "         0.2162, 0.2152, 0.3423, 0.3048, 0.2739, 0.2576, 0.2484, 0.3165, 0.3239,\n",
      "         0.3092, 0.2485, 0.2264]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_price_spread2_xs\n",
      "tensor([[0.3083, 0.2569, 0.2423, 0.2863, 0.4240, 0.4185, 0.3788, 0.3689, 0.3672,\n",
      "         0.2864, 0.2732, 0.2908, 0.3172, 0.3084, 0.2808, 0.3359, 0.3480, 0.3230,\n",
      "         0.3451, 0.3304, 0.3084, 0.3305, 0.3690, 0.3250, 0.3482, 0.3880, 0.2954,\n",
      "         0.2977, 0.4190, 0.3749, 0.2977, 0.3088, 0.3584, 0.3253, 0.4631, 0.6175,\n",
      "         0.4742, 0.5071, 0.4631, 0.4145, 0.4101, 0.3748, 0.6028, 0.5072, 0.4964,\n",
      "         0.4523, 0.3751, 0.3163, 0.2795, 0.2538, 0.2647, 0.2979, 0.2704, 0.2428,\n",
      "         0.2560, 0.4457, 0.4192, 0.4853, 0.5559, 0.3639, 0.3473, 0.3693, 0.4079,\n",
      "         0.3439, 0.2998, 0.3858, 0.3704, 0.5292, 0.5292, 0.3837, 0.3528, 0.3418,\n",
      "         0.3308, 0.5128, 0.3419, 0.2647, 0.2812, 0.3252, 0.5568, 0.3749, 0.2977,\n",
      "         0.3043, 0.5181, 0.4410, 0.4576, 0.4367, 0.2702, 0.3484, 0.3969, 0.4355,\n",
      "         0.3748, 0.4587, 0.4852, 0.3970, 0.3970, 0.3970, 0.3529, 0.3970, 0.3970,\n",
      "         0.4631, 0.3749, 0.4301, 0.4853, 0.4026, 0.3971, 0.3618, 0.2868, 0.2941,\n",
      "         0.4192, 0.3531, 0.5354, 0.3755, 0.3357, 0.3018, 0.3698, 0.5152, 0.5889,\n",
      "         0.4417, 0.3147, 0.2871]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_bid_spread_xs\n",
      "tensor([[0.3964, 0.4404, 0.4404, 0.3303, 0.2752, 0.4404, 0.4404, 0.5507, 0.4405,\n",
      "         0.3304, 0.3969, 0.5289, 0.7491, 0.2202, 0.3306, 0.2206, 1.0133, 1.1015,\n",
      "         0.8815, 0.8815, 0.8815, 0.2203, 0.5510, 0.7161, 0.2203, 0.5292, 0.2204,\n",
      "         0.2204, 1.1580, 0.9374, 0.4414, 0.7718, 0.2205, 0.2757, 1.0479, 0.6619,\n",
      "         1.9852, 0.3310, 0.9923, 1.1028, 0.7498, 0.5510, 0.5145, 0.9926, 1.1034,\n",
      "         0.9933, 0.8093, 0.5885, 0.4412, 0.3309, 0.4412, 0.2758, 0.4138, 0.5519,\n",
      "         0.5742, 0.3976, 0.6624, 0.2206, 1.6769, 0.6618, 0.6062, 0.8820, 0.9924,\n",
      "         0.6174, 0.5732, 0.6612, 1.2788, 0.2205, 0.2205, 0.3969, 0.6614, 0.6614,\n",
      "         0.6614, 1.0478, 0.3308, 0.5512, 0.6614, 1.0477, 0.6066, 0.2205, 0.2205,\n",
      "         0.5732, 0.6064, 0.6617, 0.7722, 0.4854, 0.2211, 0.6616, 0.8821, 0.6062,\n",
      "         0.8823, 0.9704, 0.8824, 0.8824, 0.8824, 0.2205, 0.2205, 0.2205, 0.4416,\n",
      "         0.6615, 0.6620, 0.7724, 0.8827, 0.7172, 0.8826, 0.2650, 1.5443, 1.5443,\n",
      "         0.4854, 0.9381, 0.6624, 0.2208, 0.2208, 0.2208, 0.7176, 1.6193, 0.9574,\n",
      "         1.1046, 0.4415, 0.3864]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_ask_spread_xs\n",
      "tensor([[0.2201, 0.2937, 0.2201, 0.7155, 1.3213, 1.3213, 1.0569, 0.8260, 0.5873,\n",
      "         0.2202, 0.2202, 0.2202, 0.4847, 0.2202, 0.4958, 0.2752, 0.4844, 0.2203,\n",
      "         0.4405, 0.2202, 0.2202, 0.7049, 0.3855, 0.9361, 0.6170, 0.2644, 0.3966,\n",
      "         0.9920, 0.3306, 0.4410, 1.1027, 0.6063, 0.7722, 1.0477, 0.7716, 0.6612,\n",
      "         0.4412, 1.1571, 0.6063, 0.8821, 1.4989, 1.1017, 0.6612, 0.4408, 0.8271,\n",
      "         0.6616, 0.2944, 0.5150, 0.5150, 0.2211, 0.2205, 0.2757, 0.2482, 0.2206,\n",
      "         0.2206, 0.2205, 0.6066, 1.5440, 0.9702, 0.9923, 0.4408, 0.6612, 0.6616,\n",
      "         0.6614, 0.3967, 0.2205, 0.4848, 1.7637, 1.7637, 0.6173, 0.2204, 0.2204,\n",
      "         0.2204, 1.2680, 0.4409, 0.3307, 0.5515, 1.1026, 1.3777, 0.8820, 0.4409,\n",
      "         0.2204, 0.2203, 0.2204, 0.4960, 0.3088, 0.3857, 0.5732, 0.7713, 0.6611,\n",
      "         0.6611, 1.0142, 0.6617, 0.2204, 0.2204, 1.5439, 1.1028, 0.8823, 0.6616,\n",
      "         0.6618, 0.6618, 0.9925, 1.3233, 0.6068, 0.3860, 0.7503, 0.6621, 0.5149,\n",
      "         1.5444, 0.4411, 1.2692, 0.4860, 0.3977, 0.2211, 0.4967, 0.3677, 1.6927,\n",
      "         0.2207, 0.2207, 0.2207]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_total_volume_xs\n",
      "tensor([[7.0309, 6.2653, 6.4536, 6.9670, 7.5380, 6.1377, 7.9356, 6.8617, 5.9738,\n",
      "         6.4877, 6.7581, 6.9295, 7.1538, 4.0775, 6.9866, 8.0020, 8.0449, 7.4685,\n",
      "         6.9315, 6.6958, 6.8459, 7.1808, 7.6401, 7.7558, 6.8200, 6.8896, 7.6658,\n",
      "         6.9575, 7.2086, 7.2800, 7.4512, 7.1982, 7.6113, 7.3402, 7.2034, 7.4343,\n",
      "         5.5568, 7.4372, 6.7811, 7.0379, 7.0527, 6.1026, 6.8512, 6.1841, 6.9866,\n",
      "         6.2577, 6.9660, 6.9949, 7.1025, 7.5486, 6.0426, 7.0148, 0.0000, 6.0039,\n",
      "         7.0992, 7.2485, 7.2086, 6.2166, 7.6192, 6.4877, 7.1808, 7.4691, 6.8035,\n",
      "         7.3746, 7.6540, 6.5028, 6.6412, 4.0943, 5.0752, 6.9508, 6.5073, 0.0000,\n",
      "         6.3333, 6.6053, 5.7900, 5.8021, 7.5262, 6.8669, 7.0344, 6.6067, 6.8565,\n",
      "         7.4110, 7.0344, 6.7811, 7.0493, 6.3491, 5.3423, 7.0951, 6.8669, 6.8835,\n",
      "         5.7961, 6.9037, 5.4337, 5.4765, 5.4806, 4.7958, 4.6151, 3.9512, 5.4510,\n",
      "         6.1985, 5.7071, 0.0000, 6.1026, 7.0758, 6.1985, 5.6937, 4.7095, 3.5553,\n",
      "         7.0510, 6.7238, 6.7499, 6.7923, 7.1884, 6.5876, 6.9670, 6.7105, 6.3404,\n",
      "         6.2672, 6.3784, 6.6107]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_volume_imbalance_xs\n",
      "tensor([[4.6653, 3.4012, 2.1203, 4.1271, 5.3193, 5.3613, 5.8236, 5.0783, 4.7958,\n",
      "         4.5951, 5.0814, 5.0626, 5.1510, 3.2581, 3.8011, 4.9090, 5.0999, 4.1795,\n",
      "         5.2983, 4.9200, 5.2149, 4.9185, 5.8615, 6.0426, 4.8520, 5.1705, 5.4116,\n",
      "         5.2730, 4.4886, 4.6492, 4.8579, 5.1090, 4.6347, 5.0073, 4.8422, 5.1084,\n",
      "         4.4067, 4.9541, 4.5747, 2.8565, 4.3360, 5.1818, 5.1259, 4.6052, 4.8922,\n",
      "         4.7600, 5.0106, 5.3327, 4.7391, 4.9921, 5.3230, 5.1676, 0.0000, 5.2933,\n",
      "         5.0739, 3.6376, 4.7493, 4.6151, 4.9459, 5.2832, 4.0818, 5.1874, 4.7791,\n",
      "         5.3538, 5.0408, 3.8607, 4.2370, 2.4849, 4.7185, 5.1240, 5.2815, 0.0000,\n",
      "         4.8203, 3.9120, 5.0752, 5.0689, 4.9505, 3.3759, 4.8363, 4.4514, 4.8847,\n",
      "         4.7005, 3.4177, 4.1217, 4.5191, 4.1141, 3.5115, 4.8828, 4.9955, 5.1075,\n",
      "         4.3944, 4.7808, 5.1985, 1.6094, 1.7918, 2.4849, 3.1781, 3.8286, 4.7185,\n",
      "         5.3107, 5.5491, 0.0000, 5.8777, 5.1417, 3.4095, 3.8330, 3.9512, 2.1203,\n",
      "         4.4864, 4.5512, 5.0830, 5.1108, 5.4170, 5.3246, 4.8714, 2.1972, 4.5609,\n",
      "         4.3086, 4.6821, 4.8866]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_money_turnover1_xs\n",
      "tensor([[2.1963, 1.9449, 1.9449, 4.6624, 5.5824, 4.6141, 5.3171, 2.3016, 1.3854,\n",
      "         4.3808, 1.7908, 1.7909, 1.7909, 1.0978, 3.3663, 5.7858, 4.6624, 4.6434,\n",
      "         4.1261, 4.1421, 1.3855, 2.0784, 5.3458, 5.7607, 4.7690, 1.9444, 4.6520,\n",
      "         1.7901, 5.4909, 6.0569, 1.6077, 1.7899, 6.0472, 5.3156, 5.1156, 5.2235,\n",
      "         1.0971, 5.5032, 4.6423, 3.3303, 3.8267, 1.0973, 4.6323, 3.2165, 5.1621,\n",
      "         1.6073, 5.3153, 1.3842, 1.3842, 1.7895, 1.0968, 4.6414, 0.0000, 1.0967,\n",
      "         1.7893, 4.7069, 5.3105, 4.6124, 5.4137, 4.6610, 5.9969, 4.7938, 5.3499,\n",
      "         1.7902, 3.2942, 4.9180, 1.7901, 0.6921, 0.6921, 2.3006, 2.7706, 0.0000,\n",
      "         4.8419, 1.6076, 1.0971, 1.3846, 5.9917, 5.9767, 5.1337, 4.5304, 4.5304,\n",
      "         5.6038, 6.0190, 4.6519, 1.6079, 1.9440, 3.5532, 1.9439, 4.8421, 4.5725,\n",
      "         4.6131, 2.3960, 0.6920, 4.5517, 4.5517, 1.3844, 1.0970, 1.0971, 1.3846,\n",
      "         1.0972, 0.6921, 0.0000, 3.0888, 5.4091, 4.2879, 1.7895, 1.0969, 1.3844,\n",
      "         4.7423, 1.6071, 3.9855, 2.6357, 2.6357, 3.2154, 4.6600, 5.5181, 3.8676,\n",
      "         1.3837, 3.3288, 1.6066]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0023]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id\n",
      "['9-633']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_id\n",
      "tensor([9.])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds_in_bucket_xs\n",
      "tensor([[  5.,  10.,  15.,  20.,  25.,  30.,  35.,  40.,  45.,  50.,  55.,  60.,\n",
      "          65.,  70.,  75.,  80.,  85.,  90.,  95., 100., 105., 110., 115., 120.,\n",
      "         125., 130., 135., 140., 145., 150., 155., 160., 165., 170., 175., 180.,\n",
      "         185., 190., 195., 200., 205., 210., 215., 220., 225., 230., 235., 240.,\n",
      "         245., 250., 255., 260., 265., 270., 275., 280., 285., 290., 295., 300.,\n",
      "         305., 310., 315., 320., 325., 330., 335., 340., 345., 350., 355., 360.,\n",
      "         365., 370., 375., 380., 385., 390., 395., 400., 405., 410., 415., 420.,\n",
      "         425., 430., 435., 440., 445., 450., 455., 460., 465., 470., 475., 480.,\n",
      "         485., 490., 495., 500., 505., 510., 515., 520., 525., 530., 535., 540.,\n",
      "         545., 550., 555., 560., 565., 570., 575., 580., 585., 590., 595., 600.]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15084/2440942144.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         )\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "\n",
    "    \n",
    "dataloader_train = DataLoader(dataset, batch_size=1,\n",
    "                                shuffle=True, num_workers=0, pin_memory=False)#, collate_fn=optiver_custom_collate_func)\n",
    "\n",
    "\n",
    "#  encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "# >>> src = torch.rand(10, 32, 512)\n",
    "# >>> out = encoder_layer(src)   \n",
    "for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "    i += 1\n",
    "    for k,v in Feature_X.items():\n",
    "        print(k)\n",
    "        print(feature_transform(v,k))\n",
    "        input()\n",
    "    print(feature_y)\n",
    "    input()\n",
    "# batch = []\n",
    "# for idx in range(len(dataset)):\n",
    "#     batch.append(dataset[idx])\n",
    "#     if idx % 128 == 0:\n",
    "#         features_x = [x[0] for x in batch]\n",
    "#         features_y = [x[1] for x in batch]\n",
    "#         features_y = torch.tensor(features_y).reshape(-1,1)\n",
    "# #         print(features_y)\n",
    "# #         input()\n",
    "#         batch = []\n",
    "    \n",
    "#     y = feature_y.to(device) * output_scaling \n",
    "#     print(Feature_X['logret1_xs'].type())\n",
    "#     pred = model(Feature_X)\n",
    "#     print(pred.type())\n",
    "#     input()\n",
    "#     for stk in Feature_X['row_id']:\n",
    "        \n",
    "#         stockid.add(stk.split(\"-\")[0])\n",
    "# for i in range(len(dataset)-10):\n",
    "#     dataset[i]\n",
    "print(\"-->\", (time.time()-stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f03435-5cde-40bf-a86b-656ab4515ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce561c-5b64-44bf-882b-41f6250b1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated(device)/1024/1024/1024\n",
    "# model.to(\"cpu\")\n",
    "# torch.cuda.memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a966ee2c-8547-48be-a912-8d05bf48b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.init()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
