{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2f18e2-9894-4f0c-bd57-8b37afc02309",
   "metadata": {},
   "source": [
    "### Can our model predict current volatility?  (forget future; first it should be capable of predicting current one with given features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c4e895-fd66-490d-a8b8-d28b324d3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from optiver_features_handler import get_features_map_for_stock, get_row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad9958-4061-4195-b9d8-77f142bc7fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750eb0e3-2860-490b-bc3c-2a512485a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\n",
    "OUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\n",
    "MODEL_OUTPUT_DIRECTORY = os.path.join(OUTPUT_DIRECTORY,\"models\")\n",
    "os.makedirs(OUTPUT_DIRECTORY,exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIRECTORY,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d3a0b1-c7aa-4df5-a088-baf5a8fb5de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02947ab-aa1d-4af0-9d6e-7a51cff159ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interval_seconds = 5\n",
    "data_intervals_count = int(600/data_interval_seconds)\n",
    "class OptiverRealizedVolatilityDataset(Dataset):\n",
    "    def __init__(self, data_directory, mode=\"train\", lazy_load=True):\n",
    "        \"\"\"initializes Optiver Competition dataset\n",
    "        `mode`: train|test\n",
    "        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n",
    "        \"\"\"\n",
    "        print(\"INIT: OptiverRealizedVolatilityDataset\")\n",
    "        if mode.lower() not in ['train','test']:\n",
    "            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n",
    "        self.data_directory = data_directory\n",
    "        self.mode = mode.lower()\n",
    "        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n",
    "#         if self.mode == 'train':\n",
    "#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n",
    "        if self.mode == 'test':\n",
    "            self.main_df['target'] = 0\n",
    "        \n",
    "        self.cache_stocks_done_set = set()\n",
    "        # this is our final features lookup where we park all our features which can be addressed by row_id\n",
    "        # which is individual train/test.csv row id using 'stock_id`-`time_id`\n",
    "        self.cache_rowid_feature_map = {}\n",
    "        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n",
    "        targets = self.main_df['target'].tolist()\n",
    "        self.stock_possible_timeids_list = {}\n",
    "        for idx, row_id in enumerate(row_id_series.tolist()):\n",
    "            stock_id = int(row_id.split('-')[0])\n",
    "            time_id = int(row_id.split('-')[1])\n",
    "            self.cache_rowid_feature_map[row_id] = {'target':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n",
    "            \n",
    "            # below code is to make sure what timeids we expect from stock data extractor\n",
    "            # in case of missing parquet files we'll have to know the keys to fill default values into\n",
    "            if stock_id not in self.stock_possible_timeids_list:\n",
    "                self.stock_possible_timeids_list[stock_id] = []\n",
    "            self.stock_possible_timeids_list[stock_id].append(time_id)\n",
    "            \n",
    "        \n",
    "        if lazy_load == False:\n",
    "            worker_data = []\n",
    "            for gkey, gdf in self.main_df.groupby(['stock_id']):\n",
    "                worker_data.append((self.data_directory, self.mode, gkey))\n",
    "#             print(\"---------- CPU COUNG:\", multiprocessing.cpu_count())\n",
    "            # NOTE: this was hell of a hunt; this windows and pytorch and jupyter combination is too tedious\n",
    "            #       make sure the function that we distribute don't call pytorch\n",
    "            chunksize = multiprocessing.cpu_count() * 2\n",
    "            processed = 0\n",
    "            for worker_data_chunk in [worker_data[i * chunksize:(i + 1) * chunksize] for i in range((len(worker_data) + chunksize - 1) // chunksize )]:\n",
    "                with Pool(multiprocessing.cpu_count()) as p:\n",
    "                    \n",
    "                    feature_set_list = p.starmap(get_features_map_for_stock, worker_data_chunk)\n",
    "                    \n",
    "                    for feature_map in feature_set_list:\n",
    "                        for rowid, features_dict in feature_map.items():\n",
    "                            for fkey,fval in features_dict.items():\n",
    "                                self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                            self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                        # udpate the indications that we've already fetched this stock and the lazy loader code won't fetch this again\n",
    "                        self.cache_stocks_done_set.add(int(rowid.split('-')[0]))\n",
    "                    \n",
    "                    processed += chunksize\n",
    "                    print(f\"Processed and loaded {processed} stocks features.\")\n",
    "    \n",
    "    def __cache_generate_features(self, main_stock_id, main_time_id):\n",
    "            \n",
    "            \n",
    "            main_row_id = get_row_id(main_stock_id, main_time_id)\n",
    "            if main_stock_id not in self.cache_stocks_done_set:\n",
    "#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))   \n",
    "                # we'll combine the featureset with the bigger feature set of all stocks\n",
    "                feature_map = get_features_map_for_stock(self.data_directory, self.mode, main_stock_id)\n",
    "                # NOTE: sometime we might now have parquet files in that case we'll have 3 entried in .csv while only 1 gets returned in feature map\n",
    "                # we need to cover for that disparity\n",
    "                for time_id in self.stock_possible_timeids_list[main_stock_id]:\n",
    "                    expected_row_id = get_row_id(main_stock_id, time_id)\n",
    "                    if expected_row_id not in feature_map:\n",
    "                        feature_map[expected_row_id] = {}\n",
    "                for rowid, features_dict in feature_map.items():\n",
    "                    for fkey,fval in features_dict.items():\n",
    "                        self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                    self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                self.cache_stocks_done_set.add(main_stock_id)\n",
    "#             print(self.cache_rowid_feature_map[main_row_id])\n",
    "#             print(torch.tensor([self.cache_rowid_feature_map[main_row_id].get('book_realized_volatility',0)]))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map[main_row_id].get('log_return1_2s', [0]*(int(600/2)))))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map.get('book_directional_volume1_2s', [0]*(int(600/2)))))\n",
    "            return self.cache_rowid_feature_map[main_row_id]\n",
    "        \n",
    "    @staticmethod\n",
    "    def transform_to_01_realized_volatility_linear_data(features_dict):\n",
    "        return (\n",
    "                {\n",
    "                    'row_id':features_dict['row_id'],\n",
    "#                     'book_realized_volatility':torch.tensor([features_dict.get('book_realized_volatility',0)]),\n",
    "                    'logrett_xs':torch.tensor(features_dict.get('logrett_xs', [0]*(int(600/data_interval_seconds)))),\n",
    "                    'trade_volume_xs':torch.tensor(features_dict.get('trade_volume_xs', [0]*(int(600/data_interval_seconds)))),\n",
    "                    'trade_ordercount_xs':torch.tensor(features_dict.get('trade_ordercount_xs', [0]*(int(600/data_interval_seconds)))),\n",
    "                    'logret1_xs':torch.tensor(features_dict.get('logret1_xs', [0]*(int(600/data_interval_seconds)))),\n",
    "                    'logret2_xs':torch.tensor(features_dict.get('logret2_xs', [0]*(int(600/data_interval_seconds)))),\n",
    "                    'book_dirvolume_xs':torch.tensor(features_dict.get('book_dirvolume_xs', [0]*(int(600/data_interval_seconds)))),\n",
    "#                     'askp2_1s':torch.tensor(features_dict.get('askp2_1s', [0]*(int(600/1)))),\n",
    "#                     'book_directional_volume1_1s':torch.tensor(features_dict.get('book_directional_volume1_1s', [0]*(int(600/1)))) \n",
    "                },\n",
    "                torch.tensor([features_dict['target']])\n",
    "#                 [features_dict['target']]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: handle for num_workers more than 0\n",
    "        #      using https://pytorch.org/docs/stable/data.html\n",
    "        #      using torch.util.data.get_worker_info()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        stock_id = self.main_df.at[idx, 'stock_id']\n",
    "        time_id = self.main_df.at[idx, 'time_id']\n",
    "        x,y = self.__cache_generate_features(stock_id,time_id)\n",
    "#         x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efcee691-c9ce-481f-a4e8-dbf97b66b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT: OptiverRealizedVolatilityDataset\n",
      "Processed and loaded 32 stocks features.\n",
      "Processed and loaded 64 stocks features.\n",
      "Processed and loaded 96 stocks features.\n",
      "Processed and loaded 128 stocks features.\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\", lazy_load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d58ff-26aa-4b02-b2ac-991ccb366c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for x in range(0,9):\n",
    "#     print(dataset[x])\n",
    "# dataset[10000] #[0]['bidp1_1s']\n",
    "dataset[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a2124b8d-39fb-4801-a3c1-378ee390a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset, batch_size=4096,shuffle=True, num_workers=0, pin_memory=True)\n",
    "sizes = set()\n",
    "for train_batch_idx, (feature_dict, feature_y) in enumerate(dataloader_train):\n",
    "    sizes.add(f\"{feature_dict['logrett_xs'].size()}\")\n",
    "        \n",
    "        \n",
    "#         print(val)\n",
    "#         input()\n",
    "#     print(x)\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ca3693b7-4338-49c6-8a4f-9a8205d4aa33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'torch.Size([2948, 120])', 'torch.Size([4096, 120])'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a72c9b-fab8-4536-b125-abc46cc800b6",
   "metadata": {},
   "source": [
    "### Learnings about model CNN input\n",
    "- it's better to use multiple channel for logreturn1 and logreturn2 than stacking it and using as one channel\n",
    "- 2 channels input for CNN is better than stacking it(dim 2, which is logret1_t1, logret2_t1, logret1_t2, logret2_t2...) and using it as one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc4b98ff-b328-465e-a04e-c25eef8ea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "15e35973-37f1-408a-b47f-3a8fa6c9879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.cnn_stack = nn.Sequential(\n",
    "            nn.Conv1d(6, 12, kernel_size=6, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.Conv1d(12, 12, kernel_size=6, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(12, 12, kernel_size=6, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Conv1d(8, 8, kernel_size=4, stride=2, padding=0), \n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "        )\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.LazyLinear(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, 64),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "#         self.basic_stack = nn.Sequential(\n",
    "#             nn.Linear(int(600/2)*1,512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "#             nn.Linear(512,1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "# #             nn.Linear(2048,1024),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Dropout(),\n",
    "#             nn.Linear(1024,512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128,1),\n",
    "#         )\n",
    "        \n",
    "    def forward(self, feature_dict):\n",
    "#         logits = self.basic_stack(x)\n",
    "#         x = self.flatten(x)\n",
    "        x = torch.cat([\n",
    "            torch.nan_to_num(feature_dict['logrett_xs'])*1000, \n",
    "                           feature_dict['trade_volume_xs'],\n",
    "                          feature_dict['trade_ordercount_xs'],\n",
    "                             feature_dict['logret1_xs']*1000,\n",
    "                             feature_dict['logret2_xs']*1000,\n",
    "                             feature_dict['book_dirvolume_xs'],\n",
    "                          ], 1)\n",
    "#         x = torch.nan_to_num(feature_dict['logrett_xs']).type(torch.cuda.FloatTensor)\n",
    "#         print(x)\n",
    "#         input()\n",
    "#         if torch.isnan(x).any():\n",
    "# #             print(x)\n",
    "#             print(feature_dict)\n",
    "#             input()\n",
    "        x = x.to(device)\n",
    "        x = x.reshape(-1,6,data_intervals_count)\n",
    "        \n",
    "        logits = self.cnn_stack(x)\n",
    "        logits = self.flatten(logits)\n",
    "        logits = self.linear_stack(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn_mse(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)))\n",
    "\n",
    "def loss_fn_mspe(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)/y))\n",
    "\n",
    "def loss_fn_orig(y, pred):\n",
    "    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "316cdf05-8827-491b-805c-902be90598c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolatilityLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VolatilityLSTM, self).__init__()\n",
    "        # 16 hidden layer size, 3 reapeted blocks of LSTM, 4 input size\n",
    "        self.input_size = 6\n",
    "        self.hidden_size = 64\n",
    "        self.repeated_lstm_cells = 1\n",
    "        self.rnn = nn.LSTM(self.input_size, self.hidden_size, self.repeated_lstm_cells, batch_first=True, dropout=0)\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size*self.repeated_lstm_cells, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, feature_dict):\n",
    "        x = torch.stack([\n",
    "            torch.nan_to_num(feature_dict['logrett_xs'])*1000, \n",
    "                           feature_dict['trade_volume_xs'],\n",
    "                          feature_dict['trade_ordercount_xs'],\n",
    "                             feature_dict['logret1_xs']*1000,\n",
    "                             feature_dict['logret2_xs']*1000,\n",
    "                             feature_dict['book_dirvolume_xs'],\n",
    "                          ], dim=2)\n",
    "        x = x.to(device)\n",
    "        x = x.reshape(-1,data_intervals_count,6)\n",
    "#         input(\"GOT FEATURES\")\n",
    "        h_0 = torch.rand(self.repeated_lstm_cells, x.size(0), self.hidden_size) #hidden state\n",
    "        c_0 = torch.rand(self.repeated_lstm_cells, x.size(0), self.hidden_size) #internal state\n",
    "        h_0 = h_0.to(device)\n",
    "        c_0 = c_0.to(device)\n",
    "#         print(h_0.device, c_0.device, x.device)\n",
    "        output, (hn, cn) = self.rnn(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "#         input(\"---- output got\")\n",
    "        hn = hn.reshape(-1, self.hidden_size*self.repeated_lstm_cells) #reshaping the data for Dense layer next\n",
    "#         input(\"---- hn got\")\n",
    "        out = self.linear_stack(hn)\n",
    "#         input(\"--- out got\")\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1f24f-c586-4e1b-aad6-fb4671c87232",
   "metadata": {},
   "source": [
    "#### analyze the initial weights (or change them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "931890cf-e329-4a15-b0d0-352dc27f985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @torch.no_grad()\n",
    "# def init_weights(m):\n",
    "# #     print(m)\n",
    "#     if type(m) == nn.Linear:\n",
    "# #         m.weight.fill_(1.0)\n",
    "#         torch.nn.init.xavier_uniform_(m.weight,gain=10)\n",
    "#         m.bias.data.uniform_(-1,1)\n",
    "# #     elif type(m) == nn.ReLU:\n",
    "# #         print(m.data)\n",
    "#     else:\n",
    "#         print(type(m))\n",
    "# #         print(m.weight)\n",
    "# model.apply(init_weights)\n",
    "# # for param in model.parameters():\n",
    "# # #     print(param)\n",
    "# #       print(param.data.size(), param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c79a7d-9e08-4433-9c77-1af049f349d6",
   "metadata": {},
   "source": [
    "### LEarning rate: our base line is 0.34 loss as that's what the optiver guys have when they use current 10 min realize vol and use it as target (copy to prediction). We create simplest neural network and work with learning rates to figure out what's best and when we see something in range of 0.35 then we've found good Learning rate\n",
    "- #### SGD: 1e-7 works best\n",
    "- #### ADAM: 1e-5, (NOTE: 1e-3 makes it behave dumb where some deep local minima gets stuck and produces constant output!)\n",
    "- TODO: analyze that constant output phenomenon more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5cb3871f-5215-4afb-92fc-47ae2d385fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-4\n",
    "# batch_size = 4096\n",
    "# epochs = 100\n",
    "\n",
    "# input_scaling = 1\n",
    "# output_scaling = 1\n",
    "\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "# strategyname = \"ret1_n_ret2\"\n",
    "# summary_writer = SummaryWriter(f'../output/training_tensorboard/{strategyname}_scaleIn{input_scaling}Out{output_scaling}_{learning_rate}_{batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9aadc54b-6009-4af6-bfb6-f6619c6acd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a7ed1-80f8-4a57-90b4-223345dde76b",
   "metadata": {},
   "source": [
    "### Learnings about training\n",
    "- (non scaling)logreturns input and volatility output; non scaled makes the model predict constant output with no variety(close to 0 std dev)\n",
    "- scaling input rids of variety issue, \n",
    "- scaling output makes the model start with low rmse initially so there's less ground to cover and we can iterate over ideas rapidly due to less epochs needed to achieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef34a31-7317-47c0-a94f-9bebe2fd2e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "---------- tradebook6fet_basic1_rnn1_0.001_32 ----------\n"
     ]
    }
   ],
   "source": [
    "# model = None\n",
    "strategyname = \"tradebook6fet_basic1_rnn1\"\n",
    "\n",
    "training_configs = []\n",
    "learning_rates_to_try = [1e-3, 1e-4]\n",
    "batch_sizes_to_try = [32, 64, 512]#,10000, 128]\n",
    "# input_scalings_to_try = [1000]\n",
    "# output_scalings_to_try = [10000]\n",
    "for learning_rate in learning_rates_to_try:\n",
    "    for batch_size in batch_sizes_to_try:\n",
    "            training_configs.append({\n",
    "                'learning_rate':learning_rate,\n",
    "                'batch_size':batch_size,\n",
    "                'input_scaling':1000,\n",
    "                'output_scaling':10000,\n",
    "            })\n",
    "\n",
    "epochs = 500\n",
    "for training_config in training_configs:\n",
    "    \n",
    "    learning_rate = training_config['learning_rate']\n",
    "    batch_size = training_config['batch_size']\n",
    "    input_scaling = training_config['input_scaling']\n",
    "    output_scaling = training_config['output_scaling']\n",
    "    # TRAINING SETUP\n",
    "    \n",
    "    #refresh the model\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    model = VolatilityLSTM()\n",
    "#     model = NeuralNetwork()\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "    \n",
    "    STRATEGY_NAME_WITH_ATTRS = f\"{strategyname}_{learning_rate}_{batch_size}\"\n",
    "    summary_writer = SummaryWriter(f'../output/training_tensorboard/{STRATEGY_NAME_WITH_ATTRS}')\n",
    "    \n",
    "    # TRAINING SETUP DONE\n",
    "    \n",
    "    print(\"DEVICE:\", device)\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    test_size = dataset_size - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    data_ohlc_sample_len = 1 # 1 for each of open high low close\n",
    "    losses_train = []\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        print(\"----------\", STRATEGY_NAME_WITH_ATTRS, \"----------\")\n",
    "\n",
    "        dataloader_train = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=0, pin_memory=True)\n",
    "        model.train()\n",
    "        \n",
    "        for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "\n",
    "            \n",
    "\n",
    "            y = feature_y * output_scaling \n",
    "            y = y.to(device)\n",
    "            feature_y = feature_y.to(device)\n",
    "            \n",
    "            pred = model(Feature_X)\n",
    "            pred.to(device)\n",
    "\n",
    "            loss_orig = loss_fn_orig(y, pred)\n",
    "#             print(Feature_X)\n",
    "#             input()\n",
    "#             print(loss_orig)\n",
    "#             input()\n",
    "            optimizer.zero_grad()\n",
    "            loss_orig.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            losses_train.append(loss_orig.item())\n",
    "#             print(X)\n",
    "#             print(loss_orig.item())\n",
    "# #             print()\n",
    "#             input()\n",
    "            # we want 5 spread out output per epoch\n",
    "            if (t*int(train_size/batch_size) + train_batch_idx + 1) % int(train_size/5/batch_size) == 0:\n",
    "                \n",
    "                # NOTE: real loss is same as upscaled normalized loss as it's percentage loss (rmspe)\n",
    "                prediction_variety = np.std((pred/output_scaling).reshape(-1).tolist()) * 100\n",
    "                #NOTE: prediction variety is important as model sometimes predits a constant value! regardless of the input, then per batch variety is lowest(0 std dev)\n",
    "#                 print(\"prediction variety\",)\n",
    "#                 print(pred.reshape(-1).tolist()[:7])\n",
    "                \n",
    "                summary_writer.add_scalar(\"Prediction Variety\", prediction_variety, t*(train_size) + (train_batch_idx*batch_size))\n",
    "                summary_writer.add_scalar(\"Training Loss\", np.mean(losses_train), t*(train_size) + (train_batch_idx*batch_size))\n",
    "\n",
    "                print(\"train:\", np.mean(losses_train), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\")\n",
    "                losses_train = []\n",
    "                \n",
    "        dataloader_test = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                shuffle=True, num_workers=0, pin_memory=True)\n",
    "        dataset_size = len(dataloader_test.dataset)\n",
    "        model.eval()\n",
    "\n",
    "        losses_test = []\n",
    "        for _, (Feature_X, feature_y) in enumerate(dataloader_test):\n",
    "            with torch.no_grad():\n",
    "\n",
    "#                 X = torch.cat([Feature_X['logret1_1s']*input_scaling, \n",
    "#                            Feature_X['logret1_1s']*input_scaling,\n",
    "#                           Feature_X['logret1_1s']*input_scaling,\n",
    "#                           ], 1)\n",
    "\n",
    "#                 X = X.reshape(-1,3,data_interval_len*data_ohlc_sample_len*1)\n",
    "                \n",
    "                y = feature_y * output_scaling\n",
    "\n",
    "                \n",
    "#                 X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                pred = model(Feature_X)\n",
    "                loss = loss_fn_orig(y, pred)\n",
    "                losses_test.append(loss.item())\n",
    "\n",
    "\n",
    "#                 summary_writer.add_scalar(\"Epoch Training Loss\", np.mean(losses_train), (t+1)*train_size)\n",
    "        summary_writer.add_scalar(\"Test Loss\", np.mean(losses_test), t*(train_size) + (train_batch_idx*batch_size))\n",
    "        print(\"train:\", np.mean(losses_train), \"test:\", np.mean(losses_test), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\")\n",
    "        losses_test = []\n",
    "        if (t+1)%50==0:\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{loss:.4f}.pth\"))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0a1f5760-8926-4064-8bf2-bddd62c80bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1633"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cea68b-9cf2-4a6c-8fab-d0e6b8f7654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f03435-5cde-40bf-a86b-656ab4515ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce561c-5b64-44bf-882b-41f6250b1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated(device)/1024/1024/1024\n",
    "# model.to(\"cpu\")\n",
    "# torch.cuda.memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a966ee2c-8547-48be-a912-8d05bf48b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.init()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
