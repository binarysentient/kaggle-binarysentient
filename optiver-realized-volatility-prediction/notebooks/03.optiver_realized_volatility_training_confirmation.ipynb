{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2f18e2-9894-4f0c-bd57-8b37afc02309",
   "metadata": {},
   "source": [
    "### Can our model predict current volatility?  (forget future; first it should be capable of predicting current one with given features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c4e895-fd66-490d-a8b8-d28b324d3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import types\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from optiver_features_handler import get_features_map_for_stock, get_row_id, realized_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750eb0e3-2860-490b-bc3c-2a512485a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\n",
    "OUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\n",
    "MODEL_OUTPUT_DIRECTORY = os.path.join(OUTPUT_DIRECTORY,\"models\")\n",
    "os.makedirs(OUTPUT_DIRECTORY,exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIRECTORY,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02947ab-aa1d-4af0-9d6e-7a51cff159ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interval_seconds = 1\n",
    "data_intervals_count = int(600/data_interval_seconds)\n",
    "class OptiverRealizedVolatilityDataset(Dataset):\n",
    "    def __init__(self, data_directory, mode=\"train\", lazy_load=True):\n",
    "        \"\"\"initializes Optiver Competition dataset\n",
    "        `mode`: train|test\n",
    "        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n",
    "        \"\"\"\n",
    "        print(\"INIT: OptiverRealizedVolatilityDataset\")\n",
    "        if mode.lower() not in ['train','test']:\n",
    "            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n",
    "        self.data_directory = data_directory\n",
    "        self.mode = mode.lower()\n",
    "        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n",
    "#         if self.mode == 'train':\n",
    "#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n",
    "        if self.mode == 'test':\n",
    "            self.main_df['target'] = 0\n",
    "        \n",
    "        self.cache_stocks_done_set = set()\n",
    "        # this is our final features lookup where we park all our features which can be addressed by row_id\n",
    "        # which is individual train/test.csv row id using 'stock_id`-`time_id`\n",
    "        self.cache_rowid_feature_map = {}\n",
    "        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n",
    "        targets = self.main_df['target'].tolist()\n",
    "        self.stock_possible_timeids_list = {}\n",
    "        for idx, row_id in enumerate(row_id_series.tolist()):\n",
    "            stock_id = int(row_id.split('-')[0])\n",
    "            time_id = int(row_id.split('-')[1])\n",
    "            self.cache_rowid_feature_map[row_id] = {'target_realized_volatility':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n",
    "            \n",
    "            # below code is to make sure what timeids we expect from stock data extractor\n",
    "            # in case of missing parquet files we'll have to know the keys to fill default values into\n",
    "            if stock_id not in self.stock_possible_timeids_list:\n",
    "                self.stock_possible_timeids_list[stock_id] = []\n",
    "            self.stock_possible_timeids_list[stock_id].append(time_id)\n",
    "            \n",
    "        \n",
    "        if lazy_load == False:\n",
    "            worker_data = []\n",
    "            for gkey, gdf in self.main_df.groupby(['stock_id']):\n",
    "                worker_data.append((self.data_directory, self.mode, gkey))\n",
    "#             print(\"---------- CPU COUNG:\", multiprocessing.cpu_count())\n",
    "            # NOTE: this was hell of a hunt; this windows and pytorch and jupyter combination is too tedious\n",
    "            #       make sure the function that we distribute don't call pytorch\n",
    "            chunksize = multiprocessing.cpu_count() * 1\n",
    "            processed = 0\n",
    "            for worker_data_chunk in [worker_data[i * chunksize:(i + 1) * chunksize] for i in range((len(worker_data) + chunksize - 1) // chunksize )]:\n",
    "                with Pool(multiprocessing.cpu_count()) as p:\n",
    "                    \n",
    "                    feature_set_list = p.starmap(get_features_map_for_stock, worker_data_chunk)\n",
    "                    \n",
    "                    for feature_map in feature_set_list:\n",
    "                        for rowid, features_dict in feature_map.items():\n",
    "                            for fkey,fval in features_dict.items():\n",
    "                                self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                            self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                        # udpate the indications that we've already fetched this stock and the lazy loader code won't fetch this again\n",
    "                        self.cache_stocks_done_set.add(int(rowid.split('-')[0]))\n",
    "                    \n",
    "                    processed += chunksize\n",
    "                    print(f\"Processed and loaded {processed} stocks features.\")\n",
    "    \n",
    "    def __cache_generate_features(self, main_stock_id, main_time_id):\n",
    "            \n",
    "            main_row_id = get_row_id(main_stock_id, main_time_id)\n",
    "            if main_stock_id not in self.cache_stocks_done_set:\n",
    "#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))   \n",
    "                # we'll combine the featureset with the bigger feature set of all stocks\n",
    "                feature_map = get_features_map_for_stock(self.data_directory, self.mode, main_stock_id)\n",
    "                # NOTE: sometime we might now have parquet files in that case we'll have 3 entried in .csv while only 1 gets returned in feature map\n",
    "                # we need to cover for that disparity\n",
    "                for time_id in self.stock_possible_timeids_list[main_stock_id]:\n",
    "                    expected_row_id = get_row_id(main_stock_id, time_id)\n",
    "                    if expected_row_id not in feature_map:\n",
    "                        feature_map[expected_row_id] = {}\n",
    "                for rowid, features_dict in feature_map.items():\n",
    "                    for fkey,fval in features_dict.items():\n",
    "                        self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                    self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                self.cache_stocks_done_set.add(main_stock_id)\n",
    "#             print(self.cache_rowid_feature_map[main_row_id])\n",
    "#             print(torch.tensor([self.cache_rowid_feature_map[main_row_id].get('book_realized_volatility',0)]))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map[main_row_id].get('log_return1_2s', [0]*(int(600/2)))))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map.get('book_directional_volume1_2s', [0]*(int(600/2)))))\n",
    "            return self.cache_rowid_feature_map[main_row_id]\n",
    "        \n",
    "    @staticmethod\n",
    "    def transform_to_01_realized_volatility_linear_data(features_dict):\n",
    "        feature_x  = {\n",
    "                    'row_id':features_dict['row_id'],\n",
    "                    'stock_id':torch.tensor(features_dict['stock_id'], dtype=torch.float32),\n",
    "                    'sequence_mask_xs': torch.tensor(features_dict.get('sequence_mask_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.bool),\n",
    "                    'seconds_in_bucket_xs': torch.tensor(features_dict.get('seconds_in_bucket_xs', [(idx) for idx in range(0,int(data_intervals_count))]), dtype=torch.float32),\n",
    "                    'has_trade_data_xs': torch.tensor(features_dict.get('has_trade_data_xs', [0]*(int(600/data_interval_seconds))), dtype=torch.float32),\n",
    "                }\n",
    "#         overview_aggregations = {\n",
    "#             'wap1': ['sum', 'std'],\n",
    "#     #         'wap2': [np.sum, np.std],\n",
    "#             'logret1': [realized_volatility],\n",
    "#             'logret2': [realized_volatility],\n",
    "#             'logrett': [realized_volatility],\n",
    "#             'wap_balance': ['sum', 'max'],\n",
    "#             'price_spread1': ['sum', 'max'],\n",
    "#             'bid_spread': ['sum', 'max'],\n",
    "#             'ask_spread': ['sum', 'max'],\n",
    "#             'total_volume': ['sum', 'max'],\n",
    "#             'volume_imbalance': ['sum', 'max'],\n",
    "#             \"bid_ask_spread\": ['sum', 'max'],\n",
    "#             'size':  ['sum', 'max','min'],\n",
    "#             'order_count': ['sum', 'max'],\n",
    "#             'trade_money_turnover': ['sum', 'max','min'],\n",
    "#         }\n",
    "        \n",
    "#         for key, aggs in overview_aggregations.items():\n",
    "#             for agg in aggs:\n",
    "#                 if isinstance(agg, types.FunctionType):\n",
    "#                     agg = agg.__name__\n",
    "#                 feature_x[f'{key}_{agg}'] = torch.tensor(features_dict.get(f'{key}_{agg}', -0.01), dtype=torch.float32)\n",
    "                \n",
    "#         for feature_name in ['wap1','wap_balance','logret1','logret2','logrett',\n",
    "#                              'price_spread1','bid_spread','ask_spread','total_volume','volume_imbalance',\n",
    "#                             'size','order_count','trade_money_turnover','trade_book_price_spread']:\n",
    "            \n",
    "#             feature_x[f'{feature_name}_sum_xs'] = torch.tensor(features_dict.get(f'{feature_name}_sum_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "#             feature_x[f'{feature_name}_max_xs'] = torch.tensor(features_dict.get(f'{feature_name}_max_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "        for feature_name in ['logret1','price_spread1','bid_spread','ask_spread','directional_volume1','directional_volume2','trade_money_turnover_per_order']: #'bid_price2','bid_size2','ask_price2','ask_size2',\n",
    "            feature_x[f'{feature_name}_xs'] = torch.tensor(features_dict.get(f'{feature_name}_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "#             feature_x[f'{feature_name}_v_xs'] = torch.tensor(features_dict.get(f'{feature_name}_v_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "#             np.concatenate([groupdf[feature_name].to_numpy(),[0]*(intervals_count-sequence_length)])\n",
    "        return (\n",
    "                feature_x,\n",
    "                {'target_realized_volatility':torch.tensor([features_dict['target_realized_volatility']])}\n",
    "#                 [features_dict['target']]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: handle for num_workers more than 0\n",
    "        #      using https://pytorch.org/docs/stable/data.html\n",
    "        #      using torch.util.data.get_worker_info()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        stock_id = self.main_df.at[idx, 'stock_id']\n",
    "        time_id = self.main_df.at[idx, 'time_id']\n",
    "        x,y = self.__cache_generate_features(stock_id,time_id)\n",
    "#         x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efcee691-c9ce-481f-a4e8-dbf97b66b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT: OptiverRealizedVolatilityDataset\n",
      "Processed and loaded 16 stocks features.\n",
      "Processed and loaded 32 stocks features.\n",
      "Processed and loaded 48 stocks features.\n",
      "Processed and loaded 64 stocks features.\n",
      "Processed and loaded 80 stocks features.\n",
      "Processed and loaded 96 stocks features.\n",
      "Processed and loaded 112 stocks features.\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\", lazy_load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d085c-aaed-4a28-9d44-93eb8e2d4e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in range(0,9):\n",
    "#     print(dataset[x])\n",
    "\n",
    "# dataset[10000] #[0]['bidp1_1s']\n",
    "for key,val in dataset[10000][0].items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eaeb87d1-d4be-4351-b483-22ac0c168909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_idx in range(len(dataset)):\n",
    "#     dataset[data_idx][0]['book_realized_volatility'] = dataset[data_idx][0]['book_realized_volatility'].type(torch.float32).to('cpu')\n",
    "#     print(dataset[data_idx][0]['book_realized_volatility'])\n",
    "#     input()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ec3227-ffc6-4099-b51c-04a3bd2df633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stock_id': {'mean': 62.43794250488281, 'std': 37.12644958496094},\n",
       " 'seconds_in_bucket_xs': {'mean': 193.01312255859375,\n",
       "  'std': 199.03518676757812},\n",
       " 'has_trade_data_xs': {'mean': 0.14914073050022125, 'std': 0.3562271296977997},\n",
       " 'logret1_xs': {'mean': -4.115666973891763e-10, 'std': 0.00030841503757983446},\n",
       " 'price_spread1_xs': {'mean': 0.00037106272066012025,\n",
       "  'std': 0.0006050662486813962},\n",
       " 'bid_spread_xs': {'mean': 0.00012593607243616134,\n",
       "  'std': 0.0002136170514859259},\n",
       " 'ask_spread_xs': {'mean': 0.00012725881242658943,\n",
       "  'std': 0.00021705604740418494},\n",
       " 'directional_volume1_xs': {'mean': 3.366671085357666,\n",
       "  'std': 3647.759033203125},\n",
       " 'directional_volume2_xs': {'mean': 22.80933380126953, 'std': 4096.1689453125},\n",
       " 'trade_money_turnover_per_order_xs': {'mean': 10.803224563598633,\n",
       "  'std': 45.278831481933594}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feature_dict = {key:[] for key in dataset[0][0].keys()}\n",
    "for data_idx in range(len(dataset)):\n",
    "    for feature_key,feature_val in dataset[data_idx][0].items():\n",
    "        all_feature_dict[feature_key].append(feature_val)\n",
    "        \n",
    "standard_scaling_feature_map = {}\n",
    "for key,val in all_feature_dict.items():\n",
    "    if type(val[0]) is not list and type(val[0]) is not str and val[0].type() is not str:\n",
    "        if len(val[0].size())>0 and \"bool\" not in str(val[0].type()).lower():\n",
    "#             print(val[0].type())\n",
    "            mean = torch.mean(torch.cat(val).reshape(-1)).item()\n",
    "            std = torch.std(torch.cat(val).reshape(-1)).item()\n",
    "            standard_scaling_feature_map[key] = {'mean':mean,'std':std}\n",
    "        elif \"bool\" not in str(val[0].type()).lower():\n",
    "            mean = torch.mean(torch.stack(val).reshape(-1)).item()\n",
    "            std = torch.std(torch.stack(val).reshape(-1)).item()\n",
    "            standard_scaling_feature_map[key] = {'mean':mean,'std':std}\n",
    "#             print(key, 'mean', , 'std', )\n",
    "standard_scaling_feature_map       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a72c9b-fab8-4536-b125-abc46cc800b6",
   "metadata": {},
   "source": [
    "### Learnings about model CNN input\n",
    "- it's better to use multiple channel for logreturn1 and logreturn2 than stacking it and using as one channel\n",
    "- 2 channels input for CNN is better than stacking it(dim 2, which is logret1_t1, logret2_t1, logret1_t2, logret2_t2...) and using it as one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc4b98ff-b328-465e-a04e-c25eef8ea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = None\n",
    "\n",
    "\n",
    "def loss_fn_mse(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)))\n",
    "\n",
    "def loss_fn_mspe(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)/y))\n",
    "\n",
    "def loss_fn_orig(y, pred):\n",
    "    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76a6a979-c114-4525-953f-f5c368fe26d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_stocks = dataset.main_df['stock_id'].max()\n",
    "number_of_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7de51c7-de13-45ff-8513-5d890d691352",
   "metadata": {},
   "outputs": [],
   "source": [
    "realize_volatility_scale_factor = 1000\n",
    "def scale_optiver_feature(feature_name, feature_tensor):\n",
    "#     standard_scaling_feature_map = {'stock_id': {'mean': 62.43794250488281, 'std': 37.12644958496094},\n",
    "#  'seconds_in_bucket_xs': {'mean': 193.01312255859375,\n",
    "#   'std': 199.03518676757812},\n",
    "#  'has_trade_data_xs': {'mean': 0.14914073050022125, 'std': 0.3562271296977997},\n",
    "#  'bid_price1_^_xs': {'mean': 0.649695634841919, 'std': 0.4768790304660797},\n",
    "#  'bid_price1_v_xs': {'mean': 0.6500793695449829, 'std': 0.477160781621933},\n",
    "#  'bid_size1_^_xs': {'mean': 603.4517822265625, 'std': 4682.9423828125},\n",
    "#  'bid_size1_v_xs': {'mean': 0.04587273672223091, 'std': 0.1751980185508728},\n",
    "#  'ask_price1_^_xs': {'mean': 0.650066614151001, 'std': 0.47715136408805847},\n",
    "#  'ask_price1_v_xs': {'mean': 0.6497082710266113, 'std': 0.4768883287906647},\n",
    "#  'ask_size1_^_xs': {'mean': 600.0850830078125, 'std': 4266.17578125},\n",
    "#  'ask_size1_v_xs': {'mean': 0.04358990117907524, 'std': 0.16984671354293823},\n",
    "#  'price_^_xs': {'mean': 0.14412905275821686, 'std': 0.3583465814590454},\n",
    "#  'price_v_xs': {'mean': 0.14414073526859283, 'std': 0.3583745062351227},\n",
    "#  'size_^_xs': {'mean': 53.09535217285156, 'std': 497.29290771484375},\n",
    "#  'size_v_xs': {'mean': 0.007974185980856419, 'std': 0.09658150374889374},\n",
    "#  'order_count_^_xs': {'mean': 0.6173872351646423, 'std': 3.3599042892456055},\n",
    "#  'order_count_v_xs': {'mean': 0.07179050892591476, 'std': 0.23097625374794006}}\n",
    "#     print(feature_name, feature_tensor.size())\n",
    "    \n",
    "#     if feature_name in ['book_realized_volatility_xs','trade_realized_volatility_xs']:\n",
    "#         # we expect feature_tensor to be log returns tensor\n",
    "#         feature_tensor = feature_tensor ** 2\n",
    "# #         print(feature_tensor)\n",
    "#         feature_tensor = torch.cumsum(feature_tensor,1)\n",
    "#         # scale it to make each step realize volatility extrapolatable to 10 min window\n",
    "# #         feature_tensor = feature_tensor * torch.tensor([data_intervals_count/idx for idx in range(1,data_intervals_count+1,1)])\n",
    "#         feature_tensor = torch.sqrt(feature_tensor) * realize_volatility_scale_factor\n",
    "        \n",
    "    if feature_name == 'sequence_mask_xs':\n",
    "        feature_tensor = feature_tensor.type(torch.float32)\n",
    "        return feature_tensor\n",
    "    if feature_name == 'has_trade_data_xs':\n",
    "        #TODO: we'll pre convert it so directly reutrn feature_tensor without converting\n",
    "        return feature_tensor.type(torch.float32)\n",
    "    if feature_name == 'seconds_in_bucket_xs':\n",
    "        return feature_tensor / standard_scaling_feature_map[feature_name]['std']/100\n",
    "    if feature_name in standard_scaling_feature_map:\n",
    "        return (feature_tensor - standard_scaling_feature_map[feature_name]['mean'])/standard_scaling_feature_map[feature_name]['std']\n",
    "#         return feature_tensor/standard_scaling_feature_map[feature_name]['std']/2\n",
    "    if feature_name in ['trade_price_local_standardized_xs','book_wap1_local_standardized_xs']:\n",
    "        #TODO: the kaggle version of pytorch dont have nan_to_num, do something here!\n",
    "        feature_tensor = torch.masked_fill(feature_tensor, torch.isinf(feature_tensor),0)\n",
    "#         feature_tensor = torch.nan_to_num(feature_tensor,nan=0, posinf=0, neginf=0)\n",
    "#     print(feature_tensor)\n",
    "#     print(torch.any(torch.isnan(feature_tensor)))\n",
    "#     input()\n",
    "    return feature_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b110429-4ffd-4923-8f5e-392e119ee6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_id\n",
      "tensor([[-1.6279]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_mask_xs\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 1.]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds_in_bucket_xs\n",
      "tensor([[0.0000e+00, 5.0242e-05, 1.0048e-04, 1.5073e-04, 2.0097e-04, 2.5121e-04,\n",
      "         3.0145e-04, 3.5170e-04, 4.0194e-04, 4.5218e-04, 5.0242e-04, 5.5267e-04,\n",
      "         6.0291e-04, 6.5315e-04, 7.0339e-04, 7.5364e-04, 8.0388e-04, 8.5412e-04,\n",
      "         9.0436e-04, 9.5461e-04, 1.0048e-03, 1.0551e-03, 1.1053e-03, 1.1556e-03,\n",
      "         1.2058e-03, 1.2561e-03, 1.3063e-03, 1.3565e-03, 1.4068e-03, 1.4570e-03,\n",
      "         1.5073e-03, 1.5575e-03, 1.6078e-03, 1.6580e-03, 1.7082e-03, 1.7585e-03,\n",
      "         1.8087e-03, 1.8590e-03, 1.9092e-03, 1.9595e-03, 2.0097e-03, 2.0599e-03,\n",
      "         2.1604e-03, 2.2107e-03, 2.2609e-03, 2.3111e-03, 2.3614e-03, 2.4116e-03,\n",
      "         2.4619e-03, 2.5121e-03, 2.5624e-03, 2.6126e-03, 2.6628e-03, 2.7131e-03,\n",
      "         2.7633e-03, 2.8136e-03, 2.8638e-03, 2.9141e-03, 2.9643e-03, 3.0145e-03,\n",
      "         3.0648e-03, 3.1150e-03, 3.1653e-03, 3.2155e-03, 3.2658e-03, 3.3160e-03,\n",
      "         3.3662e-03, 3.4165e-03, 3.4667e-03, 3.5170e-03, 3.5672e-03, 3.6175e-03,\n",
      "         3.6677e-03, 3.7179e-03, 3.7682e-03, 3.8184e-03, 3.8687e-03, 3.9189e-03,\n",
      "         3.9691e-03, 4.0194e-03, 4.0696e-03, 4.1199e-03, 4.1701e-03, 4.2204e-03,\n",
      "         4.2706e-03, 4.3208e-03, 4.3711e-03, 4.4213e-03, 4.4716e-03, 4.5218e-03,\n",
      "         4.5721e-03, 4.6223e-03, 4.6725e-03, 4.7228e-03, 4.7730e-03, 4.8233e-03,\n",
      "         4.8735e-03, 4.9238e-03, 4.9740e-03, 5.0242e-03, 5.0745e-03, 5.1247e-03,\n",
      "         5.1750e-03, 5.2252e-03, 5.2754e-03, 5.3257e-03, 5.3759e-03, 5.4262e-03,\n",
      "         5.4764e-03, 5.5267e-03, 5.5769e-03, 5.6271e-03, 5.6774e-03, 5.7276e-03,\n",
      "         5.7779e-03, 5.8281e-03, 5.8784e-03, 5.9286e-03, 5.9788e-03, 6.0291e-03,\n",
      "         6.0793e-03, 6.1296e-03, 6.1798e-03, 6.2301e-03, 6.2803e-03, 6.3305e-03,\n",
      "         6.3808e-03, 6.4310e-03, 6.4813e-03, 6.5315e-03, 6.5818e-03, 6.6320e-03,\n",
      "         6.6822e-03, 6.7325e-03, 6.7827e-03, 6.8330e-03, 6.8832e-03, 6.9334e-03,\n",
      "         6.9837e-03, 7.0339e-03, 7.0842e-03, 7.1344e-03, 7.1847e-03, 7.2349e-03,\n",
      "         7.2851e-03, 7.3354e-03, 7.3856e-03, 7.4359e-03, 7.4861e-03, 7.5364e-03,\n",
      "         7.5866e-03, 7.6368e-03, 7.6871e-03, 7.7373e-03, 7.7876e-03, 7.8378e-03,\n",
      "         7.8881e-03, 7.9383e-03, 7.9885e-03, 8.0388e-03, 8.0890e-03, 8.1393e-03,\n",
      "         8.1895e-03, 8.2397e-03, 8.2900e-03, 8.3402e-03, 8.3905e-03, 8.4407e-03,\n",
      "         8.4910e-03, 8.5412e-03, 8.5914e-03, 8.6417e-03, 8.6919e-03, 8.7422e-03,\n",
      "         8.7924e-03, 8.8427e-03, 8.8929e-03, 8.9431e-03, 8.9934e-03, 9.0436e-03,\n",
      "         9.0939e-03, 9.1441e-03, 9.1944e-03, 9.2446e-03, 9.2948e-03, 9.3451e-03,\n",
      "         9.3953e-03, 9.4456e-03, 9.4958e-03, 9.5461e-03, 9.5963e-03, 9.6465e-03,\n",
      "         9.6968e-03, 9.7470e-03, 9.7973e-03, 9.8475e-03, 9.8977e-03, 9.9480e-03,\n",
      "         9.9982e-03, 1.0048e-02, 1.0099e-02, 1.0149e-02, 1.0199e-02, 1.0249e-02,\n",
      "         1.0300e-02, 1.0350e-02, 1.0400e-02, 1.0450e-02, 1.0501e-02, 1.0551e-02,\n",
      "         1.0601e-02, 1.0651e-02, 1.0702e-02, 1.0752e-02, 1.0802e-02, 1.0852e-02,\n",
      "         1.0903e-02, 1.0953e-02, 1.1003e-02, 1.1053e-02, 1.1104e-02, 1.1154e-02,\n",
      "         1.1204e-02, 1.1254e-02, 1.1305e-02, 1.1355e-02, 1.1405e-02, 1.1455e-02,\n",
      "         1.1506e-02, 1.1556e-02, 1.1606e-02, 1.1656e-02, 1.1706e-02, 1.1757e-02,\n",
      "         1.1807e-02, 1.1857e-02, 1.1907e-02, 1.1958e-02, 1.2008e-02, 1.2058e-02,\n",
      "         1.2108e-02, 1.2159e-02, 1.2209e-02, 1.2259e-02, 1.2309e-02, 1.2360e-02,\n",
      "         1.2410e-02, 1.2460e-02, 1.2510e-02, 1.2561e-02, 1.2611e-02, 1.2661e-02,\n",
      "         1.2711e-02, 1.2762e-02, 1.2812e-02, 1.2862e-02, 1.2912e-02, 1.2963e-02,\n",
      "         1.3013e-02, 1.3063e-02, 1.3113e-02, 1.3164e-02, 1.3214e-02, 1.3264e-02,\n",
      "         1.3314e-02, 1.3364e-02, 1.3415e-02, 1.3465e-02, 1.3515e-02, 1.3565e-02,\n",
      "         1.3616e-02, 1.3666e-02, 1.3716e-02, 1.3766e-02, 1.3817e-02, 1.3867e-02,\n",
      "         1.3917e-02, 1.3967e-02, 1.4018e-02, 1.4068e-02, 1.4118e-02, 1.4168e-02,\n",
      "         1.4219e-02, 1.4269e-02, 1.4319e-02, 1.4369e-02, 1.4420e-02, 1.4470e-02,\n",
      "         1.4520e-02, 1.4570e-02, 1.4621e-02, 1.4671e-02, 1.4721e-02, 1.4771e-02,\n",
      "         1.4821e-02, 1.4872e-02, 1.4922e-02, 1.4972e-02, 1.5022e-02, 1.5073e-02,\n",
      "         1.5123e-02, 1.5173e-02, 1.5223e-02, 1.5274e-02, 1.5324e-02, 1.5374e-02,\n",
      "         1.5424e-02, 1.5475e-02, 1.5525e-02, 1.5575e-02, 1.5625e-02, 1.5676e-02,\n",
      "         1.5726e-02, 1.5776e-02, 1.5826e-02, 1.5877e-02, 1.5927e-02, 1.5977e-02,\n",
      "         1.6027e-02, 1.6078e-02, 1.6128e-02, 1.6178e-02, 1.6228e-02, 1.6279e-02,\n",
      "         1.6329e-02, 1.6379e-02, 1.6429e-02, 1.6479e-02, 1.6530e-02, 1.6580e-02,\n",
      "         1.6630e-02, 1.6680e-02, 1.6731e-02, 1.6781e-02, 1.6831e-02, 1.6881e-02,\n",
      "         1.6932e-02, 1.6982e-02, 1.7032e-02, 1.7082e-02, 1.7133e-02, 1.7183e-02,\n",
      "         1.7233e-02, 1.7283e-02, 1.7334e-02, 1.7384e-02, 1.7434e-02, 1.7484e-02,\n",
      "         1.7535e-02, 1.7585e-02, 1.7635e-02, 1.7685e-02, 1.7736e-02, 1.7786e-02,\n",
      "         1.7836e-02, 1.7886e-02, 1.7937e-02, 1.7987e-02, 1.8037e-02, 1.8087e-02,\n",
      "         1.8137e-02, 1.8188e-02, 1.8238e-02, 1.8288e-02, 1.8338e-02, 1.8389e-02,\n",
      "         1.8439e-02, 1.8489e-02, 1.8539e-02, 1.8590e-02, 1.8640e-02, 1.8690e-02,\n",
      "         1.8740e-02, 1.8791e-02, 1.8841e-02, 1.8891e-02, 1.8941e-02, 1.8992e-02,\n",
      "         1.9042e-02, 1.9092e-02, 1.9142e-02, 1.9193e-02, 1.9243e-02, 1.9293e-02,\n",
      "         1.9343e-02, 1.9394e-02, 1.9444e-02, 1.9494e-02, 1.9544e-02, 1.9595e-02,\n",
      "         1.9645e-02, 1.9695e-02, 1.9745e-02, 1.9795e-02, 1.9846e-02, 1.9896e-02,\n",
      "         1.9946e-02, 1.9996e-02, 2.0047e-02, 2.0097e-02, 2.0147e-02, 2.0197e-02,\n",
      "         2.0248e-02, 2.0298e-02, 2.0348e-02, 2.0398e-02, 2.0449e-02, 2.0499e-02,\n",
      "         2.0549e-02, 2.0599e-02, 2.0650e-02, 2.0700e-02, 2.0750e-02, 2.0800e-02,\n",
      "         2.0851e-02, 2.0901e-02, 2.0951e-02, 2.1001e-02, 2.1052e-02, 2.1102e-02,\n",
      "         2.1152e-02, 2.1202e-02, 2.1253e-02, 2.1303e-02, 2.1353e-02, 2.1403e-02,\n",
      "         2.1453e-02, 2.1504e-02, 2.1554e-02, 2.1604e-02, 2.1654e-02, 2.1705e-02,\n",
      "         2.1755e-02, 2.1805e-02, 2.1855e-02, 2.1906e-02, 2.1956e-02, 2.2006e-02,\n",
      "         2.2056e-02, 2.2107e-02, 2.2157e-02, 2.2207e-02, 2.2257e-02, 2.2308e-02,\n",
      "         2.2358e-02, 2.2458e-02, 2.2509e-02, 2.2559e-02, 2.2609e-02, 2.2659e-02,\n",
      "         2.2710e-02, 2.2760e-02, 2.2810e-02, 2.2860e-02, 2.2911e-02, 2.2961e-02,\n",
      "         2.3011e-02, 2.3061e-02, 2.3111e-02, 2.3162e-02, 2.3212e-02, 2.3262e-02,\n",
      "         2.3312e-02, 2.3363e-02, 2.3413e-02, 2.3463e-02, 2.3513e-02, 2.3564e-02,\n",
      "         2.3614e-02, 2.3664e-02, 2.3714e-02, 2.3765e-02, 2.3815e-02, 2.3865e-02,\n",
      "         2.3915e-02, 2.3966e-02, 2.4016e-02, 2.4066e-02, 2.4116e-02, 2.4167e-02,\n",
      "         2.4217e-02, 2.4267e-02, 2.4317e-02, 2.4368e-02, 2.4418e-02, 2.4468e-02,\n",
      "         2.4518e-02, 2.4569e-02, 2.4619e-02, 2.4669e-02, 2.4719e-02, 2.4769e-02,\n",
      "         2.4820e-02, 2.4870e-02, 2.4920e-02, 2.4970e-02, 2.5021e-02, 2.5071e-02,\n",
      "         2.5121e-02, 2.5171e-02, 2.5222e-02, 2.5272e-02, 2.5322e-02, 2.5372e-02,\n",
      "         2.5423e-02, 2.5473e-02, 2.5523e-02, 2.5573e-02, 2.5624e-02, 2.5674e-02,\n",
      "         2.5724e-02, 2.5774e-02, 2.5825e-02, 2.5875e-02, 2.5925e-02, 2.5975e-02,\n",
      "         2.6026e-02, 2.6076e-02, 2.6126e-02, 2.6176e-02, 2.6227e-02, 2.6277e-02,\n",
      "         2.6327e-02, 2.6377e-02, 2.6427e-02, 2.6478e-02, 2.6528e-02, 2.6578e-02,\n",
      "         2.6628e-02, 2.6679e-02, 2.6729e-02, 2.6779e-02, 2.6829e-02, 2.6880e-02,\n",
      "         2.6930e-02, 2.6980e-02, 2.7030e-02, 2.7081e-02, 2.7131e-02, 2.7181e-02,\n",
      "         2.7231e-02, 2.7282e-02, 2.7332e-02, 2.7382e-02, 2.7432e-02, 2.7483e-02,\n",
      "         2.7533e-02, 2.7583e-02, 2.7633e-02, 2.7684e-02, 2.7734e-02, 2.7784e-02,\n",
      "         2.7834e-02, 2.7885e-02, 2.7935e-02, 2.7985e-02, 2.8035e-02, 2.8085e-02,\n",
      "         2.8136e-02, 2.8186e-02, 2.8236e-02, 2.8286e-02, 2.8337e-02, 2.8387e-02,\n",
      "         2.8437e-02, 2.8487e-02, 2.8538e-02, 2.8588e-02, 2.8638e-02, 2.8688e-02,\n",
      "         2.8739e-02, 2.8789e-02, 2.8839e-02, 2.8889e-02, 2.8940e-02, 2.8990e-02,\n",
      "         2.9040e-02, 2.9090e-02, 2.9141e-02, 2.9191e-02, 2.9241e-02, 2.9291e-02,\n",
      "         2.9342e-02, 2.9392e-02, 2.9442e-02, 2.9492e-02, 2.9543e-02, 2.9593e-02,\n",
      "         2.9643e-02, 2.9693e-02, 2.9743e-02, 2.9794e-02, 2.9844e-02, 2.9894e-02,\n",
      "         2.9944e-02, 2.9995e-02, 3.0045e-02, 3.0095e-02, 0.0000e+00, 0.0000e+00]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_trade_data_xs\n",
      "tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logret1_xs\n",
      "tensor([[-1.0898e+01,  4.7013e-01, -1.1752e-01,  1.5670e-01,  1.9596e-01,\n",
      "         -2.3504e-01, -1.9597e-01, -1.5671e-01, -1.7631e-01,  1.7631e-01,\n",
      "         -5.3747e-01,  6.5500e-01, -1.1753e-01,  2.8989e-01, -5.9480e-02,\n",
      "          5.9483e-02,  1.2015e-01, -1.9825e-02,  1.3345e-06,  2.2026e-02,\n",
      "         -3.6579e-01, -9.2710e-02, -7.0687e-02,  1.3345e-06, -4.6911e-01,\n",
      "          2.9846e-01,  1.7433e-01,  3.9126e-02,  4.3743e-01, -2.4574e-01,\n",
      "          1.7556e-03,  1.1733e-01, -2.3351e-01,  1.1791e-01, -1.1308e-01,\n",
      "          2.3070e-01, -1.5925e-01, -7.7066e-02,  5.6155e-03, -4.7599e-01,\n",
      "         -2.2317e-03,  1.3345e-06,  7.9009e-01, -4.8112e-01,  4.3676e-02,\n",
      "          7.0545e-02,  4.7029e-02, -7.8293e-02,  3.1350e-01,  2.3510e-01,\n",
      "          1.3345e-06, -3.5446e-01, -9.8506e-02, -7.6041e-02,  4.0354e-02,\n",
      "         -1.6361e-01,  6.4492e-02, -9.2070e-03,  6.7980e-02, -3.5542e-02,\n",
      "         -3.2434e-02, -4.2193e-01,  1.0432e-03, -5.8122e-01,  1.3345e-06,\n",
      "          1.4113e-01, -9.8203e-02, -6.0895e-02,  6.0898e-02,  1.2689e-01,\n",
      "          4.7485e-02, -1.7437e-01,  1.3345e-06,  1.3345e-06,  5.0972e-02,\n",
      "          1.3345e-06,  8.4686e-01, -4.7026e-02,  1.3345e-06,  1.3345e-06,\n",
      "         -1.7632e-01, -1.0583e-01,  1.3345e-06,  1.4107e-01, -1.1065e-02,\n",
      "         -2.4169e-02, -2.4701e-01, -4.3899e-01,  1.3345e-06, -3.1404e-02,\n",
      "          3.1407e-02, -5.4887e-01,  3.3094e-01, -1.7233e-02,  7.8326e-02,\n",
      "         -3.9161e-02,  1.3345e-06,  3.9163e-02,  4.7043e-01, -4.8927e-01,\n",
      "         -2.5549e-01,  1.3345e-06,  8.9297e-02,  3.8114e-01,  1.3345e-06,\n",
      "         -9.9536e-03,  1.2753e-01,  1.3345e-06,  6.1997e-01,  8.5481e-02,\n",
      "         -4.4085e-01, -1.2347e-01,  7.4069e-01, -7.2505e-01,  6.0751e-01,\n",
      "         -1.7637e-01,  1.1758e-01,  2.3515e-01,  1.1754e-01, -1.1753e-01,\n",
      "         -2.3518e-01,  7.8396e-02,  2.3514e-01,  9.7945e-02,  1.3712e-01,\n",
      "          1.3345e-06, -2.3506e-01, -4.7030e-01,  7.8432e-02,  1.5673e-01,\n",
      "          1.5669e-01, -2.3508e-01,  1.1764e-01,  1.3345e-06, -2.3519e-01,\n",
      "         -6.8207e-01,  3.2931e-01,  3.5277e-01, -3.9150e-02, -7.8431e-02,\n",
      "          2.3517e-01,  2.2145e-01,  1.3345e-06,  2.0175e-01, -1.0970e-01,\n",
      "          1.3345e-06,  1.5671e-01,  4.7012e-02,  1.3345e-06,  1.3345e-06,\n",
      "         -1.2536e-01,  5.9940e-02,  1.3345e-06,  1.3345e-06, -1.4859e-03,\n",
      "         -8.4228e-01,  3.9188e-02,  3.9252e-02,  7.8404e-02, -7.5264e-01,\n",
      "         -1.6113e-01, -3.4066e-01, -1.4245e-01,  5.7515e-02,  5.5981e-01,\n",
      "         -6.8300e-02, -6.0434e-02, -2.5443e-01,  3.6150e-01,  1.3345e-06,\n",
      "          1.3345e-06,  1.9454e-02, -2.3741e-01, -2.3824e-01,  2.5777e-01,\n",
      "          1.6708e-01, -1.9089e-01,  1.3372e-01,  1.1080e-01,  2.4131e-01,\n",
      "          1.2603e-01, -5.8800e-02,  1.3345e-06,  3.5286e-01, -7.0554e-02,\n",
      "         -4.3906e-01, -2.2442e-01, -3.8147e-02,  3.7230e-01, -2.3426e-01,\n",
      "          1.3345e-06, -1.6989e-01,  6.1688e-02, -1.5933e-01,  6.5803e-05,\n",
      "          1.3345e-06,  4.3117e-01, -7.0564e-02,  1.4113e-01, -7.0562e-02,\n",
      "          4.7046e-01, -7.0554e-02,  1.3345e-06,  1.3345e-06, -4.7047e-01,\n",
      "          1.3345e-06,  7.0567e-02,  1.4385e-01,  9.1390e-02, -1.9595e-01,\n",
      "          1.0456e-01, -1.2273e-01,  3.1799e-01, -6.5521e-03,  1.3781e-01,\n",
      "         -1.6458e-01, -1.1755e-02, -4.3843e-04,  1.3345e-06, -1.9674e-02,\n",
      "          3.7291e-02, -1.0778e-02,  1.3345e-06,  1.3345e-06,  1.3345e-06,\n",
      "          7.0446e-03, -1.8631e-01,  2.3166e-01,  4.0313e-01,  6.5505e-01,\n",
      "         -1.1762e-01,  2.3518e-01,  1.3345e-06,  3.0723e-01,  6.4384e-02,\n",
      "          2.1600e-01,  1.3345e-06,  1.6450e-01, -4.6996e-02,  1.3345e-06,\n",
      "         -3.5260e-01,  1.3345e-06, -5.4060e-01,  1.8803e-01,  2.3506e-01,\n",
      "         -3.0878e-05,  1.3345e-06, -4.6256e-01, -6.7059e-01, -1.9112e-02,\n",
      "          3.5605e-02,  1.2928e-01, -1.8810e-01, -8.9374e-01, -6.7896e-02,\n",
      "          3.0310e-01,  2.4693e-01, -3.7240e-01,  1.7930e-01,  1.6798e-02,\n",
      "         -1.7651e-01, -1.7636e-01,  6.0179e-02,  2.9260e-01,  8.2821e-02,\n",
      "         -4.3556e-01,  1.3345e-06,  1.3345e-06, -7.8424e-02, -1.5685e-01,\n",
      "          7.8398e-02,  3.0031e-02,  9.2671e-02,  1.3345e-06,  3.4777e-01,\n",
      "         -1.2547e-01, -5.8378e-01,  1.3345e-06, -2.3845e-02, -4.6516e-01,\n",
      "         -4.9948e-02,  4.9950e-02,  1.4585e-01,  1.3345e-06, -3.4845e-01,\n",
      "         -2.0003e-01,  1.1816e-01,  1.1363e-01,  6.1871e-01,  2.0704e-01,\n",
      "         -9.9529e-02,  4.4691e-01,  2.3521e-01, -1.1762e-01, -7.8418e-02,\n",
      "          7.8421e-02, -4.5167e-03, -1.2306e+00, -4.9027e-01, -1.0774e-01,\n",
      "          1.3627e+00, -3.8286e-02,  7.1499e-01,  2.1626e-01, -9.4411e-02,\n",
      "         -3.2810e-01,  3.3566e-05,  5.8807e-02,  3.6452e-01, -4.2332e-01,\n",
      "         -3.0897e-05,  3.1363e-01,  3.9194e-01,  7.0544e-01,  1.1756e-01,\n",
      "         -1.9597e-01,  3.1381e-02, -4.2210e-01, -1.1784e-01, -1.9579e-01,\n",
      "         -5.1065e-01,  2.9407e-01,  1.3717e-01, -7.8373e-02, -1.1763e-01,\n",
      "         -9.1726e-01,  1.1290e+00,  1.3345e-06, -9.4163e-02,  3.1361e-01,\n",
      "          2.7424e-01,  1.1298e-01, -1.6900e-01,  1.1140e+00,  5.6403e-01,\n",
      "         -2.1147e-01, -1.1749e-01,  3.9175e-01,  8.7732e-01,  5.2734e-01,\n",
      "         -7.3235e-02, -1.9572e-01, -1.6442e-01,  1.1040e+00, -2.1295e-01,\n",
      "          5.8852e-02,  3.8666e-02,  7.0889e-02, -1.9047e-01,  3.5242e-01,\n",
      "         -5.7730e-01, -3.6223e-01,  5.8731e-01, -4.1116e-01,  2.1528e-01,\n",
      "         -1.5656e-01,  5.6383e-01,  5.6365e-01, -3.0521e-01, -9.7884e-01,\n",
      "          2.5051e-01, -2.1137e-01, -6.8117e-01,  5.6374e-01,  9.3948e-02,\n",
      "          1.3345e-06,  3.7576e-01,  3.5243e-01, -4.6985e-01,  4.3078e-01,\n",
      "          1.3345e-06, -7.8306e-01,  3.5229e-01,  1.3345e-06,  2.7423e-01,\n",
      "          7.8307e-02, -2.8208e-01,  4.6969e-02,  1.3345e-06,  5.8710e-02,\n",
      "         -4.1098e-01,  5.8729e-01, -3.5244e-01, -2.6421e-01, -4.0217e-01,\n",
      "          2.0158e-01,  1.7124e-01,  1.2993e-01, -1.8864e-01,  5.2206e-01,\n",
      "          1.8266e-01,  2.0119e-01,  2.6842e-01,  1.1744e-01, -1.1724e-01,\n",
      "          1.3345e-06, -1.5678e-01,  1.5659e-01, -2.3473e-01, -6.3030e-05,\n",
      "         -8.4559e-01,  1.4092e-01, -1.4091e-01,  3.7595e-01, -3.1319e-01,\n",
      "          7.8158e-02,  7.8413e-02,  3.1312e-01, -3.9152e-01, -1.1743e-01,\n",
      "          5.8716e-02,  1.3191e-03,  5.8179e-02, -2.3433e-01,  3.5186e-01,\n",
      "          3.1793e-01,  8.0554e-02,  1.8779e-01,  2.8204e-01, -2.8201e-01,\n",
      "         -7.8267e-02, -3.8190e-02,  1.3345e-06,  3.8193e-02, -3.8190e-02,\n",
      "          1.3345e-06,  1.1737e-01,  1.3345e-06,  1.4000e-01,  9.3955e-02,\n",
      "         -6.8090e-02, -5.1311e-02,  1.7823e-01, -8.3241e-02, -2.2721e-01,\n",
      "          1.1095e-01, -9.2614e-02,  1.3345e-06, -2.3533e-01,  3.4849e-03,\n",
      "          2.9356e-01,  1.3345e-06, -1.5612e-01, -2.3641e-02,  2.0469e-01,\n",
      "          7.1470e-04,  2.8673e-01, -2.5710e-01,  1.3345e-06, -1.1672e-01,\n",
      "         -1.5701e-04,  1.3345e-06, -2.5573e-02,  1.1377e-01,  1.3345e-06,\n",
      "         -1.1377e-01, -4.8620e-02,  3.5398e-01,  1.3345e-06,  1.0100e+00,\n",
      "         -8.7871e-02,  2.2719e-02,  1.3345e-06, -2.1986e-01,  8.9420e-02,\n",
      "          1.3345e-06, -3.9119e-02, -1.6431e-01,  1.3345e-06, -3.1297e-02,\n",
      "          7.8256e-01,  7.0416e-01,  1.3345e-06,  1.3345e-06, -4.6925e-02,\n",
      "         -3.2866e-01,  1.3345e-06,  1.3345e-06, -6.8062e-01, -3.9810e-01,\n",
      "         -1.8581e-01, -2.3089e-01,  2.2346e-03, -7.6505e-03,  7.7101e-04,\n",
      "          3.5156e-01,  5.5163e-01,  3.5291e-02, -4.8656e-02,  4.7044e-02,\n",
      "          1.3345e-06, -1.1741e-01,  3.1843e-01,  3.3610e-02,  1.0170e+00,\n",
      "          1.3345e-06, -7.8203e-02, -5.1619e-01,  4.6931e-02,  1.3345e-06,\n",
      "          5.8657e-01, -5.4359e-02, -5.4584e-01,  1.3345e-06, -1.2732e-01,\n",
      "         -1.5238e-01,  2.9333e-01,  1.3345e-06,  5.4747e-01, -7.8203e-02,\n",
      "          7.8205e-02, -7.8203e-02,  1.3345e-06, -1.1731e-01, -2.3461e-02,\n",
      "         -3.2849e-01,  2.3464e-01,  1.3345e-06, -1.3667e-02,  1.3345e-06,\n",
      "          1.3345e-06, -1.0817e+00,  1.5654e-01,  1.3345e-06, -6.9385e-03,\n",
      "          1.3345e-06,  1.3345e-06, -3.4526e-01,  7.0916e-02,  2.8128e-01,\n",
      "          5.4839e-02,  5.7306e-02,  1.3345e-06,  1.1796e-01,  1.3345e-06,\n",
      "          2.8639e-01,  3.8885e-01, -3.4825e-01, -3.2239e-01,  1.3345e-06,\n",
      "          6.7080e-02, -1.8442e-01,  1.7611e-01,  1.3345e-06,  1.3345e-06,\n",
      "          1.7610e-01, -3.1316e-01,  1.3345e-06, -3.9049e-02,  3.9052e-02,\n",
      "         -3.1306e-01,  6.2590e-02,  1.3345e-06,  2.1152e-01,  1.6415e-01,\n",
      "         -1.6414e-01,  7.0350e-02,  4.3821e-01,  4.4697e-02,  6.2854e-02,\n",
      "         -2.6399e-01, -1.8789e-01,  2.8168e-01, -1.6926e-01,  1.8312e-01,\n",
      "         -2.9553e-01,  1.6852e-01, -5.6096e-02,  1.3345e-06, -2.3270e-01,\n",
      "          7.1691e-02,  2.1711e-01,  3.2108e-01,  1.3345e-06, -2.7236e-01,\n",
      "         -2.6270e-01,  2.3337e-01,  4.6934e-02,  1.3345e-06,  1.3345e-06,\n",
      "          7.0399e-02,  4.3018e-01, -3.5978e-01, -1.4079e-01,  2.3465e-01,\n",
      "          1.8771e-01, -4.6925e-02,  1.0559e-01, -1.0558e-01,  4.6928e-02,\n",
      "          2.3462e-01,  7.8205e-02, -3.0905e-02,  5.7854e-01,  1.5638e-01,\n",
      "         -3.4234e-01,  2.3066e-01,  1.1168e-01,  1.3345e-06,  1.3345e-06]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_spread1_xs\n",
      "tensor([[-0.4934, -0.3736, -0.3736, -0.4935, -0.2538, -0.2537, -0.3736, -0.3736,\n",
      "         -0.4934, -0.3736, -0.1338, -0.3736, -0.3736, -0.2538, -0.2538, -0.2538,\n",
      "         -0.4935, -0.4935, -0.4935, -0.4935, -0.2538, -0.3736, -0.3736, -0.3736,\n",
      "         -0.2536, -0.2537, -0.3736, -0.3736, -0.4935, -0.3736, -0.2538, -0.2538,\n",
      "         -0.2538, -0.2538, -0.2538, -0.3736, -0.3736, -0.3736, -0.2538, -0.4933,\n",
      "         -0.3735, -0.3735, -0.3736, -0.2538, -0.2537, -0.2537, -0.2537, -0.3736,\n",
      "         -0.0141, -0.2537, -0.2537, -0.3736, -0.2537, -0.4934, -0.3736, -0.2537,\n",
      "         -0.4934, -0.3736, -0.4934, -0.3736, -0.3736, -0.3735, -0.3735, -0.2536,\n",
      "         -0.2536, -0.2536, -0.2536, -0.2536, -0.2536, -0.3735, -0.3735, -0.2536,\n",
      "         -0.2536, -0.2536, -0.3735, -0.3735, -0.1338, -0.1338, -0.1338, -0.1338,\n",
      "         -0.2536, -0.2536, -0.2536, -0.1338, -0.1338, -0.2536, -0.3736, -0.3734,\n",
      "         -0.3734, -0.2536, -0.3734, -0.3735, -0.2536, -0.3734, -0.3734, -0.4933,\n",
      "         -0.4933, -0.3734, -0.3736, -0.3734, -0.0138, -0.2535, -0.2536, -0.4934,\n",
      "         -0.4934, -0.3735, -0.3735, -0.3735, -0.2537, -0.1338, -0.2536, -0.2536,\n",
      "         -0.0140, -0.3736, -0.2537, -0.2537, -0.2537, -0.3736, -0.4934, -0.3736,\n",
      "         -0.3735, -0.3735, -0.3736, -0.4934, -0.3736, -0.3736, -0.3736, -0.4933,\n",
      "         -0.4933, -0.3735, -0.1339, -0.3735, -0.4934, -0.4934, -0.4933, -0.1337,\n",
      "         -0.3736, -0.2537, -0.3735, -0.2537, -0.2537, -0.3735, -0.3735, -0.3736,\n",
      "         -0.3736, -0.3736, -0.3736, -0.3736, -0.3736, -0.3736, -0.3736, -0.3736,\n",
      "         -0.3736, -0.3736, -0.3736, -0.3736, -0.4934, -0.3735, -0.3735, -0.3734,\n",
      "         -0.4933, -0.4934, -0.3735, -0.4934, -0.2536, -0.2536, -0.2536, -0.4934,\n",
      "         -0.3734, -0.3734, -0.3734, -0.2536, -0.3734, -0.4934, -0.4934, -0.3734,\n",
      "         -0.2535, -0.2535, -0.3734, -0.1337, -0.2536, -0.2536, -0.2536, -0.2536,\n",
      "         -0.2536, -0.3734, -0.2535, -0.2535, -0.2536, -0.0138, -0.0138, -0.0138,\n",
      "         -0.1337, -0.4934, -0.3734, -0.3734, -0.2536, -0.2536, -0.2536, -0.2536,\n",
      "         -0.2536, -0.2536, -0.2536, -0.2536, -0.2536, -0.2536, -0.2536, -0.1338,\n",
      "         -0.2537, -0.3735, -0.1338, -0.2537, -0.2537, -0.3735, -0.3736, -0.4934,\n",
      "         -0.4934, -0.4934, -0.4934, -0.4934, -0.4934, -0.4934, -0.4934, -0.4934,\n",
      "         -0.4934, -0.2537, -0.3735, -0.3736, -0.1338, -0.2537, -0.3736, -0.2537,\n",
      "         -0.2537, -0.3736, -0.3736, -0.3737, -0.3737, -0.3737, -0.3737, -0.3737,\n",
      "         -0.2538, -0.2538, -0.2537, -0.2538, -0.3736, -0.2538, -0.2538, -0.2538,\n",
      "         -0.4933, -0.4933, -0.2536, -0.2537, -0.3735, -0.2536, -0.1337, -0.3734,\n",
      "         -0.2536, -0.3734, -0.2537, -0.4934, -0.2536, -0.2536, -0.2536, -0.2536,\n",
      "         -0.3734, -0.3734, -0.3734, -0.3734, -0.3734, -0.3735, -0.3735, -0.3734,\n",
      "         -0.3734, -0.3734, -0.3734, -0.3734, -0.2535, -0.2535, -0.4934, -0.2534,\n",
      "         -0.2534, -0.2534, -0.2534, -0.2534, -0.2534, -0.3734, -0.3734, -0.4933,\n",
      "         -0.1336, -0.2535, -0.2535, -0.2536, -0.3734, -0.3734, -0.3734, -0.3734,\n",
      "         -0.4933, -0.4933, -0.3733, -0.3733, -0.2536, -0.3735, -0.2536, -0.2537,\n",
      "         -0.1337, -0.1337, -0.2536, -0.2536, -0.2537, -0.2536, -0.3734, -0.3735,\n",
      "         -0.2537, -0.2537, -0.2537, -0.3736, -0.1339, -0.1338, -0.2537, -0.3735,\n",
      "         -0.2536, -0.2536, -0.1338, -0.2536, -0.2537, -0.1336, -0.1338, -0.1338,\n",
      "         -0.3736, -0.3735, -0.2537, -0.3735, -0.1339, -0.2538, -0.2539, -0.3737,\n",
      "         -0.3737, -0.3735, -0.3738, -0.4935, -0.3738, -0.2541, -0.1344, -0.2540,\n",
      "         -0.3737, -0.3737, -0.3737, -0.3737, -0.4936, -0.3737, -0.2541, -0.3738,\n",
      "         -0.2540, -0.2541, -0.1344, -0.1344, -0.1343, -0.2542, -0.4936, -0.1344,\n",
      "         -0.1344, -0.2541, -0.1343, -0.1344, -0.1344, -0.1344, -0.2541, -0.2540,\n",
      "         -0.2541, -0.3737, -0.3737, -0.0147, -0.3738, -0.2541, -0.1343, -0.1343,\n",
      "         -0.2541, -0.3739, -0.3739, -0.2541, -0.4935, -0.1343, -0.4936, -0.4935,\n",
      "         -0.2540, -0.3738, -0.2541, -0.3738, -0.2541, -0.0146, -0.1343, -0.2540,\n",
      "         -0.1344, -0.1344, -0.3739, -0.2542, -0.1344, -0.1344, -0.2540, -0.3737,\n",
      "         -0.1344, -0.1344, -0.1344, -0.1343, -0.0145, -0.1344, -0.1343, -0.3737,\n",
      "         -0.1344, -0.0147, -0.2541, -0.2541, -0.2541, -0.2541, -0.2541, -0.2540,\n",
      "         -0.2540, -0.1344, -0.2542, -0.2540, -0.3737, -0.2540, -0.2540, -0.3737,\n",
      "         -0.2540, -0.2540, -0.2540, -0.2540, -0.1344, -0.1344, -0.3737, -0.1344,\n",
      "         -0.1344, -0.1344, -0.1344, -0.3739, -0.2540, -0.2540, -0.4934, -0.3737,\n",
      "         -0.3739, -0.3739, -0.2540, -0.2540, -0.3739, -0.3739, -0.3739, -0.3739,\n",
      "         -0.3739, -0.3739, -0.3739, -0.3739, -0.2540, -0.2540, -0.2540, -0.2540,\n",
      "         -0.2540, -0.3739, -0.3739, -0.3740, -0.2542, -0.3738, -0.3738, -0.3740,\n",
      "         -0.3740, -0.3740, -0.3740, -0.2543, -0.2543, -0.3740, -0.3738, -0.2545,\n",
      "         -0.2545, -0.2545, -0.2545, -0.1346, -0.1346, -0.1346, -0.2542, -0.2543,\n",
      "         -0.2543, -0.2543, -0.3739, -0.4936, -0.4936, -0.2543, -0.2542, -0.3738,\n",
      "         -0.3738, -0.2542, -0.2542, -0.2542, -0.2542, -0.3738, -0.3741, -0.3741,\n",
      "         -0.1349, -0.3740, -0.3740, -0.3740, -0.3741, -0.2545, -0.2545, -0.2545,\n",
      "         -0.1346, -0.2544, -0.2545, -0.3740, -0.3741, -0.3741, -0.3741, -0.3741,\n",
      "         -0.3741, -0.3741, -0.3741, -0.3740, -0.3741, -0.3741, -0.3740, -0.3740,\n",
      "         -0.3740, -0.1345, -0.1345, -0.1345, -0.1345, -0.1345, -0.1345, -0.2542,\n",
      "         -0.2543, -0.3738, -0.2542, -0.2542, -0.2542, -0.2542, -0.2542, -0.2544,\n",
      "         -0.2545, -0.2544, -0.1346, -0.1346, -0.1346, -0.2542, -0.2542, -0.2542,\n",
      "         -0.2542, -0.3740, -0.3738, -0.3738, -0.4934, -0.3738, -0.4936, -0.4936,\n",
      "         -0.4936, -0.3738, -0.2542, -0.3738, -0.2542, -0.4937, -0.2545, -0.2545,\n",
      "         -0.3740, -0.3738, -0.2542, -0.2542, -0.4937, -0.3738, -0.3740, -0.2542,\n",
      "         -0.2542, -0.2542, -0.2542, -0.3740, -0.2545, -0.2545, -0.2544, -0.2542,\n",
      "         -0.2544, -0.2544, -0.2544, -0.2544, -0.2544, -0.3741, -0.2544, -0.2544,\n",
      "         -0.2545, -0.3741, -0.2545, -0.2545, -0.2545, -0.2545, -0.1349, -0.3741,\n",
      "         -0.1349, -0.1348, -0.3739, -0.2545, -0.1348, -0.3739, -0.6133, -0.6133]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2328/195287288.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'row_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale_optiver_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         )\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "for key,val in dataset[10000][0].items():\n",
    "    print(key)\n",
    "#     print(val)\n",
    "    if key not in ['row_id']:\n",
    "        print(scale_optiver_feature(key, val.reshape(1,-1)))\n",
    "    input()\n",
    "    \n",
    "    \n",
    "# for idx in range(len(dataset)):\n",
    "#     feature_x = dataset[idx][0]\n",
    "#     test = scale_optiver_feature('logret1_xs', feature_x['logret1_xs'].reshape(1,-1))\n",
    "#     print(feature_x['logret1_xs'].tolist())\n",
    "#     print(test.tolist())\n",
    "#     input()\n",
    "#     realized_volatility_xs = scale_optiver_feature('realized_volatility_xs', feature_x['logret1_xs'].reshape(1,-1))\n",
    "#     print(feature_x['book_realized_volatility'])\n",
    "# #     print(feature_x['logret1_xs'])\n",
    "#     print(realized_volatility_xs)\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15e35973-37f1-408a-b47f-3a8fa6c9879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockIdEmbedding(nn.Module):\n",
    "    def __init__(self,number_of_stock_embeddings=126+10, number_of_stock_embedding_dimention=2, mode='train'):\n",
    "        super(StockIdEmbedding, self).__init__()\n",
    "        \n",
    "        self.number_of_stock_embeddings = number_of_stock_embeddings\n",
    "        self.number_of_stock_embedding_dimention = number_of_stock_embedding_dimention\n",
    "        self.stock_embedding = nn.Embedding(self.number_of_stock_embeddings, self.number_of_stock_embedding_dimention)\n",
    "        self.mode = mode\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(self.number_of_stock_embedding_dimention, 32),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return []\n",
    "    \n",
    "    def set_mode(self,mode):\n",
    "        self.mode = mode\n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "        \n",
    "        stock_id_clamped = torch.clamp(feature_dict['stock_id'],0,self.number_of_stock_embeddings-1)\n",
    "        stock_id_clamped = stock_id_clamped.type(torch.int64)\n",
    "        stock_id_clamped = stock_id_clamped.to(device).reshape(-1,1)\n",
    "        embedding_logits = self.stock_embedding(stock_id_clamped)\n",
    "        embedding_logits = embedding_logits.reshape(-1,self.number_of_stock_embedding_dimention)\n",
    "        \n",
    "        if self.mode == 'stock_id_embedding':\n",
    "            return embedding_logits\n",
    "\n",
    "            \n",
    "        logits = self.linear_stack(embedding_logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad1ae512-810e-410f-a177-127168b56106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2328/1801758537.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_batch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mFeature_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFeature_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         )\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len,1, d_model)\n",
    "#         print(pe.size())\n",
    "        pe[:,0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:,0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "#         print(\"x\",x.size())\n",
    "#         print(\"PE\",self.pe[:,:x.size(1)].size())\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class MultiFetTransformer(nn.Module):\n",
    "    def __init__(self,feature_names=None, overview_feature_names=None, mask_feature_name='sequence_mask_xs',sequence_feature_name='seconds_in_bucket_xs', mode=\"train\"):\n",
    "        \"\"\"single feature, feature learner\n",
    "        `mode`: train|feature_generator\n",
    "        \"\"\"\n",
    "        super(MultiFetTransformer,self).__init__()\n",
    "        if feature_names is None:\n",
    "             \n",
    "#             feature_names = ['seconds_in_bucket_xs_group','has_trade_data_xs'] + [ f'{feature_name}_sum_xs' for feature_name in ['wap1','wap_balance','logret1','logret2','logrett',\n",
    "#                              'price_spread1','bid_spread','ask_spread','total_volume','volume_imbalance',\n",
    "#                             'size','order_count','trade_money_turnover','trade_book_price_spread']] + [ f'{feature_name}_max_xs' for feature_name in ['wap1','wap_balance','logret1','logret2','logrett',\n",
    "#                              'price_spread1','bid_spread','ask_spread','total_volume','volume_imbalance',\n",
    "#                             'size','order_count','trade_money_turnover','trade_book_price_spread']]\n",
    "            # 'bid_price2','bid_size2','ask_price2','ask_size2',\n",
    "            feature_names = [f'{feature_name}_xs' for feature_name in ['logret1','price_spread1','bid_spread','ask_spread','directional_volume1','directional_volume2','trade_money_turnover_per_order']]\n",
    "            \n",
    "#             feature_names = ['seconds_in_bucket_xs_group','has_trade_data_xs']\n",
    "        \n",
    "        if overview_feature_names is None:\n",
    "            overview_feature_names = ['wap1_sum', 'wap1_std', 'logret1_realized_volatility', 'logret2_realized_volatility', 'logrett_realized_volatility', 'wap_balance_sum', 'wap_balance_max', 'price_spread1_sum', 'price_spread1_max', 'bid_spread_sum', 'bid_spread_max', 'ask_spread_sum', 'ask_spread_max', 'total_volume_sum', 'total_volume_max', 'volume_imbalance_sum',\n",
    "                                      'volume_imbalance_max', 'bid_ask_spread_sum', 'bid_ask_spread_max', 'size_sum', 'size_max', 'size_min', 'order_count_sum', 'order_count_max', 'trade_money_turnover_sum', 'trade_money_turnover_max', 'trade_money_turnover_min']\n",
    "            \n",
    "        \n",
    "        \n",
    "        if type(feature_names) == str:\n",
    "            feature_names = [feature_names]\n",
    "            \n",
    "        self.stock_id_embedding_dimension = 6\n",
    "        self.stock_id_embedding = StockIdEmbedding(number_of_stock_embedding_dimention=self.stock_id_embedding_dimension, mode='stock_id_embedding')\n",
    "        \n",
    "        self.feature_names = feature_names\n",
    "        self.overview_feature_names = overview_feature_names\n",
    "        self.mask_feature_name = mask_feature_name\n",
    "        self.sequence_feature_name = sequence_feature_name\n",
    "        self.features_count = len(self.feature_names)\n",
    "        self.scaled_feature_dimension = 4\n",
    "        self.feature_scalers = nn.ModuleList([nn.Sequential(\n",
    "                    # one is original feature, then stock,\n",
    "                    nn.Linear(1+self.stock_id_embedding_dimension, 32),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(32, self.scaled_feature_dimension)\n",
    "                ) for _ in self.feature_names])\n",
    "#         for feature_name in self.feature_names:\n",
    "#             self.feature_scalers.append()\n",
    "        \n",
    "        self.output_dimensions = 256\n",
    "        self.transformer_input_dimension = 64\n",
    "#         self.positional_encoding = PositionalEncoding(d_model=self.transformer_input_dimension)\n",
    "        self.mode = mode\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.transformer_input_dimension, nhead=4, activation='gelu')\n",
    "        self.encoder_stack = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "        \n",
    "        self.feature_to_feature_embedding = nn.Sequential(\n",
    "            nn.Linear(self.features_count  * self.scaled_feature_dimension, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, self.transformer_input_dimension)\n",
    "        )\n",
    "        \n",
    "        self.linear_feature_stack_ = nn.Sequential(\n",
    "#             nn.Conv1d(self.transformer_input_dimension, 32, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(32, 24, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Flatten(),\n",
    "            nn.Linear(data_intervals_count*self.transformer_input_dimension,1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024,self.output_dimensions),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, self.features_out),\n",
    "        )\n",
    "        \n",
    "        self.linear_trainer_stack_ = nn.Sequential(\n",
    "#             nn.Linear(self.output_dimensions + len(self.overview_feature_names) + self.stock_id_embedding_dimension, 256),\n",
    "            nn.Linear(self.output_dimensions, 256),\n",
    "#             nn.Linear(len(self.overview_feature_names) + self.stock_id_embedding_dimension, 256),\n",
    "#             nn.Linear(self.features_out, 1),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.Linear(self.features_out, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, 128),\n",
    "#             nn.Linear(self.features_out, 1),\n",
    "            nn.ReLU(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.Hardswish(),\n",
    "            nn.Linear(128, 1),   \n",
    "        )\n",
    "        \n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def forward(self, feature_dict, h0_tensor=None):    \n",
    "        feature_x = []\n",
    "        stock_embedding_logits = self.stock_id_embedding(feature_dict)\n",
    "        stock_embedding_logits = stock_embedding_logits.unsqueeze(1)#.unsqueeze(0)\n",
    "        stock_embedding_logits = stock_embedding_logits.repeat(1,data_intervals_count,1)\n",
    "        \n",
    "        for idx,feature_name in enumerate(self.feature_names):\n",
    "            feature_tensor = scale_optiver_feature(feature_name, feature_dict[feature_name]).to(device)\n",
    "            feature_tensor = feature_tensor.reshape(-1,feature_tensor.size(1),1)\n",
    "\n",
    "\n",
    "            feature_transformed = torch.cat([feature_tensor, stock_embedding_logits], 2)\n",
    "            \n",
    "            feature_x.append(self.feature_scalers[idx](feature_transformed))\n",
    "        \n",
    "        #combine all the features activated tensors\n",
    "        feature_x = torch.cat(feature_x,dim=2) #.reshape(-1, data_intervals_count, self.input_size_)\n",
    "\n",
    "        self.feature_to_feature_embedding = self.feature_to_feature_embedding.to(device)\n",
    "        feature_x = self.feature_to_feature_embedding(feature_x)\n",
    "        \n",
    "        #positional encoding\n",
    "        position_encodings = feature_dict[self.sequence_feature_name]\n",
    "        position_encodings = (position_encodings.to(device)+1)/10000\n",
    "        position_encodings = position_encodings.reshape(-1,feature_x.size(1),1)\n",
    "        feature_x = torch.add(feature_x, position_encodings)\n",
    "        \n",
    "        #Mask prepare\n",
    "        mask = feature_dict[self.mask_feature_name].to(device)\n",
    "        \n",
    "        \n",
    "        # make them sequence first!!\n",
    "        # RANT: all of this due to Kaggle using pytorch 1.7.0\n",
    "        feature_x = torch.stack(feature_x.unbind(0),dim=1)\n",
    "        \n",
    "        out = self.encoder_stack(feature_x, src_key_padding_mask=mask)\n",
    "        # back to batch first!\n",
    "        out = torch.stack(out.unbind(0),dim=1)\n",
    "        out = out.reshape(-1, self.transformer_input_dimension*data_intervals_count)\n",
    "        \n",
    "\n",
    "        out_ = self.linear_feature_stack_(out)\n",
    "        # we add overview features here CAT them; that's why \n",
    "        out_ = self.linear_trainer_stack_(out_)\n",
    "\n",
    "        return out_\n",
    "\n",
    "model = MultiFetTransformer()\n",
    "model.to(device)\n",
    "dataloader_train = DataLoader(dataset, batch_size=2,\n",
    "                                shuffle=True, num_workers=0, pin_memory=False)#, collate_fn=optiver_custom_collate_func)\n",
    "\n",
    "\n",
    " \n",
    "# >>> src = torch.rand(10, 32, 512)\n",
    "# >>> out = encoder_layer(src)   \n",
    "for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "    model(Feature_X)\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "58e3006a-19df-4243-b389-07e09236408e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "316cdf05-8827-491b-805c-902be90598c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VolatilityGRU(nn.Module):\n",
    "#     def __init__(self, input_size=1, hidden_size=64, repeated_cells=1):\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "class MultiFetGRU(nn.Module):\n",
    "    def __init__(self,feature_names=None, hidden_size=64, layers=4, dropout=0, features_out=32, mode=\"train\"):\n",
    "        \"\"\"single feature, feature learner\n",
    "        `mode`: train|feature_generator\n",
    "        \"\"\"\n",
    "        super(MultiFetGRU,self).__init__()\n",
    "        if feature_names is None:\n",
    "            feature_names = ['book_logret1_pow2_xs','book_logret2_pow2_xs','book_price_spread1_xs','book_bid_spread_xs',\n",
    "            'book_ask_spread_xs','book_total_volume_xs','book_volume_imbalance_xs','has_trade_data_xs',\n",
    "            'trade_volume_xs','trade_order_count_xs','trade_book_price_spread_xs','seconds_in_bucket_xs','sequence_mask_xs']\n",
    "        if type(feature_names) == str:\n",
    "            feature_names = [feature_names]\n",
    "        self.feature_names = feature_names\n",
    "        self.input_size_ = len(self.feature_names)\n",
    "        \n",
    "        self.hidden_size_ = hidden_size\n",
    "        self.repeated_lstm_cells_ = layers\n",
    "        self.dropout_ = dropout\n",
    "        self.features_out = features_out\n",
    "        self.initial_dropout_ = nn.Dropout(0.2)\n",
    "        self.rnn_ = nn.GRU(self.input_size_, self.hidden_size_, self.repeated_lstm_cells_, batch_first=True, dropout=self.dropout_)\n",
    "        \n",
    "        self.linear_feature_stack_ = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size_, self.features_out),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, self.features_out),\n",
    "        )\n",
    "        \n",
    "        self.linear_trainer_stack_ = nn.Sequential(\n",
    "            nn.Linear(self.features_out, 128),\n",
    "#             nn.Linear(self.features_out, 1),\n",
    "            nn.Hardswish(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.Hardswish(),\n",
    "            nn.Linear(128, 1),   \n",
    "        )\n",
    "        \n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def forward(self, feature_dict, h0_tensor=None):\n",
    "        \n",
    "            \n",
    "        feature_x = []\n",
    "        for feature_name in self.feature_names:\n",
    "            if feature_name == 'book_realized_volatility_xs':\n",
    "                feature_tensor = feature_dict['logret1_xs']\n",
    "            elif feature_name == 'trade_realized_volatility_xs':\n",
    "                feature_tensor = feature_dict['logrett_xs']\n",
    "            else:\n",
    "                feature_tensor = feature_dict[feature_name]\n",
    "                \n",
    "            feature_x.append(scale_optiver_feature(feature_name, feature_tensor).to(device))\n",
    "            \n",
    "        feature_x = torch.stack(feature_x,dim=2) #.reshape(-1, data_intervals_count, self.input_size_)\n",
    "        feature_x = self.initial_dropout_(feature_x)\n",
    "        if self.mode in [\"feature_generator\",\"train\"]:\n",
    "#             if h0_tensor is None:\n",
    "# #                 h_0_ = torch.rand(self.repeated_lstm_cells_, feature_x.size(0), self.hidden_size_, device=device) #hidden state\n",
    "#                 h_0_ = torch.zeros(self.repeated_lstm_cells_, feature_x.size(0), self.hidden_size_, device=device) #hidden state\n",
    "#             else:\n",
    "#                 h_0_ = h0_tensor\n",
    "            output_, hn_ = self.rnn_(feature_x)#, h_0_) #lstm with input, hidden, and internal state\n",
    "#             print('output',output_.size(), 'hn', hn_.size())\n",
    "#             print(output_[:,-1].size())\n",
    "#             input()\n",
    "#             hn_ = hn_.reshape(-1, self.hidden_size_*self.repeated_lstm_cells_) #reshaping the data for Dense layer next  \n",
    "#             output_ = output_.reshape(-1, self.hidden_size_*self.repeated_lstm_cells_) #reshaping the data for Dense layer next  \n",
    "            \n",
    "            out_ = self.linear_feature_stack_(output_[:,-1])\n",
    "            \n",
    "            if self.mode == \"train\":\n",
    "                out_ = self.linear_trainer_stack_(out_)\n",
    "            \n",
    "            return out_\n",
    "            \n",
    "            \n",
    "            \n",
    "class VolatilityBSModel(nn.Module):\n",
    "    def __init__(self, mode=\"hybrid\"):\n",
    "        \"\"\"various rnn features' fusion with fully connected nn\n",
    "        `mode`: hybrid|<feature_name>\n",
    "        \"\"\"\n",
    "        super(VolatilityBSModel, self).__init__()\n",
    "        self.mode = mode\n",
    "#         self.feature_list = ['logrett_xs','trade_volume_xs','trade_ordercount_xs','trade_money_turnover_xs','trade_money_turnover_per_order_xs',\n",
    "#                              'logret1_xs',\n",
    "#                              'book_price_spread1_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "#                              'book_total_volume_xs','book_volume_imbalance_xs','book_money_turnover_intention1_xs','book_wap1_local_standardized_xs','trade_price_local_standardized_xs']\n",
    "#         self.feature_list = [[x] for x in self.feature_list]\n",
    "        # 'trade_volume_xs','trade_ordercount_xs','trade_money_turnover_xs','trade_money_turnover_per_order_xs',\n",
    "#                             #realized_volatility_xs  \n",
    "        self.feature_list = [\n",
    "            \n",
    "            ['book_logret1_sum_xs','book_logret1_realized_volatility_xs','book_logret1_std_xs','book_logret1_mean_xs',\n",
    "            'book_logret2_sum_xs','book_logret2_realized_volatility_xs','book_logret2_std_xs','book_logret2_mean_xs',\n",
    "            'trade_size_sum_xs','trade_size_std_xs','trade_order_count_sum_xs','trade_order_count_std_xs','trade_order_count_std_xs','trade_trade_money_turnover_sum_xs','trade_trade_money_turnover_std_xs',\n",
    "            'trade_logrett_sum_xs','trade_logrett_realized_volatility_xs','trade_logrett_std_xs','trade_logrett_mean_xs',\n",
    "            'book_price_spread1_sum_xs','book_price_spread1_std_xs','book_bid_spread_sum_xs','book_bid_spread_std_xs','book_ask_spread_sum_xs',\n",
    "                    'book_ask_spread_std_xs','book_total_volume_sum_xs','book_total_volume_std_xs','book_volume_imbalance_sum_xs','book_volume_imbalance_std_xs']]\n",
    "        # ['trade_price_local_standardized_xs','trade_money_turnover_xs','book_money_turnover_intention1_xs','book_wap1_local_standardized_xs']\n",
    "#         self.feature_list = [['logrett_xs','logret1_xs'],['trade_volume_xs','trade_ordercount_xs','trade_money_turnover_per_order_xs','book_money_turnover_intention1_xs','trade_price_local_standardized_xs','book_wap1_local_standardized_xs'],['book_price_spread1_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "#                              'book_total_volume_xs','book_volume_imbalance_xs']]\n",
    "         \n",
    "        self.feature_gen_feature_size = 128\n",
    "        self.feature_gen_models = {}\n",
    "        self.rnn_hidden_size = 128\n",
    "        self.rnn_layers = 4\n",
    "        self.stock_embedding_dimention = 6\n",
    "        self.stock_id_embedding = StockIdEmbedding(number_of_stock_embedding_dimention=self.stock_embedding_dimention, mode='stock_id_embedding')\n",
    "        self.hidden_generator_network = NeuralNetwork(feature_generator_mode_hidden_size=self.feature_gen_feature_size)\n",
    "        \n",
    "        for k in self.feature_list:\n",
    "            \n",
    "#             self.feature_gen_models[str(k)]=MultiFetGRU(k, hidden_size=self.rnn_hidden_size, layers=self.rnn_layers, dropout=0.1, features_out=self.feature_gen_feature_size) \n",
    "            self.feature_gen_models[str(k)]=MultiFetTransformer(k, features_out=self.feature_gen_feature_size) \n",
    "            self.feature_gen_models[str(k)].to(device)\n",
    "        \n",
    "        \n",
    "        self.linear_fusion = nn.Sequential(\n",
    "            #self.feature_gen_feature_size*len(self.feature_list) + self.rnn_hidden_size*self.rnn_layers + 2 + 1\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LazyLinear(256),\n",
    "            nn.Hardswish(),\n",
    "#             nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.1),\n",
    "#             nn.Linear(256,256),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Dropout(0.05),\n",
    "            nn.Linear(256,64),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Dropout(0.05),\n",
    "#             nn.Linear(128,64),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Dropout(0.05),\n",
    "#             nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "        self.set_mode(self.mode)\n",
    "    \n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return self.feature_list\n",
    "    \n",
    "    def set_mode(self, mode):\n",
    "        print(f\"------- set mode : {mode} -----------\")\n",
    "        self.mode = mode\n",
    "        for feature_gen_model in self.feature_gen_models.values():\n",
    "            feature_gen_model.set_mode('feature_generator' if self.mode in ['hybrid','hybrid_feature_out','hidden_generator','ultimate'] else 'train')\n",
    "        if self.mode == 'hidden_generator':\n",
    "            self.hidden_generator_network.set_mode('train')\n",
    "        else:\n",
    "            self.hidden_generator_network.set_mode('hidden_generator')\n",
    "        self.stock_id_embedding.set_mode('stock_id_embedding')\n",
    "    \n",
    "    def parameters(self):\n",
    "        \n",
    "        generator_sources_map = {str(k):[v] for k,v in self.feature_gen_models.items()}\n",
    "        generator_sources_map['hybrid']= [self.linear_fusion, self.stock_id_embedding]\n",
    "        generator_sources_map['hidden_generator'] = [self.hidden_generator_network] \n",
    "        generator_sources_map['ultimate']= [self.linear_fusion, self.hidden_generator_network, self.stock_id_embedding] + list(self.feature_gen_models.values())\n",
    "        params = []\n",
    "        # mode and key is actually str version of array of strings with feature name as values e.g. ['logret1_xs','volume_xs']\n",
    "        if str(self.mode) in generator_sources_map:\n",
    "            for generator_source in generator_sources_map[str(self.mode)]:\n",
    "                for param in generator_source.parameters():\n",
    "                    params.append(param)\n",
    "        else:\n",
    "            return super(VolatilityBSModel,self).parameters()\n",
    "        return params\n",
    "    \n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "        \n",
    "        \n",
    "        if self.mode in self.feature_list:\n",
    "            \n",
    "            \n",
    "            # pass in some randomness to the initial hidden tensor to force it to learn some stuff on its own\n",
    "            # otherwise as the initial hidden layer contains solid infor to minimize the loss; it'll just use that hidden layer to minimize and instead\n",
    "            # learn to not learn and directly bypass initial hidden\n",
    "#             h0_tensor.masked_fill_((torch.rand(h0_tensor.size()) > 0.5).to(device), 0.0)\n",
    "#             h0_tensor = torch.zeros(self.rnn_layers, -1, self.rnn_hidden_size)\n",
    "            out = self.feature_gen_models[str(self.mode)](feature_dict, h0_tensor=None)\n",
    "            return out\n",
    "        \n",
    "        if self.mode in ['hidden_generator']:\n",
    "            out = self.hidden_generator_network(feature_dict)\n",
    "            return out\n",
    "        \n",
    "        if self.mode in ['hybrid','hybrid_feature_out','ultimate']:\n",
    "            generated_features = []\n",
    "            for feature_name, feature_gen_model in self.feature_gen_models.items():\n",
    "#                 h0_tensor = torch.zeros(self.rnn_layers, -1, self.rnn_hidden_size)\n",
    "                features_out = feature_gen_model(feature_dict, h0_tensor=None)\n",
    "                generated_features.append(features_out)\n",
    "                \n",
    "                \n",
    "            combined_features = torch.cat(generated_features, 1)#.reshape(-1, self.feature_gen_feature_size*len(self.feature_list))\n",
    "            \n",
    "            cnn_features = self.hidden_generator_network(feature_dict)\n",
    "#             cnn_features = cnn_features.reshape(self.rnn_layers,-1,self.rnn_hidden_size)\n",
    "            combined_features = torch.cat([combined_features,cnn_features],1)\n",
    "    \n",
    "            embedding_logits = self.stock_id_embedding(feature_dict)\n",
    "            combined_features = torch.cat([combined_features,embedding_logits],1)\n",
    "            \n",
    "            realized_volatility_logits = feature_dict['book_realized_volatility'].to(device).reshape(-1,1) * realize_volatility_scale_factor\n",
    "#             realized_volatility_logits = scale_optiver_feature('book_realized_volatility',feature_dict['book_realized_volatility']).to(device).reshape(-1,1)\n",
    "#             realized_volatility_logits = realized_volatility_logits # * realize_volatility_scale_factor\n",
    "            combined_features = torch.cat([combined_features,realized_volatility_logits],1)\n",
    "            \n",
    "            if self.mode == 'hybrid_feature_out':\n",
    "                return combined_features\n",
    "            \n",
    "            out = self.linear_fusion(combined_features)\n",
    "            return out\n",
    "        \n",
    "#         input(\"--- out got\")\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "630063c4-d9d7-49f5-8ead-44bcf6380654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20776/3209545698.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_batch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mFeature_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFeature_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         )\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a33b331-4eb1-4c09-84df-01b687ac3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VolatilityBSModel(use_stock_id=use_stock_id)\n",
    "# # model = NeuralNetwork(use_stock_id=False)\n",
    "# model.to(device)\n",
    "# adam_for_modes = {}\n",
    "\n",
    "# for modeidx, mode in enumerate(['yoyo','trade','experiment','book','hybrid']*1):\n",
    "#     epochs = 1\n",
    "#     model.mode = mode\n",
    "#     print(model.parameters())\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1840c-2bc4-4521-933f-994f1ceb014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VolatilityBSModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9218f-b67a-49af-bd37-ff307bd12ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49a121-34ca-498f-9267-4c20a04e2c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.children():\n",
    "    print()\n",
    "    print()\n",
    "    print(layer)\n",
    "    print(\"-------\")\n",
    "    for l in layer.children():\n",
    "        print([x for x in l.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1f24f-c586-4e1b-aad6-fb4671c87232",
   "metadata": {},
   "source": [
    "#### analyze the initial weights (or change them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "931890cf-e329-4a15-b0d0-352dc27f985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @torch.no_grad()\n",
    "# def init_weights(m):\n",
    "# #     print(m)\n",
    "#     if type(m) == nn.Linear:\n",
    "# #         m.weight.fill_(1.0)\n",
    "#         torch.nn.init.xavier_uniform_(m.weight,gain=10)\n",
    "#         m.bias.data.uniform_(-1,1)\n",
    "# #     elif type(m) == nn.ReLU:\n",
    "# #         print(m.data)\n",
    "#     else:\n",
    "#         print(type(m))\n",
    "# #         print(m.weight)\n",
    "# model.apply(init_weights)\n",
    "# # for param in model.parameters():\n",
    "# # #     print(param)\n",
    "# #       print(param.data.size(), param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c79a7d-9e08-4433-9c77-1af049f349d6",
   "metadata": {},
   "source": [
    "### LEarning rate: our base line is 0.34 loss as that's what the optiver guys have when they use current 10 min realize vol and use it as target (copy to prediction). We create simplest neural network and work with learning rates to figure out what's best and when we see something in range of 0.35 then we've found good Learning rate\n",
    "- #### SGD: 1e-7 works best\n",
    "- #### ADAM: 1e-5, (NOTE: 1e-3 makes it behave dumb where some deep local minima gets stuck and produces constant output!)\n",
    "- TODO: analyze that constant output phenomenon more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5cb3871f-5215-4afb-92fc-47ae2d385fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-4\n",
    "# batch_size = 4096\n",
    "# epochs = 100\n",
    "\n",
    "# input_scaling = 1\n",
    "# output_scaling = 1\n",
    "\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "# strategyname = \"ret1_n_ret2\"\n",
    "# summary_writer = SummaryWriter(f'../output/training_tensorboard/{strategyname}_scaleIn{input_scaling}Out{output_scaling}_{learning_rate}_{batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9aadc54b-6009-4af6-bfb6-f6619c6acd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a7ed1-80f8-4a57-90b4-223345dde76b",
   "metadata": {},
   "source": [
    "### Learnings about training\n",
    "- (non scaling)logreturns input and volatility output; non scaled makes the model predict constant output with no variety(close to 0 std dev)\n",
    "- scaling input rids of variety issue, \n",
    "- scaling output makes the model start with low rmse initially so there's less ground to cover and we can iterate over ideas rapidly due to less epochs needed to achieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef34a31-7317-47c0-a94f-9bebe2fd2e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n",
      "Current epochs: 200 LR : 0.001 batch_size: 16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "---------- OPFet_stckembed_FetEbdNTrnslt_transformer_smolsize_0.001_16 train ----------\n",
      "train: 0.6146244435249087 [ 6832/343145] recent lr: 6.6875e-06\n",
      "train: 0.5385098656482785 [13680/343145] recent lr: 1.3375e-05\n",
      "train: 0.5388720569627308 [20528/343145] recent lr: 2.00625e-05\n",
      "train: 0.5320546468980959 [27376/343145] recent lr: 2.675e-05\n",
      "train: 0.5044570299927319 [34224/343145] recent lr: 3.34375e-05\n",
      "train: 0.4871295728833876 [41072/343145] recent lr: 4.0125e-05\n",
      "train: 0.4126358000892345 [47920/343145] recent lr: 4.68125e-05\n",
      "train: 0.3689997358439125 [54768/343145] recent lr: 5.35e-05\n",
      "train: 0.34947912576042606 [61616/343145] recent lr: 6.01875e-05\n",
      "train: 0.3302931709685058 [68464/343145] recent lr: 6.6875e-05\n",
      "train: 0.3249878706149409 [75312/343145] recent lr: 7.356249999999999e-05\n",
      "train: 0.3179384259350389 [82160/343145] recent lr: 8.025e-05\n",
      "train: 0.31031627493484 [89008/343145] recent lr: 8.693749999999999e-05\n",
      "train: 0.30615468089964903 [95856/343145] recent lr: 9.3625e-05\n",
      "train: 0.30411149307249863 [102704/343145] recent lr: 0.00010031249999999999\n",
      "train: 0.29326220138747 [109552/343145] recent lr: 0.000107\n",
      "train: 0.2967155749134928 [116400/343145] recent lr: 0.00011368749999999999\n",
      "train: 0.2845025438397566 [123248/343145] recent lr: 0.000120375\n",
      "train: 0.2878806497031283 [130096/343145] recent lr: 0.0001270625\n",
      "train: 0.28967702695142444 [136944/343145] recent lr: 0.00013375\n",
      "train: 0.27699934113248487 [143792/343145] recent lr: 0.0001404375\n",
      "train: 0.2733449886126496 [150640/343145] recent lr: 0.00014712499999999998\n",
      "train: 0.2803475941557472 [157488/343145] recent lr: 0.0001538125\n",
      "train: 0.2875731101729602 [164336/343145] recent lr: 0.0001605\n",
      "train: 0.2720353784405182 [171184/343145] recent lr: 0.0001671875\n",
      "train: 0.2805016190161772 [178032/343145] recent lr: 0.00017387499999999998\n",
      "train: 0.2706864418061537 [184880/343145] recent lr: 0.0001805625\n",
      "train: 0.27149738064993206 [191728/343145] recent lr: 0.00018725\n",
      "train: 0.27191002583392315 [198576/343145] recent lr: 0.0001939375\n",
      "train: 0.27185133360674446 [205424/343145] recent lr: 0.00020062499999999998\n",
      "train: 0.2652268484582968 [212272/343145] recent lr: 0.00020731249999999998\n",
      "train: 0.25914157350834843 [219120/343145] recent lr: 0.000214\n",
      "train: 0.26427820492013593 [225968/343145] recent lr: 0.0002206875\n",
      "train: 0.26589561765578307 [232816/343145] recent lr: 0.00022737499999999998\n",
      "train: 0.2677793434568655 [239664/343145] recent lr: 0.00023406249999999998\n",
      "train: 0.26334115887217435 [246512/343145] recent lr: 0.00024075\n",
      "train: 0.2588038370361395 [253360/343145] recent lr: 0.0002474375\n",
      "train: 0.2677347388153321 [260208/343145] recent lr: 0.000254125\n",
      "train: 0.2639105524519615 [267056/343145] recent lr: 0.0002608125\n",
      "train: 0.2624146302935676 [273904/343145] recent lr: 0.0002675\n",
      "train: 0.2646472974915371 [280752/343145] recent lr: 0.00027418749999999996\n",
      "train: 0.2552125785569443 [287600/343145] recent lr: 0.000280875\n",
      "train: 0.26374267953857083 [294448/343145] recent lr: 0.0002875625\n",
      "train: 0.2603160939135841 [301296/343145] recent lr: 0.00029424999999999997\n",
      "train: 0.2558579847355869 [308144/343145] recent lr: 0.0003009375\n",
      "train: 0.2524492532432636 [314992/343145] recent lr: 0.000307625\n",
      "train: 0.25773110578411096 [321840/343145] recent lr: 0.0003143125\n",
      "train: 0.2581102485272372 [328688/343145] recent lr: 0.000321\n",
      "train: 0.248107564087226 [335536/343145] recent lr: 0.00032768749999999996\n",
      "train: 0.25503981344958887 [342384/343145] recent lr: 0.000334375\n",
      "train: 0.261790081224543 test: 0.23989247862653962 [343136/343145] recent lr: 0.00033510937499999996\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "---------- OPFet_stckembed_FetEbdNTrnslt_transformer_smolsize_0.001_16 train ----------\n",
      "train: 0.2533703997279658 [ 6096/343145] recent lr: 0.00034107812499999996\n"
     ]
    }
   ],
   "source": [
    "# model = None\n",
    "strategyname = \"OPFet_stckembed_FetEbdNTrnslt_transformer_smolsize\"\n",
    "\n",
    "\n",
    "print(\"DEVICE:\", device)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "training_configs = []\n",
    "learning_rates_to_try = [1e-3]# 1e-4]\n",
    "batch_sizes_to_try = [16]#, 512]#,10000, 128]\n",
    "# input_scalings_to_try = [1000]\n",
    "# output_scalings_to_try = [10000]\n",
    "\n",
    "for learning_rate in learning_rates_to_try:\n",
    "    for batch_size in batch_sizes_to_try:\n",
    "        for use_stock_id in [False]:\n",
    "            training_configs.append({\n",
    "                'learning_rate':learning_rate,\n",
    "                'batch_size':batch_size,\n",
    "                'use_stock_id': use_stock_id\n",
    "            })\n",
    "\n",
    "epochs = 200\n",
    "for training_config in training_configs:\n",
    "    \n",
    "    learning_rate = training_config['learning_rate']\n",
    "    batch_size = training_config['batch_size']\n",
    "    use_stock_id = training_config['use_stock_id']\n",
    "    # TRAINING SETUP\n",
    "    \n",
    "    #refresh the model\n",
    "    \n",
    "    STRATEGY_NAME_WITH_ATTRS = f\"{strategyname}_{learning_rate}_{batch_size}\"\n",
    "    summary_writer = SummaryWriter(f'../output/training_tensorboard/{STRATEGY_NAME_WITH_ATTRS}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "#     model = VolatilityBSModel()\n",
    "#MultiFetGRU\n",
    "#MultiFetTransformer\n",
    "    model = MultiFetTransformer()\n",
    "    #, hidden_size=128,layers=8,dropout=0.2,features_out=128)\n",
    "# model.to(device)\n",
    "#     model = NeuralNetwork()\n",
    "#     model = StockIdEmbedding(number_of_stock_embedding_dimention=3)\n",
    "#     model\n",
    "    model.to(device)\n",
    "    optimizer_for_modes = {}\n",
    "    optimizer_scheduler_for_modes = {}\n",
    "    recent_lr = 0.001\n",
    "    feature_modes = ['logrett_xs','trade_volume_xs','trade_ordercount_xs','trade_money_turnover_per_order_xs',\n",
    "                             'logret1_xs',\n",
    "                             'book_price_spread1_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "                             'book_total_volume_xs','book_volume_imbalance_xs']\n",
    "#     feature_modes = model.get_feature_gen_train_modes()\n",
    "    done_epochs = -1\n",
    "    for modeidx, mode in enumerate(['train']):#model.get_feature_gen_train_modes()+['hidden_generator']+['hybrid']+['ultimate']):#+model.get_feature_gen_train_modes()+['hybrid']):#'train_stock_id_embedding','train']):#+['hidden_generator']*2 + feature_modes + ['hybrid']*2):\n",
    "        model.set_mode(mode)\n",
    "        \n",
    "#         batch_size\n",
    "        if mode == 'ultimate':\n",
    "            epochs = 100\n",
    "#             learning_rate = 1e-3\n",
    "#             batch_size = 1024\n",
    "        if mode in ['hybrid']:\n",
    "            epochs = 10\n",
    "#             learning_rate = 1e-3\n",
    "#             batch_size = 256\n",
    "#         if mode in model.get_feature_gen_train_modes()+['hidden_generator']:\n",
    "            \n",
    "#             epochs = 10\n",
    "#             learning_rate = 1e-3\n",
    "#             batch_size = 64\n",
    "            \n",
    "        print(f\"Current epochs: {epochs} LR : {learning_rate} batch_size: {batch_size}\")\n",
    "#             batch_size = 2\n",
    "#             learning_rate = 1e-5\n",
    "#             batch_size = 16\n",
    "#             learning_rate=1e-4\n",
    "        \n",
    "        \n",
    "#         print(model.parameters())\n",
    "#         input()\n",
    "#         continue\n",
    "        \n",
    "        if str(mode) not in optimizer_for_modes:\n",
    "#             optimizer_for_modes[str(mode)] = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "            optimizer_for_modes[str(mode)] = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "#             optimizer_scheduler_for_modes[str(mode)] = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_for_modes[str(mode)], factor=0.1, patience=10, threshold=0.0001)\n",
    "#             optimizer_for_modes[str(mode)] = torch.optim.RMSprop(model.parameters())\n",
    "#             if mode in model.get_feature_gen_train_modes():\n",
    "#                 optimizer_for_modes[str(mode)] = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        optimizer = optimizer_for_modes[str(mode)]\n",
    "#         optimizer_scheduler = optimizer_scheduler_for_modes[str(mode)]\n",
    "        \n",
    "        # TRAINING SETUP DONE\n",
    "\n",
    "        \n",
    "\n",
    "        data_ohlc_sample_len = 1 # 1 for each of open high low close\n",
    "        losses_train = []\n",
    "        step_num = 0\n",
    "        for t in range(epochs):\n",
    "            done_epochs += 1\n",
    "            \n",
    "            \n",
    "            print(f\"Epoch {done_epochs+1}\\n-------------------------------\")\n",
    "            print(\"----------\", STRATEGY_NAME_WITH_ATTRS, mode,\"----------\")\n",
    "\n",
    "            dataloader_train = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                shuffle=True, num_workers=0, pin_memory=True)\n",
    "            model.train()\n",
    "\n",
    "            for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "                step_num += 1\n",
    "                \n",
    "#                 y = scale_optiver_feature('book_realized_volatility',feature_y['target_realized_volatility']).to(device) # * realize_volatility_scale_factor \n",
    "                y = feature_y['target_realized_volatility'].to(device) * realize_volatility_scale_factor\n",
    "                pred = model(Feature_X)\n",
    "#                 pred.to(device)\n",
    "#                 print(pred)\n",
    "#                 input()\n",
    "                loss_orig = loss_fn_orig(y, pred)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss_orig.backward()\n",
    "                \n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 64**-0.5 * min(step_num**-0.5, step_num*40000**-1.5)\n",
    "                    recent_lr = param_group['lr']\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "                losses_train.append(loss_orig.item())\n",
    "\n",
    "                if (t*int(train_size/batch_size) + train_batch_idx + 1) % int(train_size/50/batch_size) == 0:\n",
    "\n",
    "                    # NOTE: real loss is same as upscaled normalized loss as it's percentage loss (rmspe)\n",
    "                    prediction_variety = np.std((pred/realize_volatility_scale_factor).reshape(-1).tolist()) * 100\n",
    "                    #NOTE: prediction variety is important as model sometimes predits a constant value! regardless of the input, then per batch variety is lowest(0 std dev)\n",
    "\n",
    "\n",
    "                    summary_writer.add_scalar(\"Prediction Variety\", prediction_variety, done_epochs*(train_size) + (train_batch_idx*batch_size))\n",
    "                    summary_writer.add_scalar(\"Training Loss\", np.mean(losses_train), done_epochs*(train_size) + (train_batch_idx*batch_size))\n",
    "\n",
    "                    print(\"train:\", np.mean(losses_train), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\", f\"recent lr: {recent_lr}\")\n",
    "                    losses_train = []\n",
    "            \n",
    "            dataloader_test = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                    shuffle=True, num_workers=0, pin_memory=True)\n",
    "            dataset_size = len(dataloader_test.dataset)\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            losses_test = []\n",
    "            for _, (Feature_X, feature_y) in enumerate(dataloader_test):\n",
    "                with torch.no_grad():\n",
    "#                     y = scale_optiver_feature('book_realized_volatility',feature_y['target_realized_volatility']).to(device) # * realize_volatility_scale_factor\n",
    "                    y = feature_y['target_realized_volatility'].to(device) * realize_volatility_scale_factor\n",
    "                    pred = model(Feature_X)\n",
    "                    loss = loss_fn_orig(y, pred)\n",
    "                    \n",
    "                    losses_test.append(loss.item())\n",
    "#             optimizer_scheduler.step(np.mean(losses_test))\n",
    "\n",
    "    #                 summary_writer.add_scalar(\"Epoch Training Loss\", np.mean(losses_train), (t+1)*train_size)\n",
    "            summary_writer.add_scalar(\"Test Loss\", np.mean(losses_test), done_epochs*(train_size) + (train_batch_idx*batch_size))\n",
    "            print(\"train:\", np.mean(losses_train), \"test:\", np.mean(losses_test), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\", f\"recent lr: {recent_lr}\")\n",
    "            losses_test = []\n",
    "            if (t+1)%30==0:\n",
    "#                 torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{loss:.4f}.pth\"))\n",
    "                # torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"13_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))\n",
    "                model_statedict = {'base':model.state_dict()}\n",
    "#                 for k,v in model.feature_gen_models.items():\n",
    "#                     model_statedict[k] = v.state_dict()\n",
    "\n",
    "                torch.save(model_statedict, os.path.join(MODEL_OUTPUT_DIRECTORY,f\"16_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93bc88db-cae6-4c72-847f-890e0a68d299",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiFetTransformer' object has no attribute 'feature_gen_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2056/226505782.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"13_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_statedict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'base'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_gen_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mmodel_statedict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m-> 1131\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultiFetTransformer' object has no attribute 'feature_gen_models'"
     ]
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"13_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))\n",
    "model_statedict = {'base':model.state_dict()}\n",
    "for k,v in model.feature_gen_models.items():\n",
    "    model_statedict[k] = v.state_dict()\n",
    "\n",
    "torch.save(model_statedict, os.path.join(MODEL_OUTPUT_DIRECTORY,f\"17_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c40a983-cebc-400b-9b8b-95c2e4874894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f978d-06d8-408d-9d32-8d0c971f2c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c865e507-c49e-401a-b561-c0c1cba9e80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>stock_id</th>\n",
       "      <th>70</th>\n",
       "      <th>75</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.811194</td>\n",
       "      <td>0.705222</td>\n",
       "      <td>0.579442</td>\n",
       "      <td>0.449084</td>\n",
       "      <td>0.813176</td>\n",
       "      <td>0.743353</td>\n",
       "      <td>0.573379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.744891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.589083</td>\n",
       "      <td>0.469144</td>\n",
       "      <td>0.732183</td>\n",
       "      <td>0.739383</td>\n",
       "      <td>0.533038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.808020</td>\n",
       "      <td>0.754178</td>\n",
       "      <td>0.580484</td>\n",
       "      <td>0.473728</td>\n",
       "      <td>0.749465</td>\n",
       "      <td>0.756734</td>\n",
       "      <td>0.525109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.760358</td>\n",
       "      <td>0.660425</td>\n",
       "      <td>0.516831</td>\n",
       "      <td>0.285894</td>\n",
       "      <td>0.710278</td>\n",
       "      <td>0.650910</td>\n",
       "      <td>0.364603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.770228</td>\n",
       "      <td>0.753502</td>\n",
       "      <td>0.611220</td>\n",
       "      <td>0.576678</td>\n",
       "      <td>0.745659</td>\n",
       "      <td>0.778682</td>\n",
       "      <td>0.663163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.576149</td>\n",
       "      <td>0.589083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474793</td>\n",
       "      <td>0.560188</td>\n",
       "      <td>0.614096</td>\n",
       "      <td>0.499395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.340020</td>\n",
       "      <td>0.469144</td>\n",
       "      <td>0.474793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.376075</td>\n",
       "      <td>0.555407</td>\n",
       "      <td>0.634929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.844946</td>\n",
       "      <td>0.732183</td>\n",
       "      <td>0.560188</td>\n",
       "      <td>0.376075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758198</td>\n",
       "      <td>0.506475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.765444</td>\n",
       "      <td>0.739383</td>\n",
       "      <td>0.614096</td>\n",
       "      <td>0.555407</td>\n",
       "      <td>0.758198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.622576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.754608</td>\n",
       "      <td>0.721407</td>\n",
       "      <td>0.589519</td>\n",
       "      <td>0.621860</td>\n",
       "      <td>0.706099</td>\n",
       "      <td>0.779024</td>\n",
       "      <td>0.685978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.870479</td>\n",
       "      <td>0.735199</td>\n",
       "      <td>0.600032</td>\n",
       "      <td>0.445861</td>\n",
       "      <td>0.825894</td>\n",
       "      <td>0.803625</td>\n",
       "      <td>0.532197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.740288</td>\n",
       "      <td>0.692165</td>\n",
       "      <td>0.533340</td>\n",
       "      <td>0.539652</td>\n",
       "      <td>0.685939</td>\n",
       "      <td>0.747160</td>\n",
       "      <td>0.595898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.820951</td>\n",
       "      <td>0.752925</td>\n",
       "      <td>0.626719</td>\n",
       "      <td>0.525911</td>\n",
       "      <td>0.791776</td>\n",
       "      <td>0.792502</td>\n",
       "      <td>0.716967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.816374</td>\n",
       "      <td>0.731109</td>\n",
       "      <td>0.570766</td>\n",
       "      <td>0.402027</td>\n",
       "      <td>0.779568</td>\n",
       "      <td>0.763643</td>\n",
       "      <td>0.542926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.745952</td>\n",
       "      <td>0.719873</td>\n",
       "      <td>0.629644</td>\n",
       "      <td>0.498651</td>\n",
       "      <td>0.741755</td>\n",
       "      <td>0.753600</td>\n",
       "      <td>0.632786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.763163</td>\n",
       "      <td>0.714092</td>\n",
       "      <td>0.562613</td>\n",
       "      <td>0.474837</td>\n",
       "      <td>0.735885</td>\n",
       "      <td>0.754644</td>\n",
       "      <td>0.622048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.911003</td>\n",
       "      <td>0.725142</td>\n",
       "      <td>0.549348</td>\n",
       "      <td>0.298335</td>\n",
       "      <td>0.832846</td>\n",
       "      <td>0.761384</td>\n",
       "      <td>0.454619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.854270</td>\n",
       "      <td>0.744462</td>\n",
       "      <td>0.602939</td>\n",
       "      <td>0.477151</td>\n",
       "      <td>0.841559</td>\n",
       "      <td>0.802465</td>\n",
       "      <td>0.571472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.880381</td>\n",
       "      <td>0.746706</td>\n",
       "      <td>0.591059</td>\n",
       "      <td>0.443440</td>\n",
       "      <td>0.810087</td>\n",
       "      <td>0.780505</td>\n",
       "      <td>0.619799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.781451</td>\n",
       "      <td>0.753629</td>\n",
       "      <td>0.601444</td>\n",
       "      <td>0.494994</td>\n",
       "      <td>0.720945</td>\n",
       "      <td>0.736164</td>\n",
       "      <td>0.565407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "stock_id        70        75        80        81        82        83        8 \n",
       "stock_id                                                                      \n",
       "74        0.811194  0.705222  0.579442  0.449084  0.813176  0.743353  0.573379\n",
       "75        0.744891  1.000000  0.589083  0.469144  0.732183  0.739383  0.533038\n",
       "76        0.808020  0.754178  0.580484  0.473728  0.749465  0.756734  0.525109\n",
       "77        0.760358  0.660425  0.516831  0.285894  0.710278  0.650910  0.364603\n",
       "78        0.770228  0.753502  0.611220  0.576678  0.745659  0.778682  0.663163\n",
       "80        0.576149  0.589083  1.000000  0.474793  0.560188  0.614096  0.499395\n",
       "81        0.340020  0.469144  0.474793  1.000000  0.376075  0.555407  0.634929\n",
       "82        0.844946  0.732183  0.560188  0.376075  1.000000  0.758198  0.506475\n",
       "83        0.765444  0.739383  0.614096  0.555407  0.758198  1.000000  0.622576\n",
       "84        0.754608  0.721407  0.589519  0.621860  0.706099  0.779024  0.685978\n",
       "85        0.870479  0.735199  0.600032  0.445861  0.825894  0.803625  0.532197\n",
       "86        0.740288  0.692165  0.533340  0.539652  0.685939  0.747160  0.595898\n",
       "87        0.820951  0.752925  0.626719  0.525911  0.791776  0.792502  0.716967\n",
       "88        0.816374  0.731109  0.570766  0.402027  0.779568  0.763643  0.542926\n",
       "89        0.745952  0.719873  0.629644  0.498651  0.741755  0.753600  0.632786\n",
       "90        0.763163  0.714092  0.562613  0.474837  0.735885  0.754644  0.622048\n",
       "93        0.911003  0.725142  0.549348  0.298335  0.832846  0.761384  0.454619\n",
       "94        0.854270  0.744462  0.602939  0.477151  0.841559  0.802465  0.571472\n",
       "95        0.880381  0.746706  0.591059  0.443440  0.810087  0.780505  0.619799\n",
       "96        0.781451  0.753629  0.601444  0.494994  0.720945  0.736164  0.565407"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p = dataset.main_df.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "corr[[70,75,80,81,82,83,8]][65:85]\n",
    "# corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "28b838c3-3aea-416a-8b20-cf2eacddb2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Learned ----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Default ---------------------\n",
      "------- set mode : hybrid -----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"----------- Learned ----------\")\n",
    "stock_ids = torch.tensor(dataset.main_df['stock_id'].unique())\n",
    "# print(stock_ids)\n",
    "\n",
    "embedding_predictor = model.hidden_generator_network.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':stock_ids})\n",
    "datapoints = pred.tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([75,70])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([76,78,82,85,87,88,94,95])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "embedding_predictor = model.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':stock_ids})\n",
    "datapoints = pred.tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([75,70])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([76,78,82,85,87,88,94,95])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()\n",
    "\n",
    "print(\"------------- Default ---------------------\")\n",
    "default_model = VolatilityBSModel().to(device)\n",
    "embedding_predictor = default_model.hidden_generator_network.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':stock_ids})\n",
    "datapoints = pred.tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([75,70])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([76,78,82,85,87,88,94,95])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "embedding_predictor = default_model.stock_id_embedding #.set_mode('stock_id_embedding')\n",
    "pred = embedding_predictor({'stock_id':stock_ids})\n",
    "datapoints = pred.tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([75,70])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "datapoints = embedding_predictor({'stock_id':torch.tensor([76,78,82,85,87,88,94,95])}).tolist()\n",
    "plt.scatter([x[0] for x in datapoints], [x[1] for x in datapoints])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "99ec365f-201b-44f7-826c-2017b8bbc55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "50cf1a35-5d02-43e8-b242-66d082ac237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a1f5760-8926-4064-8bf2-bddd62c80bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74227704-6e5f-41fd-b9a2-242d0b8e9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optiver_custom_collate_func(batch):\n",
    "    output_x = {}\n",
    "    for k,v in batch[0][0].items():\n",
    "        output_x[k] = []\n",
    "    \n",
    "    for x_dict in [x[0] for x in batch]:\n",
    "        for k,v in x_dict.items():\n",
    "            output_x[k].append(v)\n",
    "    \n",
    "    for k,v in batch[0][0].items():\n",
    "        if type(output_x[k][0]) != str:\n",
    "            output_x[k] = torch.stack(output_x[k])\n",
    "        \n",
    "    output_y = []\n",
    "    for y in [x[1] for x in batch]:\n",
    "        output_y.append(y)\n",
    "    output_y = torch.stack(output_y)\n",
    "    \n",
    "    return (output_x, output_y)\n",
    "#     input()\n",
    "#     print(batch)\n",
    "# #     return batch\n",
    "#     input()\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82320bc2-fca3-4696-8609-6034f371e2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "17cea68b-9cf2-4a6c-8fab-d0e6b8f7654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id\n",
      "['96-13771']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_id\n",
      "tensor([96.])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds_in_bucket_xs\n",
      "tensor([[  5.,  10.,  15.,  20.,  25.,  30.,  35.,  40.,  45.,  50.,  55.,  60.,\n",
      "          65.,  70.,  75.,  80.,  85.,  90.,  95., 100., 105., 110., 115., 120.,\n",
      "         125., 130., 135., 140., 145., 150., 155., 160., 165., 170., 175., 180.,\n",
      "         185., 190., 195., 200., 205., 210., 215., 220., 225., 230., 235., 240.,\n",
      "         245., 250., 255., 260., 265., 270., 275., 280., 285., 290., 295., 300.,\n",
      "         305., 310., 315., 320., 325., 330., 335., 340., 345., 350., 355., 360.,\n",
      "         365., 370., 375., 380., 385., 390., 395., 400., 405., 410., 415., 420.,\n",
      "         425., 430., 435., 440., 445., 450., 455., 460., 465., 470., 475., 480.,\n",
      "         485., 490., 495., 500., 505., 510., 515., 520., 525., 530., 535., 540.,\n",
      "         545., 550., 555., 560., 565., 570., 575., 580., 585., 590., 595., 600.]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logrett_xs\n",
      "tensor([[-0.2583, -0.2583, -0.2583, -0.2583, -0.2565, -0.2560, -0.2566, -1.2919,\n",
      "         -0.0621, -0.0615, -0.0621, -0.0621, -0.2757, -0.2751,  2.5545, -0.3759,\n",
      "          1.5375, -1.2732, -1.4517, -0.6660, -0.6666, -0.6661, -0.0943, -0.8810,\n",
      "         -0.8745, -3.8643, -2.0211, -0.0137, -1.3876, -0.1028, -0.2772, -0.2766,\n",
      "         -2.2825, -0.0269,  1.6510, -1.3265,  2.1354,  2.1350,  0.5107,  0.5112,\n",
      "         -0.6612, -0.1995, -0.1995, -2.8131, -2.8139, -1.4733, -1.5446,  0.1518,\n",
      "          1.3928, -0.8829, -0.2122, -0.2122, -0.4472, -0.4466, -0.4472,  1.6626,\n",
      "          1.2654,  1.2647,  1.1127,  1.5446,  1.5443,  2.4143,  0.7716,  0.7715,\n",
      "         -0.4849, -0.8379, -1.7267, -0.9193, -0.5514, -0.5508, -0.2246, -0.2252,\n",
      "         -0.2252, -0.2247,  0.4583,  0.6620,  1.0186,  1.1863, -0.5597,  0.2849,\n",
      "          0.2855,  0.2849, -0.0747, -0.8219, -0.8220, -0.0287, -0.0287, -0.0287,\n",
      "          2.1707, -0.2222, -0.2222, -0.2222, -0.7705, -0.7706, -0.7707, -0.7707,\n",
      "          0.2205,  0.6614,  0.6619,  0.8823, -0.8029, -0.8023, -0.8030, -2.0030,\n",
      "         -0.0884,  0.2498,  0.2504,  0.2498, -1.9602, -2.1202, -5.4583, -0.6376,\n",
      "         -0.6383,  0.7615,  0.7620,  0.0221, -0.6903, -0.6897, -0.6903, -0.6904]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_volume_xs\n",
      "tensor([[5.3033, 0.0000, 0.0000, 5.3375, 0.0000, 0.0000, 5.3230, 5.2095, 0.0000,\n",
      "         0.0000, 0.0000, 2.1972, 0.0000, 5.1417, 4.7875, 6.5191, 4.1431, 4.8203,\n",
      "         4.6634, 0.0000, 0.0000, 6.4630, 3.0445, 2.8332, 6.4677, 4.8363, 4.5951,\n",
      "         6.3648, 4.7958, 5.5175, 0.0000, 5.3566, 6.4568, 6.1203, 5.9081, 3.2581,\n",
      "         0.0000, 6.5568, 0.0000, 0.6931, 2.1972, 0.0000, 4.6634, 0.0000, 5.7268,\n",
      "         4.3175, 1.3863, 4.9836, 0.6931, 0.6931, 0.0000, 6.2916, 0.0000, 0.0000,\n",
      "         4.6151, 6.0113, 0.0000, 0.6931, 5.3033, 0.0000, 6.1485, 4.6250, 0.0000,\n",
      "         4.6151, 6.2226, 5.3230, 2.9444, 4.6151, 0.0000, 3.0445, 0.0000, 0.0000,\n",
      "         0.0000, 5.7746, 3.6376, 0.6931, 6.0331, 1.6094, 5.2883, 0.0000, 0.0000,\n",
      "         5.7170, 4.7274, 0.0000, 4.9558, 0.0000, 0.0000, 4.9345, 4.6250, 0.0000,\n",
      "         0.0000, 5.3327, 0.0000, 0.0000, 0.0000, 1.0986, 0.6931, 0.0000, 1.0986,\n",
      "         0.6931, 0.0000, 0.0000, 5.6312, 4.6250, 5.7557, 0.0000, 0.0000, 1.0986,\n",
      "         5.7203, 4.7274, 6.2710, 0.0000, 0.6931, 0.0000, 4.9488, 4.8283, 0.0000,\n",
      "         0.0000, 0.0000, 4.6250]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_ordercount_xs\n",
      "tensor([[1.9459, 0.0000, 0.0000, 2.1972, 0.0000, 0.0000, 1.7918, 2.0794, 0.0000,\n",
      "         0.0000, 0.0000, 1.9459, 0.0000, 1.0986, 2.0794, 2.7081, 1.3863, 1.3863,\n",
      "         1.3863, 0.0000, 0.0000, 2.1972, 0.6931, 1.0986, 2.0794, 1.0986, 1.3863,\n",
      "         2.6391, 1.6094, 2.0794, 0.0000, 2.0794, 2.7081, 2.5649, 2.3979, 0.6931,\n",
      "         0.0000, 2.6391, 0.0000, 0.6931, 1.3863, 0.0000, 1.0986, 0.0000, 2.0794,\n",
      "         1.3863, 0.6931, 1.3863, 0.6931, 0.6931, 0.0000, 2.4849, 0.0000, 0.0000,\n",
      "         1.0986, 2.5649, 0.0000, 0.6931, 1.3863, 0.0000, 2.9444, 1.0986, 0.0000,\n",
      "         1.0986, 2.3979, 1.6094, 1.6094, 1.3863, 0.0000, 0.6931, 0.0000, 0.0000,\n",
      "         0.0000, 2.3979, 1.0986, 0.6931, 2.7081, 0.6931, 1.9459, 0.0000, 0.0000,\n",
      "         2.0794, 1.3863, 0.0000, 1.3863, 0.0000, 0.0000, 2.4849, 1.3863, 0.0000,\n",
      "         0.0000, 1.7918, 0.0000, 0.0000, 0.0000, 1.0986, 0.6931, 0.0000, 1.0986,\n",
      "         0.6931, 0.0000, 0.0000, 1.7918, 1.3863, 2.6391, 0.0000, 0.0000, 1.0986,\n",
      "         2.6391, 1.9459, 2.4849, 0.0000, 0.6931, 0.0000, 2.6391, 1.7918, 0.0000,\n",
      "         0.0000, 0.0000, 1.0986]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_money_turnover_xs\n",
      "tensor([[5.3024, 0.0000, 0.0000, 5.3366, 0.0000, 0.0000, 5.3220, 5.2083, 0.0000,\n",
      "         0.0000, 0.0000, 2.1962, 0.0000, 5.1404, 4.7865, 6.5181, 4.1423, 4.8193,\n",
      "         4.6623, 0.0000, 0.0000, 6.4617, 3.0432, 2.8319, 6.4662, 4.8344, 4.5930,\n",
      "         6.3626, 4.7936, 5.5152, 0.0000, 5.3543, 6.4542, 6.1178, 5.9057, 3.2557,\n",
      "         0.0000, 6.5547, 0.0000, 0.6922, 2.1954, 0.0000, 4.6614, 0.0000, 5.7242,\n",
      "         4.3147, 1.3841, 4.9807, 0.6917, 0.6917, 0.0000, 6.2886, 0.0000, 0.0000,\n",
      "         4.6121, 6.0084, 0.0000, 0.6918, 5.3008, 0.0000, 6.1462, 4.6230, 0.0000,\n",
      "         4.6133, 6.2206, 5.3210, 2.9424, 4.6129, 0.0000, 3.0423, 0.0000, 0.0000,\n",
      "         0.0000, 5.7721, 3.6353, 0.6920, 6.0309, 1.6078, 5.2861, 0.0000, 0.0000,\n",
      "         5.7150, 4.7253, 0.0000, 4.9536, 0.0000, 0.0000, 4.9322, 4.6229, 0.0000,\n",
      "         0.0000, 5.3307, 0.0000, 0.0000, 0.0000, 1.0970, 0.6920, 0.0000, 1.0971,\n",
      "         0.6921, 0.0000, 0.0000, 5.6288, 4.6224, 5.7532, 0.0000, 0.0000, 1.0969,\n",
      "         5.7175, 4.7245, 6.2677, 0.0000, 0.6913, 0.0000, 4.9453, 4.8249, 0.0000,\n",
      "         0.0000, 0.0000, 4.6213]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_money_turnover_per_order_xs\n",
      "tensor([[3.5353, 0.0000, 0.0000, 3.2903, 0.0000, 0.0000, 3.7319, 3.7785, 0.0000,\n",
      "         0.0000, 0.0000, 1.2229, 0.0000, 4.4531, 2.8894, 4.6450, 3.0749, 3.7367,\n",
      "         3.9878, 0.0000, 0.0000, 5.2162, 3.0432, 2.1959, 4.5295, 4.8344, 4.5930,\n",
      "         4.7957, 3.9296, 4.8843, 0.0000, 3.4363, 4.5166, 4.8029, 4.8006, 3.2557,\n",
      "         0.0000, 5.0585, 0.0000, 0.6922, 1.2978, 0.0000, 3.9776, 0.0000, 3.7977,\n",
      "         3.2425, 1.3841, 3.8957, 0.6917, 0.6917, 0.0000, 3.9091, 0.0000, 0.0000,\n",
      "         3.9288, 3.5501, 0.0000, 0.6918, 5.0114, 0.0000, 3.2916, 3.9396, 0.0000,\n",
      "         3.9300, 4.0547, 4.9082, 1.7030, 3.5339, 0.0000, 3.0423, 0.0000, 0.0000,\n",
      "         0.0000, 4.3371, 2.9681, 0.6920, 4.2836, 1.6078, 3.5194, 0.0000, 0.0000,\n",
      "         3.7886, 3.6443, 0.0000, 3.8690, 0.0000, 0.0000, 3.0954, 3.9492, 0.0000,\n",
      "         0.0000, 4.0278, 0.0000, 0.0000, 0.0000, 0.6919, 0.6920, 0.0000, 0.6920,\n",
      "         0.6921, 0.0000, 0.0000, 4.0336, 3.5432, 4.0198, 0.0000, 0.0000, 0.6919,\n",
      "         3.4135, 3.4472, 4.2308, 0.0000, 0.6913, 0.0000, 4.6442, 3.2470, 0.0000,\n",
      "         0.0000, 0.0000, 4.6213]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logret1_xs\n",
      "tensor([[-7.7680e-01, -7.7680e-01, -6.8623e-02,  6.7328e-01, -4.8532e-02,\n",
      "          3.7172e-01,  5.6857e-02, -2.5074e-01, -9.9375e-01,  1.7405e-01,\n",
      "          6.8433e-02,  7.0038e-01,  1.2492e-01, -1.3495e+00,  1.0666e+00,\n",
      "          5.1070e-01, -4.1751e-02, -6.4015e-03, -6.7136e-01, -2.6296e-02,\n",
      "          3.9817e-02, -9.8889e-01, -8.4128e-01, -1.1107e+00, -2.2903e+00,\n",
      "         -1.5481e+00, -1.5099e+00, -9.2775e-01, -7.4993e-01, -1.3067e+00,\n",
      "          1.8113e-01, -3.6812e-01, -2.8201e+00,  1.3158e-01,  1.6369e+00,\n",
      "          1.9151e-01,  3.7371e-01,  1.7664e+00,  1.1575e+00, -1.1125e-01,\n",
      "          7.9329e-02,  8.9109e-01, -3.4531e+00, -2.2108e+00, -1.5161e+00,\n",
      "         -5.4590e-01, -7.1532e-01,  1.3627e-01, -5.4038e-01,  9.0180e-01,\n",
      "         -7.0203e-01, -1.4106e+00, -1.5298e-01, -1.5299e-01, -1.0891e-03,\n",
      "          2.9099e+00, -2.2905e-01, -4.5718e-01,  2.1538e+00,  1.5167e-01,\n",
      "          4.0605e+00,  1.8936e-01,  5.3510e-01,  1.5238e+00,  4.5908e-01,\n",
      "         -1.1990e+00, -7.9428e-01, -1.3792e+00,  2.2514e-01, -9.8336e-01,\n",
      "          8.9667e-01, -5.0048e-01, -5.0050e-01, -9.5374e-01,  6.0001e-02,\n",
      "          2.2383e-01,  3.2025e-01,  1.1402e+00,  5.8908e-01, -1.5750e+00,\n",
      "         -3.1343e-02,  1.3332e+00,  5.6344e-01,  9.6391e-01,  3.1342e-01,\n",
      "         -3.0081e+00, -5.3374e-01, -6.4902e-01,  1.7861e+00, -1.9742e-01,\n",
      "          1.2865e+00, -3.0998e-01, -1.3622e+00,  5.5502e-01,  0.0000e+00,\n",
      "         -2.9586e+00,  1.2638e+00,  7.6171e-01,  6.1231e-01,  9.0020e-01,\n",
      "         -1.0147e-03, -1.3182e+00, -1.3183e+00, -7.5227e-01, -1.5973e+00,\n",
      "         -8.1991e-01,  1.1986e+00,  1.4711e-01, -2.0264e+00, -1.3291e+00,\n",
      "         -5.3050e+00, -2.0951e+00,  5.3387e-01, -7.8693e-02,  5.3789e-01,\n",
      "          1.9783e+00, -2.8291e+00,  1.2949e+00, -1.7899e-01, -4.8892e-02]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logret2_xs\n",
      "tensor([[ 0.9859,  0.9859, -0.1455, -1.2756,  1.5009, -0.0909, -0.0406, -0.5837,\n",
      "         -0.5796, -0.9822,  0.3893,  0.4608, -0.0628, -0.1517,  0.0359,  0.9919,\n",
      "         -0.5060,  0.0652, -1.9585, -0.5432,  0.4822, -0.3305, -0.2238,  0.0935,\n",
      "         -2.9143, -2.2891, -1.2099, -1.0210, -2.9866,  0.9696,  0.1230, -1.1505,\n",
      "         -1.7135,  0.2416,  1.8676, -0.2321,  0.1684,  1.8445, -0.0817, -1.3762,\n",
      "          1.8866,  1.9385, -2.1587, -1.1067, -4.4063, -0.5619, -0.2444, -0.4791,\n",
      "         -0.6453, -0.6869,  1.8901, -2.1611, -0.0178, -0.0178, -0.3498,  0.6256,\n",
      "          0.5493,  2.6553,  1.5744, -0.6500,  2.3321,  2.1582,  1.2279,  0.2242,\n",
      "         -0.4004, -0.2631, -2.0794, -0.5214,  0.0000,  1.0671, -0.2156, -0.0635,\n",
      "         -0.0635, -3.0902,  1.6110,  0.0915, -0.6223,  1.2058, -0.1832,  0.1907,\n",
      "         -0.1133,  1.5597,  0.1667, -1.8668, -0.6013, -0.3198,  1.0539, -1.2939,\n",
      "          2.3039,  1.6961,  0.4793, -3.0270, -1.4520,  1.6252, -0.0483, -1.6339,\n",
      "          0.6728,  1.1467, -0.3846, -0.1781,  1.4351, -0.7536, -0.7537, -1.5527,\n",
      "         -2.6237,  0.2717, -0.5195,  0.2717,  0.9765, -3.4961, -3.2493, -3.2187,\n",
      "         -0.4303,  0.1247,  1.2807,  0.0454, -2.5902,  1.0193,  0.6861, -0.4485]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_directional_volume1_xs\n",
      "tensor([[   4.2000,   70.0000,  106.3333,   34.7500,  245.0000,  212.0000,\n",
      "          303.2000,  -21.7500,  -24.3333,  -51.5000,  -70.2000,  -80.0000,\n",
      "         -127.0000,   -1.0000,   12.2500,   61.5000,   82.0000,   57.6667,\n",
      "         -217.0000, -191.0000, -199.0000, -127.8000, -184.5000, -257.2500,\n",
      "          -76.4000, -102.6000,  -63.4000,  -81.2500, -136.7500,   20.0000,\n",
      "          -77.7500, -219.5000,  -30.7500,   14.0000,   -4.5000,  -96.0000,\n",
      "           -4.0000,   64.7500,  -74.0000,  -94.6000,  -91.0000,  -99.0000,\n",
      "          -40.0000,   97.0000,   81.7500, -100.0000, -100.6667, -118.3333,\n",
      "          -33.0000,  -98.2500,  -50.0000, -121.7500,    0.0000,  -99.0000,\n",
      "          -99.0000,  -98.6000, -152.5000,    0.0000,   35.6000,  148.5000,\n",
      "          -74.5000,   76.7500,  157.0000, -159.2000, -155.4000,  -12.7500,\n",
      "          -58.2000,  -11.0000, -111.0000,  -21.6000, -104.3333,    0.0000,\n",
      "          -28.0000,  -46.2500,  -99.0000,  -98.5000,   26.7500,   -1.5000,\n",
      "          -19.2500,   88.7500,   80.5000,  111.4000,   -8.2500,  -66.0000,\n",
      "          -85.5000,  -19.6000,   -2.5000,   75.0000,  143.5000,  176.0000,\n",
      "          100.0000,  -32.2000,    2.0000,    7.0000,    7.0000,   10.0000,\n",
      "          -12.5000,  -23.0000,  -98.5000, -188.0000,  -99.0000,    0.0000,\n",
      "          -80.0000,  -24.7500,   -3.0000,    0.8000,    1.0000,    0.6667,\n",
      "           21.0000,  -49.5000,  -33.0000,  -44.0000, -137.4000, -115.0000,\n",
      "          -28.7500,  -14.6667,   11.3333,  -23.3333,  -93.2500, -123.7500]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_directional_volume2_xs\n",
      "tensor([[  17.8000,  -99.0000,  -99.0000,   26.2500,  -41.7500,    0.0000,\n",
      "           34.0000,    5.7500,  -95.6667,  -46.5000,  -89.8000,  -77.0000,\n",
      "          -44.6000,  -24.0000,  -44.0000,   51.5000,   33.0000,  -10.6667,\n",
      "           18.0000,   55.0000,   16.0000,   -8.0000, -165.7500, -162.7500,\n",
      "          -50.6000,  -72.4000, -159.6000,  -76.2500,   56.2500,   -7.5000,\n",
      "          -50.0000,   68.0000,  -71.2500,  -93.5000, -121.2500,  -68.4000,\n",
      "          -77.0000,   23.5000,  -19.5000,   78.2000,   15.6000,  -78.0000,\n",
      "         -126.6667, -101.0000,   50.5000,  -15.7500,  -48.3333,  -87.6667,\n",
      "           57.6667,  244.5000, -154.0000,  -52.7500,    0.0000,  -99.0000,\n",
      "          -59.0000,   61.6000,   38.0000, -100.0000,  -95.6000,   47.5000,\n",
      "           37.2500,  101.2500,  -39.0000,  -51.2000,    1.8000,   13.2500,\n",
      "           26.4000,    0.0000,    0.0000, -145.4000,  -91.3333,    0.0000,\n",
      "          -95.0000,   14.2500,  -60.0000,  -59.5000,  113.5000,  -10.2500,\n",
      "          105.7500,   -4.0000,   50.7500,   -2.4000,  -19.7500,  112.0000,\n",
      "          164.7500,   71.0000,   -0.5000,   55.2000,    3.2500,  -11.7500,\n",
      "          -20.0000,   78.8000,  178.0000,   -3.0000,   -2.0000,    0.0000,\n",
      "          -10.5000,  -22.0000,  -12.5000,  -13.5000, -157.0000,    0.0000,\n",
      "         -276.0000, -144.2500,   32.2500,    1.6000,   50.0000,    6.6667,\n",
      "          -96.4000,  -43.7500, -127.2500, -120.8000,  -86.8000,  -89.3333,\n",
      "          -79.7500,   15.3333,   35.3333,  -50.0000,  -13.7500,   -7.7500]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_price_spread1_xs\n",
      "tensor([[0.2467, 0.1835, 0.1762, 0.1817, 0.2643, 0.2423, 0.2291, 0.2313, 0.2644,\n",
      "         0.2313, 0.2115, 0.2159, 0.1939, 0.2644, 0.1982, 0.2863, 0.1982, 0.1909,\n",
      "         0.2129, 0.2203, 0.1982, 0.2379, 0.2754, 0.1598, 0.2645, 0.3086, 0.2337,\n",
      "         0.1764, 0.2701, 0.2371, 0.1433, 0.1709, 0.2592, 0.1930, 0.2812, 0.4852,\n",
      "         0.2315, 0.3583, 0.3032, 0.2161, 0.1852, 0.2095, 0.4852, 0.3639, 0.3033,\n",
      "         0.2869, 0.2648, 0.2059, 0.1839, 0.1986, 0.1986, 0.2428, 0.2042, 0.1655,\n",
      "         0.1765, 0.3839, 0.2923, 0.3089, 0.2912, 0.1985, 0.2426, 0.2150, 0.2425,\n",
      "         0.2160, 0.2028, 0.2976, 0.1940, 0.3308, 0.3308, 0.2823, 0.2646, 0.2536,\n",
      "         0.2426, 0.2812, 0.2647, 0.1765, 0.1599, 0.1102, 0.3583, 0.2646, 0.2316,\n",
      "         0.2249, 0.4355, 0.3528, 0.3307, 0.3573, 0.2095, 0.2250, 0.2316, 0.3087,\n",
      "         0.2205, 0.2602, 0.3307, 0.2867, 0.2867, 0.2206, 0.2206, 0.2867, 0.2867,\n",
      "         0.3308, 0.2425, 0.2536, 0.2647, 0.2702, 0.2702, 0.2603, 0.0662, 0.0882,\n",
      "         0.2162, 0.2152, 0.3423, 0.3048, 0.2739, 0.2576, 0.2484, 0.3165, 0.3239,\n",
      "         0.3092, 0.2485, 0.2264]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_price_spread2_xs\n",
      "tensor([[0.3083, 0.2569, 0.2423, 0.2863, 0.4240, 0.4185, 0.3788, 0.3689, 0.3672,\n",
      "         0.2864, 0.2732, 0.2908, 0.3172, 0.3084, 0.2808, 0.3359, 0.3480, 0.3230,\n",
      "         0.3451, 0.3304, 0.3084, 0.3305, 0.3690, 0.3250, 0.3482, 0.3880, 0.2954,\n",
      "         0.2977, 0.4190, 0.3749, 0.2977, 0.3088, 0.3584, 0.3253, 0.4631, 0.6175,\n",
      "         0.4742, 0.5071, 0.4631, 0.4145, 0.4101, 0.3748, 0.6028, 0.5072, 0.4964,\n",
      "         0.4523, 0.3751, 0.3163, 0.2795, 0.2538, 0.2647, 0.2979, 0.2704, 0.2428,\n",
      "         0.2560, 0.4457, 0.4192, 0.4853, 0.5559, 0.3639, 0.3473, 0.3693, 0.4079,\n",
      "         0.3439, 0.2998, 0.3858, 0.3704, 0.5292, 0.5292, 0.3837, 0.3528, 0.3418,\n",
      "         0.3308, 0.5128, 0.3419, 0.2647, 0.2812, 0.3252, 0.5568, 0.3749, 0.2977,\n",
      "         0.3043, 0.5181, 0.4410, 0.4576, 0.4367, 0.2702, 0.3484, 0.3969, 0.4355,\n",
      "         0.3748, 0.4587, 0.4852, 0.3970, 0.3970, 0.3970, 0.3529, 0.3970, 0.3970,\n",
      "         0.4631, 0.3749, 0.4301, 0.4853, 0.4026, 0.3971, 0.3618, 0.2868, 0.2941,\n",
      "         0.4192, 0.3531, 0.5354, 0.3755, 0.3357, 0.3018, 0.3698, 0.5152, 0.5889,\n",
      "         0.4417, 0.3147, 0.2871]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_bid_spread_xs\n",
      "tensor([[0.3964, 0.4404, 0.4404, 0.3303, 0.2752, 0.4404, 0.4404, 0.5507, 0.4405,\n",
      "         0.3304, 0.3969, 0.5289, 0.7491, 0.2202, 0.3306, 0.2206, 1.0133, 1.1015,\n",
      "         0.8815, 0.8815, 0.8815, 0.2203, 0.5510, 0.7161, 0.2203, 0.5292, 0.2204,\n",
      "         0.2204, 1.1580, 0.9374, 0.4414, 0.7718, 0.2205, 0.2757, 1.0479, 0.6619,\n",
      "         1.9852, 0.3310, 0.9923, 1.1028, 0.7498, 0.5510, 0.5145, 0.9926, 1.1034,\n",
      "         0.9933, 0.8093, 0.5885, 0.4412, 0.3309, 0.4412, 0.2758, 0.4138, 0.5519,\n",
      "         0.5742, 0.3976, 0.6624, 0.2206, 1.6769, 0.6618, 0.6062, 0.8820, 0.9924,\n",
      "         0.6174, 0.5732, 0.6612, 1.2788, 0.2205, 0.2205, 0.3969, 0.6614, 0.6614,\n",
      "         0.6614, 1.0478, 0.3308, 0.5512, 0.6614, 1.0477, 0.6066, 0.2205, 0.2205,\n",
      "         0.5732, 0.6064, 0.6617, 0.7722, 0.4854, 0.2211, 0.6616, 0.8821, 0.6062,\n",
      "         0.8823, 0.9704, 0.8824, 0.8824, 0.8824, 0.2205, 0.2205, 0.2205, 0.4416,\n",
      "         0.6615, 0.6620, 0.7724, 0.8827, 0.7172, 0.8826, 0.2650, 1.5443, 1.5443,\n",
      "         0.4854, 0.9381, 0.6624, 0.2208, 0.2208, 0.2208, 0.7176, 1.6193, 0.9574,\n",
      "         1.1046, 0.4415, 0.3864]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_ask_spread_xs\n",
      "tensor([[0.2201, 0.2937, 0.2201, 0.7155, 1.3213, 1.3213, 1.0569, 0.8260, 0.5873,\n",
      "         0.2202, 0.2202, 0.2202, 0.4847, 0.2202, 0.4958, 0.2752, 0.4844, 0.2203,\n",
      "         0.4405, 0.2202, 0.2202, 0.7049, 0.3855, 0.9361, 0.6170, 0.2644, 0.3966,\n",
      "         0.9920, 0.3306, 0.4410, 1.1027, 0.6063, 0.7722, 1.0477, 0.7716, 0.6612,\n",
      "         0.4412, 1.1571, 0.6063, 0.8821, 1.4989, 1.1017, 0.6612, 0.4408, 0.8271,\n",
      "         0.6616, 0.2944, 0.5150, 0.5150, 0.2211, 0.2205, 0.2757, 0.2482, 0.2206,\n",
      "         0.2206, 0.2205, 0.6066, 1.5440, 0.9702, 0.9923, 0.4408, 0.6612, 0.6616,\n",
      "         0.6614, 0.3967, 0.2205, 0.4848, 1.7637, 1.7637, 0.6173, 0.2204, 0.2204,\n",
      "         0.2204, 1.2680, 0.4409, 0.3307, 0.5515, 1.1026, 1.3777, 0.8820, 0.4409,\n",
      "         0.2204, 0.2203, 0.2204, 0.4960, 0.3088, 0.3857, 0.5732, 0.7713, 0.6611,\n",
      "         0.6611, 1.0142, 0.6617, 0.2204, 0.2204, 1.5439, 1.1028, 0.8823, 0.6616,\n",
      "         0.6618, 0.6618, 0.9925, 1.3233, 0.6068, 0.3860, 0.7503, 0.6621, 0.5149,\n",
      "         1.5444, 0.4411, 1.2692, 0.4860, 0.3977, 0.2211, 0.4967, 0.3677, 1.6927,\n",
      "         0.2207, 0.2207, 0.2207]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_total_volume_xs\n",
      "tensor([[7.0309, 6.2653, 6.4536, 6.9670, 7.5380, 6.1377, 7.9356, 6.8617, 5.9738,\n",
      "         6.4877, 6.7581, 6.9295, 7.1538, 4.0775, 6.9866, 8.0020, 8.0449, 7.4685,\n",
      "         6.9315, 6.6958, 6.8459, 7.1808, 7.6401, 7.7558, 6.8200, 6.8896, 7.6658,\n",
      "         6.9575, 7.2086, 7.2800, 7.4512, 7.1982, 7.6113, 7.3402, 7.2034, 7.4343,\n",
      "         5.5568, 7.4372, 6.7811, 7.0379, 7.0527, 6.1026, 6.8512, 6.1841, 6.9866,\n",
      "         6.2577, 6.9660, 6.9949, 7.1025, 7.5486, 6.0426, 7.0148, 0.0000, 6.0039,\n",
      "         7.0992, 7.2485, 7.2086, 6.2166, 7.6192, 6.4877, 7.1808, 7.4691, 6.8035,\n",
      "         7.3746, 7.6540, 6.5028, 6.6412, 4.0943, 5.0752, 6.9508, 6.5073, 0.0000,\n",
      "         6.3333, 6.6053, 5.7900, 5.8021, 7.5262, 6.8669, 7.0344, 6.6067, 6.8565,\n",
      "         7.4110, 7.0344, 6.7811, 7.0493, 6.3491, 5.3423, 7.0951, 6.8669, 6.8835,\n",
      "         5.7961, 6.9037, 5.4337, 5.4765, 5.4806, 4.7958, 4.6151, 3.9512, 5.4510,\n",
      "         6.1985, 5.7071, 0.0000, 6.1026, 7.0758, 6.1985, 5.6937, 4.7095, 3.5553,\n",
      "         7.0510, 6.7238, 6.7499, 6.7923, 7.1884, 6.5876, 6.9670, 6.7105, 6.3404,\n",
      "         6.2672, 6.3784, 6.6107]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_volume_imbalance_xs\n",
      "tensor([[4.6653, 3.4012, 2.1203, 4.1271, 5.3193, 5.3613, 5.8236, 5.0783, 4.7958,\n",
      "         4.5951, 5.0814, 5.0626, 5.1510, 3.2581, 3.8011, 4.9090, 5.0999, 4.1795,\n",
      "         5.2983, 4.9200, 5.2149, 4.9185, 5.8615, 6.0426, 4.8520, 5.1705, 5.4116,\n",
      "         5.2730, 4.4886, 4.6492, 4.8579, 5.1090, 4.6347, 5.0073, 4.8422, 5.1084,\n",
      "         4.4067, 4.9541, 4.5747, 2.8565, 4.3360, 5.1818, 5.1259, 4.6052, 4.8922,\n",
      "         4.7600, 5.0106, 5.3327, 4.7391, 4.9921, 5.3230, 5.1676, 0.0000, 5.2933,\n",
      "         5.0739, 3.6376, 4.7493, 4.6151, 4.9459, 5.2832, 4.0818, 5.1874, 4.7791,\n",
      "         5.3538, 5.0408, 3.8607, 4.2370, 2.4849, 4.7185, 5.1240, 5.2815, 0.0000,\n",
      "         4.8203, 3.9120, 5.0752, 5.0689, 4.9505, 3.3759, 4.8363, 4.4514, 4.8847,\n",
      "         4.7005, 3.4177, 4.1217, 4.5191, 4.1141, 3.5115, 4.8828, 4.9955, 5.1075,\n",
      "         4.3944, 4.7808, 5.1985, 1.6094, 1.7918, 2.4849, 3.1781, 3.8286, 4.7185,\n",
      "         5.3107, 5.5491, 0.0000, 5.8777, 5.1417, 3.4095, 3.8330, 3.9512, 2.1203,\n",
      "         4.4864, 4.5512, 5.0830, 5.1108, 5.4170, 5.3246, 4.8714, 2.1972, 4.5609,\n",
      "         4.3086, 4.6821, 4.8866]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_money_turnover1_xs\n",
      "tensor([[2.1963, 1.9449, 1.9449, 4.6624, 5.5824, 4.6141, 5.3171, 2.3016, 1.3854,\n",
      "         4.3808, 1.7908, 1.7909, 1.7909, 1.0978, 3.3663, 5.7858, 4.6624, 4.6434,\n",
      "         4.1261, 4.1421, 1.3855, 2.0784, 5.3458, 5.7607, 4.7690, 1.9444, 4.6520,\n",
      "         1.7901, 5.4909, 6.0569, 1.6077, 1.7899, 6.0472, 5.3156, 5.1156, 5.2235,\n",
      "         1.0971, 5.5032, 4.6423, 3.3303, 3.8267, 1.0973, 4.6323, 3.2165, 5.1621,\n",
      "         1.6073, 5.3153, 1.3842, 1.3842, 1.7895, 1.0968, 4.6414, 0.0000, 1.0967,\n",
      "         1.7893, 4.7069, 5.3105, 4.6124, 5.4137, 4.6610, 5.9969, 4.7938, 5.3499,\n",
      "         1.7902, 3.2942, 4.9180, 1.7901, 0.6921, 0.6921, 2.3006, 2.7706, 0.0000,\n",
      "         4.8419, 1.6076, 1.0971, 1.3846, 5.9917, 5.9767, 5.1337, 4.5304, 4.5304,\n",
      "         5.6038, 6.0190, 4.6519, 1.6079, 1.9440, 3.5532, 1.9439, 4.8421, 4.5725,\n",
      "         4.6131, 2.3960, 0.6920, 4.5517, 4.5517, 1.3844, 1.0970, 1.0971, 1.3846,\n",
      "         1.0972, 0.6921, 0.0000, 3.0888, 5.4091, 4.2879, 1.7895, 1.0969, 1.3844,\n",
      "         4.7423, 1.6071, 3.9855, 2.6357, 2.6357, 3.2154, 4.6600, 5.5181, 3.8676,\n",
      "         1.3837, 3.3288, 1.6066]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0023]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id\n",
      "['9-633']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_id\n",
      "tensor([9.])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds_in_bucket_xs\n",
      "tensor([[  5.,  10.,  15.,  20.,  25.,  30.,  35.,  40.,  45.,  50.,  55.,  60.,\n",
      "          65.,  70.,  75.,  80.,  85.,  90.,  95., 100., 105., 110., 115., 120.,\n",
      "         125., 130., 135., 140., 145., 150., 155., 160., 165., 170., 175., 180.,\n",
      "         185., 190., 195., 200., 205., 210., 215., 220., 225., 230., 235., 240.,\n",
      "         245., 250., 255., 260., 265., 270., 275., 280., 285., 290., 295., 300.,\n",
      "         305., 310., 315., 320., 325., 330., 335., 340., 345., 350., 355., 360.,\n",
      "         365., 370., 375., 380., 385., 390., 395., 400., 405., 410., 415., 420.,\n",
      "         425., 430., 435., 440., 445., 450., 455., 460., 465., 470., 475., 480.,\n",
      "         485., 490., 495., 500., 505., 510., 515., 520., 525., 530., 535., 540.,\n",
      "         545., 550., 555., 560., 565., 570., 575., 580., 585., 590., 595., 600.]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15084/2440942144.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         )\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "\n",
    "    \n",
    "dataloader_train = DataLoader(dataset, batch_size=1,\n",
    "                                shuffle=True, num_workers=0, pin_memory=False)#, collate_fn=optiver_custom_collate_func)\n",
    "\n",
    "\n",
    "#  encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "# >>> src = torch.rand(10, 32, 512)\n",
    "# >>> out = encoder_layer(src)   \n",
    "for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "    i += 1\n",
    "    for k,v in Feature_X.items():\n",
    "        print(k)\n",
    "        print(feature_transform(v,k))\n",
    "        input()\n",
    "    print(feature_y)\n",
    "    input()\n",
    "# batch = []\n",
    "# for idx in range(len(dataset)):\n",
    "#     batch.append(dataset[idx])\n",
    "#     if idx % 128 == 0:\n",
    "#         features_x = [x[0] for x in batch]\n",
    "#         features_y = [x[1] for x in batch]\n",
    "#         features_y = torch.tensor(features_y).reshape(-1,1)\n",
    "# #         print(features_y)\n",
    "# #         input()\n",
    "#         batch = []\n",
    "    \n",
    "#     y = feature_y.to(device) * output_scaling \n",
    "#     print(Feature_X['logret1_xs'].type())\n",
    "#     pred = model(Feature_X)\n",
    "#     print(pred.type())\n",
    "#     input()\n",
    "#     for stk in Feature_X['row_id']:\n",
    "        \n",
    "#         stockid.add(stk.split(\"-\")[0])\n",
    "# for i in range(len(dataset)-10):\n",
    "#     dataset[i]\n",
    "print(\"-->\", (time.time()-stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f03435-5cde-40bf-a86b-656ab4515ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce561c-5b64-44bf-882b-41f6250b1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated(device)/1024/1024/1024\n",
    "# model.to(\"cpu\")\n",
    "# torch.cuda.memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a966ee2c-8547-48be-a912-8d05bf48b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.init()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
