{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2f18e2-9894-4f0c-bd57-8b37afc02309",
   "metadata": {},
   "source": [
    "### Can our model predict current volatility?  (forget future; first it should be capable of predicting current one with given features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c4e895-fd66-490d-a8b8-d28b324d3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from optiver_features_handler import get_features_map_for_stock, get_row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750eb0e3-2860-490b-bc3c-2a512485a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\n",
    "OUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\n",
    "MODEL_OUTPUT_DIRECTORY = os.path.join(OUTPUT_DIRECTORY,\"models\")\n",
    "os.makedirs(OUTPUT_DIRECTORY,exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIRECTORY,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02947ab-aa1d-4af0-9d6e-7a51cff159ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interval_seconds = 5\n",
    "data_intervals_count = int(600/data_interval_seconds)\n",
    "class OptiverRealizedVolatilityDataset(Dataset):\n",
    "    def __init__(self, data_directory, mode=\"train\", lazy_load=True):\n",
    "        \"\"\"initializes Optiver Competition dataset\n",
    "        `mode`: train|test\n",
    "        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n",
    "        \"\"\"\n",
    "        print(\"INIT: OptiverRealizedVolatilityDataset\")\n",
    "        if mode.lower() not in ['train','test']:\n",
    "            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n",
    "        self.data_directory = data_directory\n",
    "        self.mode = mode.lower()\n",
    "        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n",
    "#         if self.mode == 'train':\n",
    "#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n",
    "        if self.mode == 'test':\n",
    "            self.main_df['target'] = 0\n",
    "        \n",
    "        self.cache_stocks_done_set = set()\n",
    "        # this is our final features lookup where we park all our features which can be addressed by row_id\n",
    "        # which is individual train/test.csv row id using 'stock_id`-`time_id`\n",
    "        self.cache_rowid_feature_map = {}\n",
    "        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n",
    "        targets = self.main_df['target'].tolist()\n",
    "        self.stock_possible_timeids_list = {}\n",
    "        for idx, row_id in enumerate(row_id_series.tolist()):\n",
    "            stock_id = int(row_id.split('-')[0])\n",
    "            time_id = int(row_id.split('-')[1])\n",
    "            self.cache_rowid_feature_map[row_id] = {'target':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n",
    "            \n",
    "            # below code is to make sure what timeids we expect from stock data extractor\n",
    "            # in case of missing parquet files we'll have to know the keys to fill default values into\n",
    "            if stock_id not in self.stock_possible_timeids_list:\n",
    "                self.stock_possible_timeids_list[stock_id] = []\n",
    "            self.stock_possible_timeids_list[stock_id].append(time_id)\n",
    "            \n",
    "        \n",
    "        if lazy_load == False:\n",
    "            worker_data = []\n",
    "            for gkey, gdf in self.main_df.groupby(['stock_id']):\n",
    "                worker_data.append((self.data_directory, self.mode, gkey))\n",
    "#             print(\"---------- CPU COUNG:\", multiprocessing.cpu_count())\n",
    "            # NOTE: this was hell of a hunt; this windows and pytorch and jupyter combination is too tedious\n",
    "            #       make sure the function that we distribute don't call pytorch\n",
    "            chunksize = multiprocessing.cpu_count() * 1\n",
    "            processed = 0\n",
    "            for worker_data_chunk in [worker_data[i * chunksize:(i + 1) * chunksize] for i in range((len(worker_data) + chunksize - 1) // chunksize )]:\n",
    "                with Pool(multiprocessing.cpu_count()) as p:\n",
    "                    \n",
    "                    feature_set_list = p.starmap(get_features_map_for_stock, worker_data_chunk)\n",
    "                    \n",
    "                    for feature_map in feature_set_list:\n",
    "                        for rowid, features_dict in feature_map.items():\n",
    "                            for fkey,fval in features_dict.items():\n",
    "                                self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                            self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                        # udpate the indications that we've already fetched this stock and the lazy loader code won't fetch this again\n",
    "                        self.cache_stocks_done_set.add(int(rowid.split('-')[0]))\n",
    "                    \n",
    "                    processed += chunksize\n",
    "                    print(f\"Processed and loaded {processed} stocks features.\")\n",
    "    \n",
    "    def __cache_generate_features(self, main_stock_id, main_time_id):\n",
    "            \n",
    "            main_row_id = get_row_id(main_stock_id, main_time_id)\n",
    "            if main_stock_id not in self.cache_stocks_done_set:\n",
    "#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))   \n",
    "                # we'll combine the featureset with the bigger feature set of all stocks\n",
    "                feature_map = get_features_map_for_stock(self.data_directory, self.mode, main_stock_id)\n",
    "                # NOTE: sometime we might now have parquet files in that case we'll have 3 entried in .csv while only 1 gets returned in feature map\n",
    "                # we need to cover for that disparity\n",
    "                for time_id in self.stock_possible_timeids_list[main_stock_id]:\n",
    "                    expected_row_id = get_row_id(main_stock_id, time_id)\n",
    "                    if expected_row_id not in feature_map:\n",
    "                        feature_map[expected_row_id] = {}\n",
    "                for rowid, features_dict in feature_map.items():\n",
    "                    for fkey,fval in features_dict.items():\n",
    "                        self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                    self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                self.cache_stocks_done_set.add(main_stock_id)\n",
    "#             print(self.cache_rowid_feature_map[main_row_id])\n",
    "#             print(torch.tensor([self.cache_rowid_feature_map[main_row_id].get('book_realized_volatility',0)]))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map[main_row_id].get('log_return1_2s', [0]*(int(600/2)))))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map.get('book_directional_volume1_2s', [0]*(int(600/2)))))\n",
    "            return self.cache_rowid_feature_map[main_row_id]\n",
    "        \n",
    "    @staticmethod\n",
    "    def transform_to_01_realized_volatility_linear_data(features_dict):\n",
    "        return (\n",
    "                {\n",
    "                    'row_id':features_dict['row_id'],\n",
    "                    'stock_id':torch.tensor(features_dict['stock_id'], dtype=torch.float32),\n",
    "                    'seconds_in_bucket_xs': torch.tensor(np.nan_to_num(features_dict.get('seconds_in_bucket_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "#                     'book_realized_volatility':torch.tensor([features_dict.get('book_realized_volatility',0)]),\n",
    "                    # TRADE FEATURES\n",
    "                    'logrett_xs': torch.tensor(np.nan_to_num(features_dict.get('logrett_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_volume_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_volume_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_ordercount_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_ordercount_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_money_turnover_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_money_turnover_per_order_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_per_order_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    \n",
    "#                     'trade_money_turnover_mean': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_mean', 0)), dtype=torch.float32),\n",
    "#                     'trade_money_turnover_std': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_std', 0)), dtype=torch.float32),\n",
    "                    # BOOK FEATURES\n",
    "                    'logret1_xs': torch.tensor(np.nan_to_num(features_dict.get('logret1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'logret2_xs': torch.tensor(np.nan_to_num(features_dict.get('logret2_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_directional_volume1_xs': torch.tensor(np.nan_to_num(features_dict.get('book_directional_volume1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_directional_volume2_xs': torch.tensor(np.nan_to_num(features_dict.get('book_directional_volume2_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_price_spread1_xs': torch.tensor(np.nan_to_num(features_dict.get('book_price_spread1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_price_spread2_xs': torch.tensor(np.nan_to_num(features_dict.get('book_price_spread2_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_bid_spread_xs': torch.tensor(np.nan_to_num(features_dict.get('book_bid_spread_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_ask_spread_xs': torch.tensor(np.nan_to_num(features_dict.get('book_ask_spread_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_total_volume_xs': torch.tensor(np.nan_to_num(features_dict.get('book_total_volume_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_volume_imbalance_xs': torch.tensor(np.nan_to_num(features_dict.get('book_volume_imbalance_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_money_turnover1_xs': torch.tensor(np.nan_to_num(features_dict.get('book_money_turnover1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    \n",
    "#                     'askp2_1s':torch.tensor(features_dict.get('askp2_1s', [0]*(int(600/1)))),\n",
    "#                     'book_directional_volume1_1s':torch.tensor(features_dict.get('book_directional_volume1_1s', [0]*(int(600/1)))) \n",
    "                },\n",
    "                torch.tensor([features_dict['target']])\n",
    "#                 [features_dict['target']]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: handle for num_workers more than 0\n",
    "        #      using https://pytorch.org/docs/stable/data.html\n",
    "        #      using torch.util.data.get_worker_info()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        stock_id = self.main_df.at[idx, 'stock_id']\n",
    "        time_id = self.main_df.at[idx, 'time_id']\n",
    "        x,y = self.__cache_generate_features(stock_id,time_id)\n",
    "#         x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efcee691-c9ce-481f-a4e8-dbf97b66b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT: OptiverRealizedVolatilityDataset\n",
      "Processed and loaded 16 stocks features.\n",
      "Processed and loaded 32 stocks features.\n",
      "Processed and loaded 48 stocks features.\n",
      "Processed and loaded 64 stocks features.\n",
      "Processed and loaded 80 stocks features.\n",
      "Processed and loaded 96 stocks features.\n",
      "Processed and loaded 112 stocks features.\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\", lazy_load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d58ff-26aa-4b02-b2ac-991ccb366c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for x in range(0,9):\n",
    "#     print(dataset[x])\n",
    "# dataset[10000] #[0]['bidp1_1s']\n",
    "for key,val in dataset[10000][0].items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2124b8d-39fb-4801-a3c1-378ee390a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset, batch_size=4096,shuffle=True, num_workers=0, pin_memory=True)\n",
    "sizes = set()\n",
    "for train_batch_idx, (feature_dict, feature_y) in enumerate(dataloader_train):\n",
    "    sizes.add(f\"{feature_dict['logrett_xs'].size()}\")\n",
    "        \n",
    "        \n",
    "#         print(val)\n",
    "#         input()\n",
    "#     print(x)\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3693b7-4338-49c6-8a4f-9a8205d4aa33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'torch.Size([2948, 120])', 'torch.Size([4096, 120])'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a72c9b-fab8-4536-b125-abc46cc800b6",
   "metadata": {},
   "source": [
    "### Learnings about model CNN input\n",
    "- it's better to use multiple channel for logreturn1 and logreturn2 than stacking it and using as one channel\n",
    "- 2 channels input for CNN is better than stacking it(dim 2, which is logret1_t1, logret2_t1, logret1_t2, logret2_t2...) and using it as one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc4b98ff-b328-465e-a04e-c25eef8ea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = None\n",
    "\n",
    "\n",
    "def loss_fn_mse(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)))\n",
    "\n",
    "def loss_fn_mspe(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)/y))\n",
    "\n",
    "def loss_fn_orig(y, pred):\n",
    "    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e35973-37f1-408a-b47f-3a8fa6c9879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, mode='both', use_stock_id = True):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.use_stock_id = use_stock_id\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mode = mode\n",
    "        self.cnn_stack = nn.Sequential(\n",
    "            nn.Conv1d(9, 24, kernel_size=6, stride=2, padding=0),\n",
    "            nn.GELU(),\n",
    "#             nn.BatchNorm1d(4),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.Conv1d(24, 32, kernel_size=4, stride=2, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(32, 48, kernel_size=3, stride=1, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(48, 64, kernel_size=2, stride=1, padding=0),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(8),\n",
    "#             nn.Conv1d(4, 4, kernel_size=6, stride=3, padding=0),\n",
    "#             nn.GELU(),\n",
    "#             nn.Conv1d(4, 4, kernel_size=4, stride=2, padding=0),\n",
    "#             nn.GELU(),\n",
    "#             nn.BatchNorm1d(4),\n",
    "#             nn.Conv1d(4, 4, kernel_size=6, stride=2, padding=0),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Conv1d(8, 8, kernel_size=4, stride=2, padding=0), \n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "        )\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(1600, 512),\n",
    "            nn.GELU(),\n",
    "#             nn.Dropout(0.3),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 16),\n",
    "#             nn.ReLU()\n",
    "        )\n",
    "        self.linear_hybrid = nn.Sequential(\n",
    "            nn.Linear(256+6, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "#         self.basic_stack = nn.Sequential(\n",
    "#             nn.Linear(int(600/2)*1,512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "#             nn.Linear(512,1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "# #             nn.Linear(2048,1024),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Dropout(),\n",
    "#             nn.Linear(1024,512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128,1),\n",
    "#         )\n",
    "        \n",
    "    def forward(self, feature_dict):\n",
    "#         logits = self.basic_stack(x)\n",
    "#         x = self.flatten(x)\n",
    "        x = torch.cat([\n",
    "#                             feature_dict['logrett_xs'].to(device)*10000, \n",
    "#                                torch.log(feature_dict['trade_volume_xs'].to(device)+0.001),\n",
    "#                               torch.log(feature_dict['trade_ordercount_xs'].to(device)+0.001),\n",
    "#                             feature_dict['trade_volume_xs'].to(device),\n",
    "#                                         feature_dict['trade_ordercount_xs'].to(device),\n",
    "#                                         feature_dict['book_total_volume_xs'].to(device),\n",
    "#                                         feature_dict['book_volume_imbalance_xs'].to(device)\n",
    "#                             feature_dict['logrett_xs'].to(device)*10000, \n",
    "                               torch.log(feature_dict['trade_volume_xs'].to(device)+0.001),\n",
    "                              torch.log(feature_dict['trade_ordercount_xs'].to(device)+0.001),\n",
    "                                feature_dict['logret1_xs'].to(device)*10000,\n",
    "                            \n",
    "                                 feature_dict['logret2_xs'].to(device)*10000,\n",
    "                                    feature_dict['book_price_spread1_xs'].to(device)*10000, \n",
    "#                                 feature_dict['book_price_spread2_xs'].to(device)*10000, \n",
    "                                feature_dict['book_bid_spread_xs'].to(device)*10000, \n",
    "                                feature_dict['book_ask_spread_xs'].to(device)*10000, \n",
    "#                                  feature_dict['book_directional_volume1_xs'].to(device),\n",
    "#                                 feature_dict['book_price_spread1_xs'].to(device)*1000,\n",
    "                                torch.log(feature_dict['book_total_volume_xs'].to(device)+0.001),\n",
    "                                torch.log(feature_dict['book_volume_imbalance_xs'].to(device)+0.001),\n",
    "# #                              feature_dict['book_dirvolume_xs'],\n",
    "                          ], 1)\n",
    "\n",
    "#         x = torch.nan_to_num(feature_dict['logrett_xs']).type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        \n",
    "#         print(x)\n",
    "#         input()\n",
    "#         if torch.isnan(x).any():\n",
    "# #             print(x)\n",
    "#             print(feature_dict)\n",
    "#             input()\n",
    "        x = x.to(device)\n",
    "        x = x.reshape(-1,9,data_intervals_count)\n",
    "        \n",
    "        logits = self.cnn_stack(x)\n",
    "        logits = self.flatten(logits)\n",
    "        \n",
    "       \n",
    "        \n",
    "        logits = self.linear_stack(logits)\n",
    "        logits = torch.cat( [logits, \n",
    "                             torch.log(feature_dict['trade_money_turnover_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001), \n",
    "                                           torch.log(feature_dict['trade_money_turnover_std'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "                                           torch.log(feature_dict['trade_price_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "                                           torch.log(feature_dict['book_money_turnover_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "                                           torch.log(feature_dict['book_money_turnover_std'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "                                           torch.log(feature_dict['book_price_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001)\n",
    "                                      ], 1)\n",
    "        \n",
    "        if self.use_stock_id:\n",
    "            stock_id = torch.tensor(feature_dict['stock_id']).reshape(-1,1)\n",
    "            stock_id = stock_id.to(device)\n",
    "            logits = torch.cat([logits, stock_id], 1)\n",
    "            \n",
    "        logits = self.linear_hybrid(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "316cdf05-8827-491b-805c-902be90598c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VolatilityGRU(nn.Module):\n",
    "#     def __init__(self, input_size=1, hidden_size=64, repeated_cells=1):\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "class SingleFetGRU(nn.Module):\n",
    "    def __init__(self, hidden_size=64, layers=1, dropout=0, features_out=32, mode=\"train\"):\n",
    "        \"\"\"single feature, feature learner\n",
    "        `mode`: train|feature_generator\n",
    "        \"\"\"\n",
    "        super(SingleFetGRU, self).__init__()\n",
    "        self.input_size_ = 1\n",
    "        self.hidden_size_ = hidden_size\n",
    "        self.repeated_lstm_cells_ = layers\n",
    "        self.dropout_ = dropout\n",
    "        self.features_out = features_out\n",
    "        \n",
    "        self.rnn_ = nn.GRU(self.input_size_, self.hidden_size_, self.repeated_lstm_cells_, batch_first=True, dropout=self.dropout_)\n",
    "        \n",
    "        self.linear_feature_stack_ = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size_*self.repeated_lstm_cells_, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, self.features_out),\n",
    "        )\n",
    "        \n",
    "        self.linear_trainer_stack_ = nn.Sequential(\n",
    "            nn.Linear(self.features_out, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 1),   \n",
    "        )\n",
    "        \n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def forward(self, feature_tensor):\n",
    "        if self.mode in [\"feature_generator\",\"train\"]:\n",
    "            h_0_ = torch.rand(self.repeated_lstm_cells_, feature_tensor.size(0), self.hidden_size_, device=device) #hidden state\n",
    "            output_, hn_ = self.rnn_(feature_tensor, h_0_) #lstm with input, hidden, and internal state\n",
    "            hn_ = hn_.reshape(-1, self.hidden_size_*self.repeated_lstm_cells_) #reshaping the data for Dense layer next  \n",
    "            \n",
    "            out_ = self.linear_feature_stack_(hn_)\n",
    "            \n",
    "            if self.mode == \"train\":\n",
    "                out_ = self.linear_trainer_stack_(out_)\n",
    "            \n",
    "            return out_\n",
    "            \n",
    "            \n",
    "            \n",
    "class VolatilityBSModel(nn.Module):\n",
    "    def __init__(self, mode=\"hybrid\", use_stock_id=False):\n",
    "        \"\"\"various rnn features' fusion with fully connected nn\n",
    "        `mode`: hybrid|<feature_name>\n",
    "        \"\"\"\n",
    "        super(VolatilityBSModel, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.use_stock_id = use_stock_id\n",
    "        self.feature_list = ['logrett_xs','trade_volume_xs','trade_ordercount_xs','trade_money_turnover_xs','trade_money_turnover_per_order_xs',\n",
    "                             'logret1_xs','logret2_xs','book_directional_volume1_xs','book_directional_volume2_xs',\n",
    "                             'book_price_spread1_xs','book_price_spread2_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "                             'book_total_volume_xs','book_volume_imbalance_xs','book_money_turnover1_xs']\n",
    "        self.feature_gen_feature_size = 32\n",
    "        self.feature_gen_models = {}\n",
    "        for k in self.feature_list:\n",
    "            self.feature_gen_models[k]=SingleFetGRU(hidden_size=64, layers=1, dropout=0, features_out=self.feature_gen_feature_size) \n",
    "            self.feature_gen_models[k].to(device)\n",
    "        \n",
    "        \n",
    "        self.linear_fusion = nn.Sequential(\n",
    "            nn.Linear(self.feature_gen_feature_size*len(self.feature_list) + (1 if self.use_stock_id else 0), 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "        self.set_mode(self.mode)\n",
    "    \n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return self.feature_list\n",
    "    \n",
    "    def set_mode(self, mode):\n",
    "        print(f\"------- set mode : {mode} -----------\")\n",
    "        self.mode = mode\n",
    "        for feature_gen_model in self.feature_gen_models.values():\n",
    "            feature_gen_model.set_mode('feature_generator' if self.mode in ['hybrid','hybrid_feature_out'] else 'train')\n",
    "    \n",
    "    def parameters(self):\n",
    "        \n",
    "        generator_sources_map = {k:[v] for k,v in self.feature_gen_models.items()}\n",
    "        generator_sources_map['hybrid']= [self.linear_fusion]\n",
    "        \n",
    "        params = []\n",
    "        if self.mode in generator_sources_map:\n",
    "            for generator_source in generator_sources_map[self.mode]:\n",
    "                for param in generator_source.parameters():\n",
    "                    params.append(param)\n",
    "        else:\n",
    "            return super(VolatilityBSModel,self).parameters()\n",
    "        return params\n",
    "    \n",
    "    def feature_transform(self, feature_x, feature_name):\n",
    "        if feature_name in ['logrett_xs',\n",
    "                             'logret1_xs','logret2_xs',\n",
    "                             'book_price_spread1_xs','book_price_spread2_xs','book_bid_spread_xs','book_ask_spread_xs']:\n",
    "            return feature_x * 10000\n",
    "        if feature_name in ['trade_volume_xs','trade_money_turnover_xs','trade_money_turnover_per_order_xs',\n",
    "                             'book_total_volume_xs','book_volume_imbalance_xs','book_money_turnover1_xs']:\n",
    "            return torch.log(feature_x + 0.001)\n",
    "        return feature_x\n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "        \n",
    "        if self.mode in self.feature_list:\n",
    "            feature_x = feature_dict[self.mode].to(device).reshape(-1, data_intervals_count ,1)\n",
    "            feature_x = self.feature_transform(feature_x, self.mode)\n",
    "            out = self.feature_gen_models[self.mode](feature_x)\n",
    "            return out\n",
    "        \n",
    "        if self.mode in ['hybrid','hybrid_feature_out']:\n",
    "            generated_features = []\n",
    "            for feature_name, feature_gen_model in self.feature_gen_models.items():\n",
    "                feature_x = feature_dict[feature_name].to(device).reshape(-1, data_intervals_count ,1)\n",
    "                feature_x = self.feature_transform(feature_x, feature_name)\n",
    "                features_out = feature_gen_model(feature_x)\n",
    "                generated_features.append(features_out)\n",
    "                \n",
    "                \n",
    "            combined_features = torch.cat(generated_features, 1).reshape(-1, self.feature_gen_feature_size*len(self.feature_list))\n",
    "            \n",
    "            if self.use_stock_id:\n",
    "                stock_id = feature_dict['stock_id'].to(device).reshape(-1,1)\n",
    "                combined_features = torch.cat([combined_features, stock_id], 1)\n",
    "                \n",
    "            if self.mode == 'hybrid_feature_out':\n",
    "                return combined_features\n",
    "            \n",
    "            out = self.linear_fusion(combined_features)\n",
    "            return out\n",
    "        \n",
    "#         input(\"--- out got\")\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a33b331-4eb1-4c09-84df-01b687ac3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VolatilityBSModel(use_stock_id=use_stock_id)\n",
    "# # model = NeuralNetwork(use_stock_id=False)\n",
    "# model.to(device)\n",
    "# adam_for_modes = {}\n",
    "\n",
    "# for modeidx, mode in enumerate(['yoyo','trade','experiment','book','hybrid']*1):\n",
    "#     epochs = 1\n",
    "#     model.mode = mode\n",
    "#     print(model.parameters())\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2f1840c-2bc4-4521-933f-994f1ceb014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VolatilityBSModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "57c9218f-b67a-49af-bd37-ff307bd12ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49a121-34ca-498f-9267-4c20a04e2c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.children():\n",
    "    print()\n",
    "    print()\n",
    "    print(layer)\n",
    "    print(\"-------\")\n",
    "    for l in layer.children():\n",
    "        print([x for x in l.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1f24f-c586-4e1b-aad6-fb4671c87232",
   "metadata": {},
   "source": [
    "#### analyze the initial weights (or change them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "931890cf-e329-4a15-b0d0-352dc27f985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @torch.no_grad()\n",
    "# def init_weights(m):\n",
    "# #     print(m)\n",
    "#     if type(m) == nn.Linear:\n",
    "# #         m.weight.fill_(1.0)\n",
    "#         torch.nn.init.xavier_uniform_(m.weight,gain=10)\n",
    "#         m.bias.data.uniform_(-1,1)\n",
    "# #     elif type(m) == nn.ReLU:\n",
    "# #         print(m.data)\n",
    "#     else:\n",
    "#         print(type(m))\n",
    "# #         print(m.weight)\n",
    "# model.apply(init_weights)\n",
    "# # for param in model.parameters():\n",
    "# # #     print(param)\n",
    "# #       print(param.data.size(), param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c79a7d-9e08-4433-9c77-1af049f349d6",
   "metadata": {},
   "source": [
    "### LEarning rate: our base line is 0.34 loss as that's what the optiver guys have when they use current 10 min realize vol and use it as target (copy to prediction). We create simplest neural network and work with learning rates to figure out what's best and when we see something in range of 0.35 then we've found good Learning rate\n",
    "- #### SGD: 1e-7 works best\n",
    "- #### ADAM: 1e-5, (NOTE: 1e-3 makes it behave dumb where some deep local minima gets stuck and produces constant output!)\n",
    "- TODO: analyze that constant output phenomenon more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5cb3871f-5215-4afb-92fc-47ae2d385fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-4\n",
    "# batch_size = 4096\n",
    "# epochs = 100\n",
    "\n",
    "# input_scaling = 1\n",
    "# output_scaling = 1\n",
    "\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "# strategyname = \"ret1_n_ret2\"\n",
    "# summary_writer = SummaryWriter(f'../output/training_tensorboard/{strategyname}_scaleIn{input_scaling}Out{output_scaling}_{learning_rate}_{batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9aadc54b-6009-4af6-bfb6-f6619c6acd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a7ed1-80f8-4a57-90b4-223345dde76b",
   "metadata": {},
   "source": [
    "### Learnings about training\n",
    "- (non scaling)logreturns input and volatility output; non scaled makes the model predict constant output with no variety(close to 0 std dev)\n",
    "- scaling input rids of variety issue, \n",
    "- scaling output makes the model start with low rmse initially so there's less ground to cover and we can iterate over ideas rapidly due to less epochs needed to achieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cef34a31-7317-47c0-a94f-9bebe2fd2e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n",
      "------- set mode : hybrid -----------\n",
      "------- set mode : hybrid -----------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "---------- ultimate_hybridEXP3_RNN_5s_StkFalse_0.001_64 hybrid ----------\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 512])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function AddmmBackward returned an invalid gradient at index 1 - got [64, 128] but expected shape compatible with [64, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15084/4069423315.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                 \u001b[0mloss_orig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Function AddmmBackward returned an invalid gradient at index 1 - got [64, 128] but expected shape compatible with [64, 512]"
     ]
    }
   ],
   "source": [
    "# model = None\n",
    "strategyname = \"ultimate_hybridEXP3_RNN_5s\"\n",
    "\n",
    "\n",
    "print(\"DEVICE:\", device)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "training_configs = []\n",
    "learning_rates_to_try = [1e-3]# 1e-4]\n",
    "batch_sizes_to_try = [64]#, 512]#,10000, 128]\n",
    "# input_scalings_to_try = [1000]\n",
    "# output_scalings_to_try = [10000]\n",
    "output_scaling = 10000\n",
    "for learning_rate in learning_rates_to_try:\n",
    "    for batch_size in batch_sizes_to_try:\n",
    "        for use_stock_id in [False, True]:\n",
    "            training_configs.append({\n",
    "                'learning_rate':learning_rate,\n",
    "                'batch_size':batch_size,\n",
    "                'use_stock_id': use_stock_id\n",
    "            })\n",
    "\n",
    "epochs = 200\n",
    "for training_config in training_configs:\n",
    "    \n",
    "    learning_rate = training_config['learning_rate']\n",
    "    batch_size = training_config['batch_size']\n",
    "    use_stock_id = training_config['use_stock_id']\n",
    "    # TRAINING SETUP\n",
    "    \n",
    "    #refresh the model\n",
    "    \n",
    "    STRATEGY_NAME_WITH_ATTRS = f\"{strategyname}_Stk{use_stock_id}_{learning_rate}_{batch_size}\"\n",
    "    summary_writer = SummaryWriter(f'../output/training_tensorboard/{STRATEGY_NAME_WITH_ATTRS}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    model = VolatilityBSModel(use_stock_id=use_stock_id)\n",
    "#     model = NeuralNetwork(use_stock_id=False)\n",
    "    model.to(device)\n",
    "    optimizer_for_modes = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    for modeidx, mode in enumerate(['hybrid'] + model.get_feature_gen_train_modes() + ['hybrid']):\n",
    "        epochs = 1\n",
    "        model.set_mode(mode)\n",
    "#         print(model.parameters())\n",
    "#         input()\n",
    "#         continue\n",
    "        if mode not in optimizer_for_modes:\n",
    "            optimizer_for_modes[mode] = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "#             optimizer_for_modes[mode] = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        optimizer = optimizer_for_modes[mode]\n",
    "\n",
    "        \n",
    "        # TRAINING SETUP DONE\n",
    "\n",
    "        \n",
    "\n",
    "        data_ohlc_sample_len = 1 # 1 for each of open high low close\n",
    "        losses_train = []\n",
    "        for t in range(epochs):\n",
    "            t = modeidx*epochs + t\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            print(\"----------\", STRATEGY_NAME_WITH_ATTRS, mode,\"----------\")\n",
    "\n",
    "            dataloader_train = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                shuffle=True, num_workers=0, pin_memory=True)\n",
    "#             model.train()\n",
    "\n",
    "            for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "\n",
    "                y = feature_y.to(device) * output_scaling \n",
    "\n",
    "                pred = model(Feature_X)\n",
    "#                 pred.to(device)\n",
    "#                 print(pred)\n",
    "#                 input()\n",
    "                loss_orig = loss_fn_orig(y, pred)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss_orig.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "                losses_train.append(loss_orig.item())\n",
    "\n",
    "                if (t*int(train_size/batch_size) + train_batch_idx + 1) % int(train_size/20/batch_size) == 0:\n",
    "\n",
    "                    # NOTE: real loss is same as upscaled normalized loss as it's percentage loss (rmspe)\n",
    "                    prediction_variety = np.std((pred/output_scaling).reshape(-1).tolist()) * 100\n",
    "                    #NOTE: prediction variety is important as model sometimes predits a constant value! regardless of the input, then per batch variety is lowest(0 std dev)\n",
    "\n",
    "\n",
    "                    summary_writer.add_scalar(\"Prediction Variety\", prediction_variety, t*(train_size) + (train_batch_idx*batch_size))\n",
    "                    summary_writer.add_scalar(\"Training Loss\", np.mean(losses_train), t*(train_size) + (train_batch_idx*batch_size))\n",
    "\n",
    "                    print(\"train:\", np.mean(losses_train), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\")\n",
    "                    losses_train = []\n",
    "\n",
    "            dataloader_test = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                    shuffle=True, num_workers=0, pin_memory=True)\n",
    "            dataset_size = len(dataloader_test.dataset)\n",
    "            \n",
    "#             model.eval()\n",
    "\n",
    "            losses_test = []\n",
    "            for _, (Feature_X, feature_y) in enumerate(dataloader_test):\n",
    "                with torch.no_grad():\n",
    "                    y = feature_y.to(device) * output_scaling\n",
    "                    pred = model(Feature_X)\n",
    "                    loss = loss_fn_orig(y, pred)\n",
    "                    losses_test.append(loss.item())\n",
    "\n",
    "\n",
    "    #                 summary_writer.add_scalar(\"Epoch Training Loss\", np.mean(losses_train), (t+1)*train_size)\n",
    "            summary_writer.add_scalar(\"Test Loss\", np.mean(losses_test), t*(train_size) + (train_batch_idx*batch_size))\n",
    "            print(\"train:\", np.mean(losses_train), \"test:\", np.mean(losses_test), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\")\n",
    "            losses_test = []\n",
    "            if (t+1)%50==0:\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{loss:.4f}.pth\"))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99ec365f-201b-44f7-826c-2017b8bbc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"11_{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{np.mean(losses_test):.4f}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0a1f5760-8926-4064-8bf2-bddd62c80bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1633"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74227704-6e5f-41fd-b9a2-242d0b8e9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optiver_custom_collate_func(batch):\n",
    "    output_x = {}\n",
    "    for k,v in batch[0][0].items():\n",
    "        output_x[k] = []\n",
    "    \n",
    "    for x_dict in [x[0] for x in batch]:\n",
    "        for k,v in x_dict.items():\n",
    "            output_x[k].append(v)\n",
    "    \n",
    "    for k,v in batch[0][0].items():\n",
    "        if type(output_x[k][0]) != str:\n",
    "            output_x[k] = torch.stack(output_x[k])\n",
    "        \n",
    "    output_y = []\n",
    "    for y in [x[1] for x in batch]:\n",
    "        output_y.append(y)\n",
    "    output_y = torch.stack(output_y)\n",
    "    \n",
    "    return (output_x, output_y)\n",
    "#     input()\n",
    "#     print(batch)\n",
    "# #     return batch\n",
    "#     input()\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17cea68b-9cf2-4a6c-8fab-d0e6b8f7654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 27.83697199821472\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "\n",
    "    \n",
    "dataloader_train = DataLoader(dataset, batch_size=3,\n",
    "                                shuffle=False, num_workers=0, pin_memory=False)#, collate_fn=optiver_custom_collate_func)\n",
    "# #             model.train()\n",
    "# i = 0\n",
    "# stockid = set()\n",
    "for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "    i += 1\n",
    "#     print(feature_y)\n",
    "#     input()\n",
    "# batch = []\n",
    "# for idx in range(len(dataset)):\n",
    "#     batch.append(dataset[idx])\n",
    "#     if idx % 128 == 0:\n",
    "#         features_x = [x[0] for x in batch]\n",
    "#         features_y = [x[1] for x in batch]\n",
    "#         features_y = torch.tensor(features_y).reshape(-1,1)\n",
    "# #         print(features_y)\n",
    "# #         input()\n",
    "#         batch = []\n",
    "    \n",
    "#     y = feature_y.to(device) * output_scaling \n",
    "#     print(Feature_X['logret1_xs'].type())\n",
    "#     pred = model(Feature_X)\n",
    "#     print(pred.type())\n",
    "#     input()\n",
    "#     for stk in Feature_X['row_id']:\n",
    "        \n",
    "#         stockid.add(stk.split(\"-\")[0])\n",
    "# for i in range(len(dataset)-10):\n",
    "#     dataset[i]\n",
    "print(\"-->\", (time.time()-stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f03435-5cde-40bf-a86b-656ab4515ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce561c-5b64-44bf-882b-41f6250b1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated(device)/1024/1024/1024\n",
    "# model.to(\"cpu\")\n",
    "# torch.cuda.memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a966ee2c-8547-48be-a912-8d05bf48b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.init()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
