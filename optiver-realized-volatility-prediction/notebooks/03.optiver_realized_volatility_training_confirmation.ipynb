{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2f18e2-9894-4f0c-bd57-8b37afc02309",
   "metadata": {},
   "source": [
    "### Can our model predict current volatility?  (forget future; first it should be capable of predicting current one with given features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c4e895-fd66-490d-a8b8-d28b324d3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from optiver_features_handler import get_features_map_for_stock, get_row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad9958-4061-4195-b9d8-77f142bc7fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750eb0e3-2860-490b-bc3c-2a512485a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\n",
    "OUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\n",
    "MODEL_OUTPUT_DIRECTORY = os.path.join(OUTPUT_DIRECTORY,\"models\")\n",
    "os.makedirs(OUTPUT_DIRECTORY,exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIRECTORY,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d3a0b1-c7aa-4df5-a088-baf5a8fb5de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02947ab-aa1d-4af0-9d6e-7a51cff159ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interval_seconds = 5\n",
    "class OptiverRealizedVolatilityDataset(Dataset):\n",
    "    def __init__(self, data_directory, mode=\"train\", lazy_load=True):\n",
    "        \"\"\"initializes Optiver Competition dataset\n",
    "        `mode`: train|test\n",
    "        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n",
    "        \"\"\"\n",
    "        print(\"INIT: OptiverRealizedVolatilityDataset\")\n",
    "        if mode.lower() not in ['train','test']:\n",
    "            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n",
    "        self.data_directory = data_directory\n",
    "        self.mode = mode.lower()\n",
    "        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n",
    "#         if self.mode == 'train':\n",
    "#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n",
    "        if self.mode == 'test':\n",
    "            self.main_df['target'] = 0\n",
    "        \n",
    "        self.cache_stocks_done_set = set()\n",
    "        # this is our final features lookup where we park all our features which can be addressed by row_id\n",
    "        # which is individual train/test.csv row id using 'stock_id`-`time_id`\n",
    "        self.cache_rowid_feature_map = {}\n",
    "        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n",
    "        targets = self.main_df['target'].tolist()\n",
    "        self.stock_possible_timeids_list = {}\n",
    "        for idx, row_id in enumerate(row_id_series.tolist()):\n",
    "            stock_id = int(row_id.split('-')[0])\n",
    "            time_id = int(row_id.split('-')[1])\n",
    "            self.cache_rowid_feature_map[row_id] = {'target':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n",
    "            \n",
    "            # below code is to make sure what timeids we expect from stock data extractor\n",
    "            # in case of missing parquet files we'll have to know the keys to fill default values into\n",
    "            if stock_id not in self.stock_possible_timeids_list:\n",
    "                self.stock_possible_timeids_list[stock_id] = []\n",
    "            self.stock_possible_timeids_list[stock_id].append(time_id)\n",
    "            \n",
    "        \n",
    "        if lazy_load == False:\n",
    "            worker_data = []\n",
    "            for gkey, gdf in self.main_df.groupby(['stock_id']):\n",
    "                worker_data.append((self.data_directory, self.mode, gkey))\n",
    "#             print(\"---------- CPU COUNG:\", multiprocessing.cpu_count())\n",
    "            # NOTE: this was hell of a hunt; this windows and pytorch and jupyter combination is too tedious\n",
    "            #       make sure the function that we distribute don't call pytorch\n",
    "            chunksize = multiprocessing.cpu_count() * 2\n",
    "            processed = 0\n",
    "            for worker_data_chunk in [worker_data[i * chunksize:(i + 1) * chunksize] for i in range((len(worker_data) + chunksize - 1) // chunksize )]:\n",
    "                with Pool(multiprocessing.cpu_count()) as p:\n",
    "                    \n",
    "                    feature_set_list = p.starmap(get_features_map_for_stock, worker_data_chunk)\n",
    "                    \n",
    "                    for feature_map in feature_set_list:\n",
    "                        for rowid, features_dict in feature_map.items():\n",
    "                            for fkey,fval in features_dict.items():\n",
    "                                self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                            self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                        # udpate the indications that we've already fetched this stock and the lazy loader code won't fetch this again\n",
    "                        self.cache_stocks_done_set.add(int(rowid.split('-')[0]))\n",
    "                    \n",
    "                    processed += chunksize\n",
    "                    print(f\"Processed and loaded {processed} stocks features.\")\n",
    "    \n",
    "    def __cache_generate_features(self, main_stock_id, main_time_id):\n",
    "            \n",
    "            \n",
    "            main_row_id = get_row_id(main_stock_id, main_time_id)\n",
    "            if main_stock_id not in self.cache_stocks_done_set:\n",
    "#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))   \n",
    "                # we'll combine the featureset with the bigger feature set of all stocks\n",
    "                feature_map = get_features_map_for_stock(self.data_directory, self.mode, main_stock_id)\n",
    "                # NOTE: sometime we might now have parquet files in that case we'll have 3 entried in .csv while only 1 gets returned in feature map\n",
    "                # we need to cover for that disparity\n",
    "                for time_id in self.stock_possible_timeids_list[main_stock_id]:\n",
    "                    expected_row_id = get_row_id(main_stock_id, time_id)\n",
    "                    if expected_row_id not in feature_map:\n",
    "                        feature_map[expected_row_id] = {}\n",
    "                for rowid, features_dict in feature_map.items():\n",
    "                    for fkey,fval in features_dict.items():\n",
    "                        self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                    self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                self.cache_stocks_done_set.add(main_stock_id)\n",
    "#             print(self.cache_rowid_feature_map[main_row_id])\n",
    "#             print(torch.tensor([self.cache_rowid_feature_map[main_row_id].get('book_realized_volatility',0)]))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map[main_row_id].get('log_return1_2s', [0]*(int(600/2)))))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map.get('book_directional_volume1_2s', [0]*(int(600/2)))))\n",
    "            return self.cache_rowid_feature_map[main_row_id]\n",
    "        \n",
    "    @staticmethod\n",
    "    def transform_to_01_realized_volatility_linear_data(features_dict):\n",
    "        return (\n",
    "                {\n",
    "                    'row_id':features_dict['row_id'],\n",
    "#                     'book_realized_volatility':torch.tensor([features_dict.get('book_realized_volatility',0)]),\n",
    "                    'logrett_xs':torch.tensor(features_dict.get('logrett_xs', [0]*(int(600/data_interval_seconds)))),\n",
    "                    'trade_volume_xs':torch.tensor(features_dict.get('trade_volume_xs', [0]*(int(600/data_interval_seconds)))),\n",
    "                    'trade_ordercount_xs':torch.tensor(features_dict.get('trade_ordercount_xs', [0]*(int(600/data_interval_seconds)))),\n",
    "                    'logret1_xs':torch.tensor(features_dict.get('logret1_xs', [0]*(int(600/data_interval_seconds)))),\n",
    "                    'logret2_xs':torch.tensor(features_dict.get('logret2_xs', [0]*(int(600/data_interval_seconds)))),\n",
    "                    'book_dirvolume_xs':torch.tensor(features_dict.get('book_dirvolume_xs', [0]*(int(600/data_interval_seconds)))),\n",
    "#                     'askp2_1s':torch.tensor(features_dict.get('askp2_1s', [0]*(int(600/1)))),\n",
    "#                     'book_directional_volume1_1s':torch.tensor(features_dict.get('book_directional_volume1_1s', [0]*(int(600/1)))) \n",
    "                },\n",
    "                torch.tensor([features_dict['target']])\n",
    "#                 [features_dict['target']]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: handle for num_workers more than 0\n",
    "        #      using https://pytorch.org/docs/stable/data.html\n",
    "        #      using torch.util.data.get_worker_info()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        stock_id = self.main_df.at[idx, 'stock_id']\n",
    "        time_id = self.main_df.at[idx, 'time_id']\n",
    "        x,y = self.__cache_generate_features(stock_id,time_id)\n",
    "#         x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcee691-c9ce-481f-a4e8-dbf97b66b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT: OptiverRealizedVolatilityDataset\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\", lazy_load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202d58ff-26aa-4b02-b2ac-991ccb366c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'row_id': '0-146',\n",
       "  'logrett_xs': tensor([ 8.1332e-05,  8.1332e-05,  8.1444e-05,  8.1318e-05,  8.1312e-05,\n",
       "           8.1424e-05,  8.1299e-05,  8.1411e-05,  8.1285e-05,  3.9986e-05,\n",
       "           3.9984e-05, -1.6422e-05, -1.6541e-05, -1.6422e-05, -1.6542e-05,\n",
       "          -1.6423e-05, -1.6542e-05, -1.6423e-05, -1.6543e-05, -1.6424e-05,\n",
       "          -1.6543e-05, -1.6424e-05, -1.6544e-05, -1.6425e-05, -1.6544e-05,\n",
       "          -1.6426e-05, -1.6545e-05, -1.6426e-05, -1.6545e-05, -1.6427e-05,\n",
       "          -2.0881e-04, -2.0885e-04, -2.0890e-04, -1.0124e-05, -1.0244e-05,\n",
       "          -1.0124e-05, -1.0244e-05, -1.0125e-05, -9.5294e-06, -9.6486e-06,\n",
       "          -9.5294e-06, -9.6487e-06, -9.5296e-06, -9.6490e-06, -9.5298e-06,\n",
       "          -9.6490e-06, -9.5301e-06, -9.6492e-06, -9.5303e-06, -9.6494e-06,\n",
       "          -9.5303e-06, -9.5305e-06, -9.6497e-06, -9.5307e-06, -9.6498e-06,\n",
       "          -9.5309e-06, -9.6500e-06, -9.5311e-06, -9.6503e-06, -9.5312e-06,\n",
       "          -9.6504e-06, -9.5314e-06, -9.5314e-06, -9.6508e-06, -9.5317e-06,\n",
       "          -9.6508e-06, -9.5319e-06, -9.6511e-06, -9.5320e-06, -9.6513e-06,\n",
       "          -9.5322e-06, -9.6514e-06, -9.5324e-06, -9.6517e-06, -9.5326e-06,\n",
       "          -6.8280e-05, -6.8165e-05, -6.8289e-05, -6.8294e-05, -6.8179e-05,\n",
       "          -6.8303e-05, -6.0380e-05, -6.0384e-05, -6.0447e-05, -6.0391e-05,\n",
       "          -6.0395e-05, -6.0398e-05, -6.0462e-05, -6.0406e-05, -6.0409e-05,\n",
       "          -3.9241e-05, -3.9183e-05, -3.9244e-05, -3.9246e-05, -1.5925e-05,\n",
       "          -1.5926e-05, -1.5926e-05, -1.5867e-05, -1.5926e-05, -1.5927e-05,\n",
       "          -1.5927e-05, -1.5927e-05, -1.5927e-05, -1.5928e-05, -1.5928e-05,\n",
       "          -1.5868e-05, -1.5928e-05, -1.5929e-05, -1.5929e-05, -1.5929e-05,\n",
       "          -1.5929e-05, -1.5930e-05, -1.5930e-05, -1.5870e-05, -1.5931e-05,\n",
       "          -1.5931e-05, -1.5931e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00]),\n",
       "  'trade_volume_xs': tensor([  2.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 288.,   0.,   6.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   1.,   0.,   0., 597.,   0.,   0.,   0.,\n",
       "            0., 205.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0., 340.,   0.,   0.,   0.,   0.,   0.,  10.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,  63.,   0.,   0.,   0.,  38.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  12.,   0.,   0.,   0.]),\n",
       "  'trade_ordercount_xs': tensor([ 2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  8.,  0.,  1.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  1.,  0.,  0., 11.,  0.,  0.,  0.,  0.,  6.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  5.,  0.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.]),\n",
       "  'logret1_xs': tensor([ 1.1981e-03,  1.1981e-03,  9.7270e-04,  4.4988e-05, -3.4609e-04,\n",
       "          -3.0564e-04, -5.6302e-06, -2.3947e-04,  2.8629e-05,  1.1154e-04,\n",
       "          -2.4085e-05,  5.2293e-06, -5.1451e-04, -5.1478e-04, -5.1504e-04,\n",
       "           9.1065e-04, -1.0691e-03, -5.3426e-04,  2.2809e-05,  1.0154e-04,\n",
       "           5.4588e-04,  7.1200e-06, -7.6381e-06, -1.3986e-04, -4.1054e-05,\n",
       "          -1.2461e-03,  5.1735e-04, -3.5736e-04,  1.0820e-03,  4.5366e-05,\n",
       "           4.5364e-05,  5.8942e-06, -1.5344e-06, -1.5344e-06, -9.8136e-08,\n",
       "          -9.8136e-08, -9.8136e-08, -9.8136e-08,  5.3769e-06,  1.1355e-03,\n",
       "           1.9444e-04,  1.4576e-03,  5.2682e-05, -6.6722e-05,  4.7329e-04,\n",
       "           7.8420e-05,  3.4420e-07,  1.8477e-06,  7.3063e-07,  6.8800e-07,\n",
       "          -1.0770e-04, -1.1652e-04,  7.5328e-04,  4.6400e-04, -3.0466e-04,\n",
       "          -1.6024e-06, -1.6024e-06, -1.1285e-05, -2.8494e-06,  3.3982e-07,\n",
       "           1.1990e-05,  5.7865e-06, -5.6401e-06, -2.7992e-06,  6.8560e-04,\n",
       "           0.0000e+00, -7.0439e-04, -9.1459e-05, -9.1467e-05,  1.2820e-04,\n",
       "           3.4376e-04, -2.7284e-07,  5.4131e-04,  8.3113e-05,  3.5587e-04,\n",
       "          -1.4954e-04, -1.4956e-04,  2.7068e-04, -2.7068e-04, -2.7075e-04,\n",
       "          -2.7083e-04,  4.0612e-04,  4.0596e-04,  5.1749e-06,  5.1748e-06,\n",
       "           5.1748e-06,  5.1748e-06,  6.6464e-06, -6.6736e-04,  6.5371e-04,\n",
       "           1.7686e-06,  0.0000e+00,  7.2935e-06,  1.0798e-04,  1.0796e-04,\n",
       "           2.2585e-04, -1.8117e-04,  4.4926e-05, -9.8269e-05,  3.9661e-04,\n",
       "          -1.8023e-04,  1.4366e-04,  1.9476e-04, -7.5751e-06, -2.4056e-04,\n",
       "          -1.2062e-04, -1.2064e-04,  0.0000e+00, -5.1196e-06,  8.4514e-05,\n",
       "           1.2853e-04, -1.5854e-04, -1.5857e-04,  9.5527e-04,  2.3160e-04,\n",
       "           4.9692e-06,  4.9692e-06,  1.9650e-08,  1.3931e-06,  0.0000e+00]),\n",
       "  'logret2_xs': tensor([-3.5749e-04, -3.5749e-04,  1.3294e-03,  2.9159e-04, -7.9414e-04,\n",
       "          -9.5184e-04,  5.6530e-04,  3.7139e-04, -2.5758e-05, -9.0284e-05,\n",
       "           7.9942e-05,  2.5926e-05, -1.5223e-05, -1.5223e-05, -1.5223e-05,\n",
       "          -3.4865e-04, -3.3038e-04, -1.4505e-03, -4.0280e-04,  1.2826e-03,\n",
       "          -2.1133e-05, -1.5179e-05, -1.4913e-04, -8.7918e-04,  9.7591e-04,\n",
       "          -1.4428e-03, -2.0388e-05, -3.8036e-04,  7.5004e-04, -3.4580e-06,\n",
       "          -3.4580e-06, -5.0683e-04,  0.0000e+00,  0.0000e+00,  2.0197e-04,\n",
       "           2.0193e-04,  2.0189e-04,  2.0185e-04, -1.8155e-04,  2.3129e-03,\n",
       "          -3.1090e-04,  5.4546e-04, -6.8283e-05,  4.8366e-04,  2.6225e-05,\n",
       "          -1.1275e-04, -1.3164e-04,  1.5147e-04,  1.6759e-04,  5.3853e-04,\n",
       "           2.3633e-04, -8.3874e-04,  9.2563e-04,  1.2016e-03, -7.7389e-04,\n",
       "           3.3071e-04,  3.3060e-04, -2.4469e-04,  2.4639e-04, -4.1303e-06,\n",
       "          -3.5900e-04, -1.1463e-03, -6.3921e-04,  7.3280e-04, -4.5582e-07,\n",
       "          -2.9287e-04,  1.3604e-03,  3.4350e-04,  3.4338e-04,  0.0000e+00,\n",
       "          -1.7751e-03, -3.4478e-04,  2.6030e-03, -1.0412e-05,  1.4353e-04,\n",
       "          -9.8570e-04, -9.8667e-04,  1.8364e-03,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -1.1181e-04, -1.1182e-04,  7.7913e-05,  7.7907e-05,\n",
       "           7.7901e-05,  7.7895e-05, -7.4137e-05, -1.4248e-05,  9.2408e-05,\n",
       "          -5.6055e-05,  0.0000e+00,  2.2104e-05,  1.4307e-04,  1.4305e-04,\n",
       "          -2.7749e-04,  1.7649e-04,  1.7539e-04,  1.6085e-04,  3.4372e-04,\n",
       "           1.0076e-04, -9.7201e-06, -8.2824e-05, -7.0764e-05,  4.4558e-05,\n",
       "           5.4774e-05,  5.4771e-05, -1.8651e-05, -1.0534e-03,  2.0631e-04,\n",
       "           6.4481e-04,  2.8703e-04,  2.8695e-04,  3.1788e-05,  4.3598e-05,\n",
       "           6.2992e-06,  6.2991e-06, -8.8174e-06, -2.5948e-05, -1.1078e-05]),\n",
       "  'book_dirvolume_xs': tensor([ 262., -106., 1165.,  711.,  341.,  131.,  100.,  271.,  246.,  273.,\n",
       "           407.,  544.,    0.,    0.,   89.,  545.,  649.,   32.,   39.,  785.,\n",
       "           870.,  384.,  891.,  328.,  221., -191.,   31., -217.,  469.,    0.,\n",
       "          1004.,  420.,    0.,  320.,    0.,    0.,    0.,  737., 1642., 1055.,\n",
       "          1399.,  597., -616.,  132.,  147.,   97.,  291.,  284.,  181.,  304.,\n",
       "          1010.,    5.,  353.,  938.,  652.,    0.,  581.,  620.,  579.,  386.,\n",
       "          1475.,  396.,  375.,  773.,  605.,  401.,  100.,    0.,   -3.,  195.,\n",
       "           142., -103.,  171., 1024.,  290.,    0.,   59.,  128.,    0.,    0.,\n",
       "            28.,    0.,  225.,    0.,    0.,    0.,  794.,  371., 1054.,  840.,\n",
       "           369.,  369.,  469.,    0.,  785.,  966.,  974., 1118., 1298., 1119.,\n",
       "           550.,  275., 1442.,  902.,  868.,    0.,  298.,  297.,  198.,  848.,\n",
       "           653.,    0.,   90.,  765.,  495.,    0.,  779.,  783.,  885.,  442.])},\n",
       " tensor([0.0054]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for x in range(0,9):\n",
    "#     print(dataset[x])\n",
    "# dataset[10000] #[0]['bidp1_1s']\n",
    "dataset[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a72c9b-fab8-4536-b125-abc46cc800b6",
   "metadata": {},
   "source": [
    "### Learnings about model CNN input\n",
    "- it's better to use multiple channel for logreturn1 and logreturn2 than stacking it and using as one channel\n",
    "- 2 channels input for CNN is better than stacking it(dim 2, which is logret1_t1, logret2_t1, logret1_t2, logret2_t2...) and using it as one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15e35973-37f1-408a-b47f-3a8fa6c9879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.cnn_stack = nn.Sequential(\n",
    "            nn.Conv1d(3, 8, kernel_size=6, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.Conv1d(8, 8, kernel_size=6, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size=6, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Conv1d(8, 8, kernel_size=4, stride=2, padding=0), \n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "        )\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.LazyLinear(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, 64),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "#         self.basic_stack = nn.Sequential(\n",
    "#             nn.Linear(int(600/2)*1,512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "#             nn.Linear(512,1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "# #             nn.Linear(2048,1024),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Dropout(),\n",
    "#             nn.Linear(1024,512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128,1),\n",
    "#         )\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         logits = self.basic_stack(x)\n",
    "#         x = self.flatten(x)\n",
    "        logits = self.cnn_stack(x)\n",
    "        logits = self.flatten(logits)\n",
    "        logits = self.linear_stack(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn_mse(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)))\n",
    "\n",
    "def loss_fn_mspe(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)/y))\n",
    "\n",
    "def loss_fn_orig(y, pred):\n",
    "    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ae32ecc-6ec5-40b2-a608-93c7d231eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "316cdf05-8827-491b-805c-902be90598c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolatilityLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VolatilityLSTM, self).__init__()\n",
    "        # 16 hidden layer size, 3 reapeted blocks of LSTM, 4 input size\n",
    "        self.input_size = 3\n",
    "        self.hidden_size = 16\n",
    "        self.repeated_lstm_cells = 1\n",
    "        self.rnn = nn.LSTM(self.input_size, self.hidden_size, self.repeated_lstm_cells, batch_first=True, dropout=0)\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "#         input(\"GOT FEATURES\")\n",
    "        h_0 = torch.rand(self.repeated_lstm_cells, x.size(0), self.hidden_size) #hidden state\n",
    "        c_0 = torch.rand(self.repeated_lstm_cells, x.size(0), self.hidden_size) #internal state\n",
    "        h_0 = h_0.to(device)\n",
    "        c_0 = c_0.to(device)\n",
    "#         print(h_0.device, c_0.device, x.device)\n",
    "        output, (hn, cn) = self.rnn(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "#         input(\"---- output got\")\n",
    "        hn = hn.reshape(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "#         input(\"---- hn got\")\n",
    "        out = self.linear_stack(hn)\n",
    "#         input(\"--- out got\")\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1f24f-c586-4e1b-aad6-fb4671c87232",
   "metadata": {},
   "source": [
    "#### analyze the initial weights (or change them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "931890cf-e329-4a15-b0d0-352dc27f985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @torch.no_grad()\n",
    "# def init_weights(m):\n",
    "# #     print(m)\n",
    "#     if type(m) == nn.Linear:\n",
    "# #         m.weight.fill_(1.0)\n",
    "#         torch.nn.init.xavier_uniform_(m.weight,gain=10)\n",
    "#         m.bias.data.uniform_(-1,1)\n",
    "# #     elif type(m) == nn.ReLU:\n",
    "# #         print(m.data)\n",
    "#     else:\n",
    "#         print(type(m))\n",
    "# #         print(m.weight)\n",
    "# model.apply(init_weights)\n",
    "# # for param in model.parameters():\n",
    "# # #     print(param)\n",
    "# #       print(param.data.size(), param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c79a7d-9e08-4433-9c77-1af049f349d6",
   "metadata": {},
   "source": [
    "### LEarning rate: our base line is 0.34 loss as that's what the optiver guys have when they use current 10 min realize vol and use it as target (copy to prediction). We create simplest neural network and work with learning rates to figure out what's best and when we see something in range of 0.35 then we've found good Learning rate\n",
    "- #### SGD: 1e-7 works best\n",
    "- #### ADAM: 1e-5, (NOTE: 1e-3 makes it behave dumb where some deep local minima gets stuck and produces constant output!)\n",
    "- TODO: analyze that constant output phenomenon more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb3871f-5215-4afb-92fc-47ae2d385fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-4\n",
    "# batch_size = 4096\n",
    "# epochs = 100\n",
    "\n",
    "# input_scaling = 1\n",
    "# output_scaling = 1\n",
    "\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "# strategyname = \"ret1_n_ret2\"\n",
    "# summary_writer = SummaryWriter(f'../output/training_tensorboard/{strategyname}_scaleIn{input_scaling}Out{output_scaling}_{learning_rate}_{batch_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a7ed1-80f8-4a57-90b4-223345dde76b",
   "metadata": {},
   "source": [
    "### Learnings about training\n",
    "- (non scaling)logreturns input and volatility output; non scaled makes the model predict constant output with no variety(close to 0 std dev)\n",
    "- scaling input rids of variety issue, \n",
    "- scaling output makes the model start with low rmse initially so there's less ground to cover and we can iterate over ideas rapidly due to less epochs needed to achieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cef34a31-7317-47c0-a94f-9bebe2fd2e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "---------- logret1n2nt_rnn_scaleIn1000Out10000_0.001_256 ----------\n",
      "{'row_id': ['9-25999', '51-30225', '78-3580', '4-28596', '30-4132', '87-17973', '61-24816', '43-4487', '33-18247', '67-11367', '93-17157', '84-26956', '107-17639', '30-22304', '47-24707', '42-7679', '21-14016', '120-5424', '67-11994', '116-8773', '43-12726', '20-21148', '56-15516', '59-23768', '1-9465', '80-2102', '120-10163', '68-23819', '80-20826', '63-2136', '75-17875', '7-10547', '39-15516', '94-6316', '126-2174', '115-20551', '2-5795', '5-1350', '112-29577', '35-29494', '22-19810', '102-12713', '16-1135', '95-1600', '77-13071', '40-31359', '87-7674', '99-2385', '21-12531', '52-4091', '93-32174', '3-10688', '86-7548', '20-13928', '78-16649', '60-28256', '66-26455', '84-32767', '19-32280', '41-18447', '4-32009', '67-1322', '66-12751', '103-17259', '43-904', '39-21404', '80-2169', '86-2902', '14-11579', '51-18766', '5-14311', '32-7821', '23-25654', '0-16460', '111-16611', '39-25131', '55-1948', '80-18483', '69-20557', '58-19699', '58-21272', '44-32753', '113-436', '76-28426', '55-8205', '102-2929', '81-1205', '84-16454', '72-8186', '29-30256', '94-18020', '51-31944', '51-18972', '120-7566', '124-13941', '112-11243', '33-31836', '16-3661', '52-8360', '63-23683', '16-20563', '8-10827', '4-17305', '72-11497', '52-27526', '59-3720', '110-27127', '9-6979', '43-29252', '112-11641', '55-23144', '81-4746', '97-27581', '113-14632', '123-29003', '109-17949', '30-25993', '6-18495', '62-317', '108-28512', '90-6914', '99-11872', '19-31254', '64-2199', '75-18495', '42-4186', '96-14601', '126-26523', '75-7624', '61-14601', '62-18993', '86-4318', '13-21647', '41-9438', '36-3445', '33-5975', '74-14054', '52-17091', '28-2893', '124-18247', '43-9091', '30-27307', '113-18494', '13-3008', '34-22598', '109-16649', '44-11137', '59-27669', '89-1028', '124-6643', '102-16919', '120-32470', '78-1904', '7-19294', '67-16943', '124-26999', '113-32017', '28-6520', '38-7854', '78-2170', '87-21169', '44-20691', '5-10480', '53-32046', '34-15061', '62-29634', '41-24357', '114-22064', '67-1359', '40-1712', '14-6619', '115-6679', '46-7799', '29-12279', '16-21300', '69-19551', '86-14632', '124-13851', '18-20972', '116-30492', '5-20996', '94-27534', '97-8615', '48-30376', '52-19417', '76-19300', '32-12012', '31-9665', '88-17293', '29-12794', '72-2844', '9-27964', '67-31251', '40-28676', '111-27071', '47-28676', '41-8346', '27-23427', '13-28837', '8-23342', '109-109', '11-10291', '26-4219', '123-20785', '47-9072', '0-10985', '101-15291', '70-13163', '39-32249', '23-554', '109-27799', '101-9060', '75-31701', '33-32693', '61-31071', '62-697', '40-29150', '78-18203', '62-8708', '28-957', '125-18629', '103-22115', '125-5816', '107-20017', '0-6809', '101-19597', '14-3552', '40-211', '126-6076', '108-16320', '7-16941', '97-32053', '18-12465', '7-23751', '58-17528', '103-5297', '18-31836', '8-7043', '85-18996', '8-3033', '1-13421', '105-16637', '72-17966', '28-18764', '28-15529', '59-29374', '86-17911', '63-9076', '95-17634', '51-659', '6-32109', '34-18358', '40-5609', '100-23600', '124-5987', '59-17377'], 'logrett_1s': tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.9045e-05, 7.8979e-05,\n",
      "         1.5794e-04]]), 'logret1_1s': tensor([[ 9.3289e-04,  9.3289e-04,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -7.4718e-04,  ...,  1.3027e-03,\n",
      "         -1.3162e-03,  0.0000e+00],\n",
      "        ...,\n",
      "        [-1.8988e-04, -1.8988e-04,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-2.2842e-05, -2.2842e-05, -2.9720e-05,  ..., -5.4405e-05,\n",
      "          1.5965e-05, -1.1992e-05],\n",
      "        [-1.8619e-04, -1.8619e-04,  1.1386e-04,  ...,  1.1655e-04,\n",
      "          1.3168e-04,  3.0155e-04]]), 'logret2_1s': tensor([[-1.7262e-04, -1.7262e-04,  7.2863e-04,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.1286e-03,  1.1286e-03,  1.4719e-04,  ..., -5.9817e-05,\n",
      "         -2.8268e-04,  0.0000e+00],\n",
      "        ...,\n",
      "        [-8.2445e-05, -8.2445e-05,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 2.8134e-05,  2.8134e-05, -7.9326e-06,  ..., -8.8453e-06,\n",
      "          4.1819e-06, -4.1819e-06],\n",
      "        [-4.0442e-06, -4.0442e-06, -1.4996e-04,  ...,  5.9288e-05,\n",
      "          9.7330e-05,  1.5368e-04]])}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23428/3314389925.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mfeature_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFeature_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23428/2192192107.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GOT FEATURES\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mh_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeated_lstm_cells\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mc_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeated_lstm_cells\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#internal state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         )\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "strategyname = \"logret1n2nt_rnn\"\n",
    "\n",
    "training_configs = []\n",
    "learning_rates_to_try = [1e-3]\n",
    "batch_sizes_to_try = [256]#,10000, 128]\n",
    "input_scalings_to_try = [1000]\n",
    "output_scalings_to_try = [10000]\n",
    "for learning_rate in learning_rates_to_try:\n",
    "    for batch_size in batch_sizes_to_try:\n",
    "        for input_scaling in input_scalings_to_try:\n",
    "            for output_scaling in output_scalings_to_try:\n",
    "                    training_configs.append({\n",
    "                        'learning_rate':learning_rate,\n",
    "                        'batch_size':batch_size,\n",
    "                        'input_scaling':input_scaling,\n",
    "                        'output_scaling':output_scaling,\n",
    "                    })\n",
    "\n",
    "epochs = 200\n",
    "for training_config in training_configs:\n",
    "    \n",
    "    learning_rate = training_config['learning_rate']\n",
    "    batch_size = training_config['batch_size']\n",
    "    input_scaling = training_config['input_scaling']\n",
    "    output_scaling = training_config['output_scaling']\n",
    "    # TRAINING SETUP\n",
    "    \n",
    "    #refresh the model\n",
    "#     model = NeuralNetwork()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    model = VolatilityLSTM()\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "    \n",
    "    STRATEGY_NAME_WITH_ATTRS = f\"{strategyname}_scaleIn{input_scaling}Out{output_scaling}_{learning_rate}_{batch_size}\"\n",
    "    summary_writer = SummaryWriter(f'../output/training_tensorboard/{STRATEGY_NAME_WITH_ATTRS}')\n",
    "    \n",
    "    # TRAINING SETUP DONE\n",
    "    \n",
    "    print(\"DEVICE:\", device)\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    test_size = dataset_size - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    data_interval_len = int(600/1)\n",
    "    data_ohlc_sample_len = 1 # 1 for each of open high low close\n",
    "    losses_train = []\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        print(\"----------\", STRATEGY_NAME_WITH_ATTRS, \"----------\")\n",
    "\n",
    "        dataloader_train = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=0, pin_memory=True)\n",
    "        model.train()\n",
    "        \n",
    "        for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "\n",
    "            X = torch.stack([Feature_X['logret1_1s']*input_scaling, \n",
    "                           Feature_X['logret1_1s']*input_scaling,\n",
    "                          Feature_X['logret1_1s']*input_scaling,\n",
    "                          ], dim=2)\n",
    "\n",
    "            X = X.reshape(-1,data_interval_len,3)\n",
    "\n",
    "            y = feature_y * output_scaling\n",
    "\n",
    "\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            feature_y = feature_y.to(device)\n",
    "            pred = model(Feature_X)\n",
    "            pred.to(device)\n",
    "\n",
    "            loss_orig = loss_fn_orig(y, pred)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_orig.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            losses_train.append(loss_orig.item())\n",
    "            # we want 5 spread out output per epoch\n",
    "            if (t*int(train_size/batch_size) + train_batch_idx + 1) % int(train_size/5/batch_size) == 0:\n",
    "                \n",
    "                # NOTE: real loss is same as upscaled normalized loss as it's percentage loss (rmspe)\n",
    "                prediction_variety = np.std((pred/output_scaling).reshape(-1).tolist()) * 100\n",
    "                #NOTE: prediction variety is important as model sometimes predits a constant value! regardless of the input, then per batch variety is lowest(0 std dev)\n",
    "#                 print(\"prediction variety\",)\n",
    "#                 print(pred.reshape(-1).tolist()[:7])\n",
    "                \n",
    "                summary_writer.add_scalar(\"Prediction Variety\", prediction_variety, t*(train_size) + (train_batch_idx*batch_size))\n",
    "                summary_writer.add_scalar(\"Training Loss\", np.mean(losses_train), t*(train_size) + (train_batch_idx*batch_size))\n",
    "\n",
    "                print(\"train:\", np.mean(losses_train), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\")\n",
    "                losses_train = []\n",
    "                \n",
    "        dataloader_test = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                shuffle=True, num_workers=0, pin_memory=True)\n",
    "        dataset_size = len(dataloader_test.dataset)\n",
    "        model.eval()\n",
    "\n",
    "        losses_test = []\n",
    "        for _, (Feature_X, feature_y) in enumerate(dataloader_test):\n",
    "            with torch.no_grad():\n",
    "\n",
    "#                 X = torch.cat([Feature_X['logret1_1s']*input_scaling, \n",
    "#                            Feature_X['logret1_1s']*input_scaling,\n",
    "#                           Feature_X['logret1_1s']*input_scaling,\n",
    "#                           ], 1)\n",
    "\n",
    "#                 X = X.reshape(-1,3,data_interval_len*data_ohlc_sample_len*1)\n",
    "                X = torch.stack([Feature_X['logret1_1s']*input_scaling, \n",
    "                           Feature_X['logret1_1s']*input_scaling,\n",
    "                          Feature_X['logret1_1s']*input_scaling,\n",
    "                          ], dim=2)\n",
    "\n",
    "                X = X.reshape(-1,data_interval_len,3)\n",
    "                y = feature_y * output_scaling\n",
    "\n",
    "                X = X.type(torch.cuda.FloatTensor)\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                pred = model(X)\n",
    "                loss = loss_fn_orig(y, pred)\n",
    "                losses_test.append(loss.item())\n",
    "\n",
    "\n",
    "#                 summary_writer.add_scalar(\"Epoch Training Loss\", np.mean(losses_train), (t+1)*train_size)\n",
    "        summary_writer.add_scalar(\"Test Loss\", np.mean(losses_test), t*(train_size) + (train_batch_idx*batch_size))\n",
    "        print(\"train:\", np.mean(losses_train), \"test:\", np.mean(losses_test), f\"[{train_batch_idx*batch_size:>5d}/{train_size:>5d}]\")\n",
    "        losses_test = []\n",
    "        if (t+1)%50==0:\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIRECTORY,f\"{STRATEGY_NAME_WITH_ATTRS}_epoch_{t}_tloss_{loss:.4f}.pth\"))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f5760-8926-4064-8bf2-bddd62c80bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cea68b-9cf2-4a6c-8fab-d0e6b8f7654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f03435-5cde-40bf-a86b-656ab4515ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce561c-5b64-44bf-882b-41f6250b1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated(device)/1024/1024/1024\n",
    "# model.to(\"cpu\")\n",
    "# torch.cuda.memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a966ee2c-8547-48be-a912-8d05bf48b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.init()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
