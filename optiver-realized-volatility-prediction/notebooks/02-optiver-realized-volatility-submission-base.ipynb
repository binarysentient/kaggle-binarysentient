{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from optiver_features_handler import get_features_map_for_stock, get_row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\n",
    "TRADE_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_train.parquet\")\n",
    "TRADE_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_test.parquet\")\n",
    "BOOK_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_train.parquet\")\n",
    "BOOK_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_test.parquet\")\n",
    "OUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\n",
    "os.makedirs(OUTPUT_DIRECTORY,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIRECTORY,\"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIRECTORY,\"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interval_seconds = 5\n",
    "data_intervals_count = int(600/data_interval_seconds)\n",
    "class OptiverRealizedVolatilityDataset(Dataset):\n",
    "    def __init__(self, data_directory, mode=\"train\", lazy_load=True):\n",
    "        \"\"\"initializes Optiver Competition dataset\n",
    "        `mode`: train|test\n",
    "        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n",
    "        \"\"\"\n",
    "        print(\"INIT: OptiverRealizedVolatilityDataset\")\n",
    "        if mode.lower() not in ['train','test']:\n",
    "            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n",
    "        self.data_directory = data_directory\n",
    "        self.mode = mode.lower()\n",
    "        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n",
    "#         if self.mode == 'train':\n",
    "#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n",
    "        if self.mode == 'test':\n",
    "            self.main_df['target'] = 0\n",
    "        \n",
    "        self.cache_stocks_done_set = set()\n",
    "        # this is our final features lookup where we park all our features which can be addressed by row_id\n",
    "        # which is individual train/test.csv row id using 'stock_id`-`time_id`\n",
    "        self.cache_rowid_feature_map = {}\n",
    "        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n",
    "        targets = self.main_df['target'].tolist()\n",
    "        self.stock_possible_timeids_list = {}\n",
    "        for idx, row_id in enumerate(row_id_series.tolist()):\n",
    "            stock_id = int(row_id.split('-')[0])\n",
    "            time_id = int(row_id.split('-')[1])\n",
    "            self.cache_rowid_feature_map[row_id] = {'target':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n",
    "            \n",
    "            # below code is to make sure what timeids we expect from stock data extractor\n",
    "            # in case of missing parquet files we'll have to know the keys to fill default values into\n",
    "            if stock_id not in self.stock_possible_timeids_list:\n",
    "                self.stock_possible_timeids_list[stock_id] = []\n",
    "            self.stock_possible_timeids_list[stock_id].append(time_id)\n",
    "            \n",
    "        \n",
    "        if lazy_load == False:\n",
    "            worker_data = []\n",
    "            for gkey, gdf in self.main_df.groupby(['stock_id']):\n",
    "                worker_data.append((self.data_directory, self.mode, gkey))\n",
    "#             print(\"---------- CPU COUNG:\", multiprocessing.cpu_count())\n",
    "            # NOTE: this was hell of a hunt; this windows and pytorch and jupyter combination is too tedious\n",
    "            #       make sure the function that we distribute don't call pytorch\n",
    "            chunksize = multiprocessing.cpu_count() * 1\n",
    "            processed = 0\n",
    "            for worker_data_chunk in [worker_data[i * chunksize:(i + 1) * chunksize] for i in range((len(worker_data) + chunksize - 1) // chunksize )]:\n",
    "                with Pool(multiprocessing.cpu_count()) as p:\n",
    "                    \n",
    "                    feature_set_list = p.starmap(get_features_map_for_stock, worker_data_chunk)\n",
    "                    \n",
    "                    for feature_map in feature_set_list:\n",
    "                        for rowid, features_dict in feature_map.items():\n",
    "                            for fkey,fval in features_dict.items():\n",
    "                                self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                            self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                        # udpate the indications that we've already fetched this stock and the lazy loader code won't fetch this again\n",
    "                        self.cache_stocks_done_set.add(int(rowid.split('-')[0]))\n",
    "                    \n",
    "                    processed += chunksize\n",
    "                    print(f\"Processed and loaded {processed} stocks features.\")\n",
    "    \n",
    "    def __cache_generate_features(self, main_stock_id, main_time_id):\n",
    "            \n",
    "            main_row_id = get_row_id(main_stock_id, main_time_id)\n",
    "            if main_stock_id not in self.cache_stocks_done_set:\n",
    "#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))   \n",
    "                # we'll combine the featureset with the bigger feature set of all stocks\n",
    "                feature_map = get_features_map_for_stock(self.data_directory, self.mode, main_stock_id)\n",
    "                # NOTE: sometime we might now have parquet files in that case we'll have 3 entried in .csv while only 1 gets returned in feature map\n",
    "                # we need to cover for that disparity\n",
    "                for time_id in self.stock_possible_timeids_list[main_stock_id]:\n",
    "                    expected_row_id = get_row_id(main_stock_id, time_id)\n",
    "                    if expected_row_id not in feature_map:\n",
    "                        feature_map[expected_row_id] = {}\n",
    "                for rowid, features_dict in feature_map.items():\n",
    "                    for fkey,fval in features_dict.items():\n",
    "                        self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                    self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                self.cache_stocks_done_set.add(main_stock_id)\n",
    "#             print(self.cache_rowid_feature_map[main_row_id])\n",
    "#             print(torch.tensor([self.cache_rowid_feature_map[main_row_id].get('book_realized_volatility',0)]))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map[main_row_id].get('log_return1_2s', [0]*(int(600/2)))))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map.get('book_directional_volume1_2s', [0]*(int(600/2)))))\n",
    "            return self.cache_rowid_feature_map[main_row_id]\n",
    "        \n",
    "    @staticmethod\n",
    "    def transform_to_01_realized_volatility_linear_data(features_dict):\n",
    "        return (\n",
    "                {\n",
    "                    'row_id':features_dict['row_id'],\n",
    "                    'stock_id':torch.tensor(features_dict['stock_id'], dtype=torch.float32),\n",
    "                    'seconds_in_bucket_xs': torch.tensor(np.nan_to_num(features_dict.get('seconds_in_bucket_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "#                     'book_realized_volatility':torch.tensor([features_dict.get('book_realized_volatility',0)]),\n",
    "                    # TRADE FEATURES\n",
    "                    'logrett_xs': torch.tensor(np.nan_to_num(features_dict.get('logrett_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_volume_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_volume_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_ordercount_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_ordercount_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_money_turnover_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_money_turnover_per_order_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_per_order_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    \n",
    "#                     'trade_money_turnover_mean': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_mean', 0)), dtype=torch.float32),\n",
    "#                     'trade_money_turnover_std': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_std', 0)), dtype=torch.float32),\n",
    "                    # BOOK FEATURES\n",
    "                    'logret1_xs': torch.tensor(np.nan_to_num(features_dict.get('logret1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'logret2_xs': torch.tensor(np.nan_to_num(features_dict.get('logret2_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_directional_volume1_xs': torch.tensor(np.nan_to_num(features_dict.get('book_directional_volume1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_directional_volume2_xs': torch.tensor(np.nan_to_num(features_dict.get('book_directional_volume2_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_price_spread1_xs': torch.tensor(np.nan_to_num(features_dict.get('book_price_spread1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_price_spread2_xs': torch.tensor(np.nan_to_num(features_dict.get('book_price_spread2_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_bid_spread_xs': torch.tensor(np.nan_to_num(features_dict.get('book_bid_spread_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_ask_spread_xs': torch.tensor(np.nan_to_num(features_dict.get('book_ask_spread_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_total_volume_xs': torch.tensor(np.nan_to_num(features_dict.get('book_total_volume_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_volume_imbalance_xs': torch.tensor(np.nan_to_num(features_dict.get('book_volume_imbalance_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_money_turnover1_xs': torch.tensor(np.nan_to_num(features_dict.get('book_money_turnover1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    \n",
    "#                     'askp2_1s':torch.tensor(features_dict.get('askp2_1s', [0]*(int(600/1)))),\n",
    "#                     'book_directional_volume1_1s':torch.tensor(features_dict.get('book_directional_volume1_1s', [0]*(int(600/1)))) \n",
    "                },\n",
    "                torch.tensor([features_dict['target']])\n",
    "#                 [features_dict['target']]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: handle for num_workers more than 0\n",
    "        #      using https://pytorch.org/docs/stable/data.html\n",
    "        #      using torch.util.data.get_worker_info()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        stock_id = self.main_df.at[idx, 'stock_id']\n",
    "        time_id = self.main_df.at[idx, 'time_id']\n",
    "        x,y = self.__cache_generate_features(stock_id,time_id)\n",
    "#         x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT: OptiverRealizedVolatilityDataset\n"
     ]
    }
   ],
   "source": [
    "# dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\")\n",
    "dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"test\", lazy_load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'row_id': '0-4',\n",
       "  'stock_id': tensor(0.),\n",
       "  'seconds_in_bucket_xs': tensor([  5.,  10.,  15.,  20.,  25.,  30.,  35.,  40.,  45.,  50.,  55.,  60.,\n",
       "           65.,  70.,  75.,  80.,  85.,  90.,  95., 100., 105., 110., 115., 120.,\n",
       "          125., 130., 135., 140., 145., 150., 155., 160., 165., 170., 175., 180.,\n",
       "          185., 190., 195., 200., 205., 210., 215., 220., 225., 230., 235., 240.,\n",
       "          245., 250., 255., 260., 265., 270., 275., 280., 285., 290., 295., 300.,\n",
       "          305., 310., 315., 320., 325., 330., 335., 340., 345., 350., 355., 360.,\n",
       "          365., 370., 375., 380., 385., 390., 395., 400., 405., 410., 415., 420.,\n",
       "          425., 430., 435., 440., 445., 450., 455., 460., 465., 470., 475., 480.,\n",
       "          485., 490., 495., 500., 505., 510., 515., 520., 525., 530., 535., 540.,\n",
       "          545., 550., 555., 560., 565., 570., 575., 580., 585., 590., 595., 600.]),\n",
       "  'logrett_xs': tensor([ 0.0000e+00,  0.0000e+00, -9.8319e-05, -9.8328e-05, -9.8338e-05,\n",
       "           9.8938e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]),\n",
       "  'trade_volume_xs': tensor([  0.,   1.,   0.,   0., 100., 100.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  'trade_ordercount_xs': tensor([0., 1., 0., 0., 7., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'trade_money_turnover_xs': tensor([  0.0000,   1.0003,   0.0000,   0.0000, 100.0049, 100.0059,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000]),\n",
       "  'trade_money_turnover_per_order_xs': tensor([ 0.0000,  1.0003,  0.0000,  0.0000, 14.2864, 33.3353,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]),\n",
       "  'logret1_xs': tensor([0.0003, 0.0003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000]),\n",
       "  'logret2_xs': tensor([0.0003, 0.0003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000]),\n",
       "  'book_directional_volume1_xs': tensor([   9., -270.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]),\n",
       "  'book_directional_volume2_xs': tensor([-78., -86.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  'book_price_spread1_xs': tensor([0.0005, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006]),\n",
       "  'book_price_spread2_xs': tensor([0.0010, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012]),\n",
       "  'book_bid_spread_xs': tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004]),\n",
       "  'book_ask_spread_xs': tensor([4.9203e-05, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04]),\n",
       "  'book_total_volume_xs': tensor([626., 426.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  'book_volume_imbalance_xs': tensor([ 69., 356.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  'book_money_turnover1_xs': tensor([182.0558,  20.0120,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000])},\n",
       " tensor([0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = None\n",
    "\n",
    "\n",
    "def loss_fn_mse(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)))\n",
    "\n",
    "def loss_fn_mspe(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)/y))\n",
    "\n",
    "def loss_fn_orig(y, pred):\n",
    "    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, feature_generator_mode_hidden_size=64, mode='both', use_stock_id = False):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.use_stock_id = use_stock_id\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mode = mode\n",
    "        self.feature_generator_mode_hidden_size = feature_generator_mode_hidden_size\n",
    "        self.cnn_stack = nn.Sequential(\n",
    "            nn.Conv1d(10, 16, kernel_size=4, stride=2, padding=0),\n",
    "            nn.GELU(),\n",
    "#             nn.BatchNorm1d(4),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.Conv1d(16, 24, kernel_size=4, stride=2, padding=0),\n",
    "            nn.GELU(),\n",
    "#             nn.Conv1d(16, 24, kernel_size=2, stride=1, padding=0),\n",
    "#             nn.GELU(),\n",
    "#             nn.Conv1d(24, 32, kernel_size=2, stride=1, padding=0),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(8),\n",
    "#             nn.Conv1d(4, 4, kernel_size=6, stride=3, padding=0),\n",
    "#             nn.GELU(),\n",
    "#             nn.Conv1d(4, 4, kernel_size=4, stride=2, padding=0),\n",
    "#             nn.GELU(),\n",
    "#             nn.BatchNorm1d(4),\n",
    "#             nn.Conv1d(4, 4, kernel_size=6, stride=2, padding=0),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Conv1d(8, 8, kernel_size=4, stride=2, padding=0), \n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "        )\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(672,512),\n",
    "            nn.GELU(),\n",
    "#             nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(512, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, self.feature_generator_mode_hidden_size),\n",
    "            nn.GELU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 16),\n",
    "#             nn.ReLU()\n",
    "        )\n",
    "        self.linear_hybrid = nn.Sequential(\n",
    "            nn.Linear(self.feature_generator_mode_hidden_size, 32),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(256, 256),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "#         self.basic_stack = nn.Sequential(\n",
    "#             nn.Linear(int(600/2)*1,512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "#             nn.Linear(512,1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "# #             nn.Linear(2048,1024),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Dropout(),\n",
    "#             nn.Linear(1024,512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128,1),\n",
    "#         )\n",
    "\n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return []\n",
    "    \n",
    "    def set_mode(self,mode):\n",
    "        self.mode = mode\n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "#         logits = self.basic_stack(x)\n",
    "#         x = self.flatten(x)\n",
    "        x = torch.cat([\n",
    "                            feature_dict['logrett_xs'].to(device)*10000, \n",
    "#                                torch.log(feature_dict['trade_volume_xs'].to(device)+0.001),\n",
    "#                               torch.log(feature_dict['trade_ordercount_xs'].to(device)+0.001),\n",
    "#                             feature_dict['trade_volume_xs'].to(device),\n",
    "#                                         feature_dict['trade_ordercount_xs'].to(device),\n",
    "#                                         feature_dict['book_total_volume_xs'].to(device),\n",
    "#                                         feature_dict['book_volume_imbalance_xs'].to(device)\n",
    "#                             feature_dict['logrett_xs'].to(device)*10000, \n",
    "#                                torch.log(feature_dict['trade_volume_xs'].to(device)+0.001),\n",
    "#                               torch.log(feature_dict['trade_ordercount_xs'].to(device)+0.001),\n",
    "            \n",
    "                                feature_dict['logret1_xs'].to(device)*10000,\n",
    "                            \n",
    "                                 feature_dict['logret2_xs'].to(device)*10000,\n",
    "                                    feature_dict['book_price_spread1_xs'].to(device)*10000, \n",
    "#                                 feature_dict['book_price_spread2_xs'].to(device)*10000, \n",
    "                                feature_dict['book_bid_spread_xs'].to(device)*10000, \n",
    "                                feature_dict['book_ask_spread_xs'].to(device)*10000, \n",
    "#                                  feature_dict['book_directional_volume1_xs'].to(device),\n",
    "#                                 feature_dict['book_price_spread1_xs'].to(device)*1000,\n",
    "                                torch.log(feature_dict['book_total_volume_xs'].to(device)+1),\n",
    "                                torch.log(feature_dict['book_volume_imbalance_xs'].to(device)+1),\n",
    "                                torch.log(feature_dict['book_money_turnover1_xs'].to(device)+1),\n",
    "                                torch.log(feature_dict['trade_money_turnover_xs'].to(device)+1),\n",
    "# #                              feature_dict['book_dirvolume_xs'],\n",
    "                          ], 1)\n",
    "\n",
    "#         x = torch.nan_to_num(feature_dict['logrett_xs']).type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        \n",
    "#         print(x)\n",
    "#         input()\n",
    "#         if torch.isnan(x).any():\n",
    "# #             print(x)\n",
    "#             print(feature_dict)\n",
    "#             input()\n",
    "        x = x.to(device)\n",
    "        x = x.reshape(-1,10,data_intervals_count)\n",
    "        \n",
    "        logits = self.cnn_stack(x)\n",
    "        logits = self.flatten(logits)\n",
    "        \n",
    "       \n",
    "        \n",
    "        logits = self.linear_stack(logits)\n",
    "        \n",
    "        if self.mode == 'hidden_generator':\n",
    "            return logits\n",
    "#         logits = torch.cat( [logits, \n",
    "#                              torch.log(feature_dict['trade_money_turnover_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001), \n",
    "#                                            torch.log(feature_dict['trade_money_turnover_std'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                            torch.log(feature_dict['trade_price_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                            torch.log(feature_dict['book_money_turnover_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                            torch.log(feature_dict['book_money_turnover_std'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                            torch.log(feature_dict['book_price_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001)\n",
    "#                                       ], 1)\n",
    "        \n",
    "        if self.use_stock_id:\n",
    "            stock_id = torch.tensor(feature_dict['stock_id']).reshape(-1,1)\n",
    "            stock_id = stock_id.to(device)\n",
    "            logits = torch.cat([logits, stock_id], 1)\n",
    "            \n",
    "        logits = self.linear_hybrid(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VolatilityGRU(nn.Module):\n",
    "#     def __init__(self, input_size=1, hidden_size=64, repeated_cells=1):\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "class SingleFetGRU(nn.Module):\n",
    "    def __init__(self, hidden_size=64, layers=1, dropout=0, features_out=32, mode=\"train\"):\n",
    "        \"\"\"single feature, feature learner\n",
    "        `mode`: train|feature_generator\n",
    "        \"\"\"\n",
    "        super(SingleFetGRU, self).__init__()\n",
    "        self.input_size_ = 1\n",
    "        self.hidden_size_ = hidden_size\n",
    "        self.repeated_lstm_cells_ = layers\n",
    "        self.dropout_ = dropout\n",
    "        self.features_out = features_out\n",
    "        \n",
    "        self.rnn_ = nn.GRU(self.input_size_, self.hidden_size_, self.repeated_lstm_cells_, batch_first=True, dropout=self.dropout_)\n",
    "        \n",
    "        self.linear_feature_stack_ = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size_*self.repeated_lstm_cells_, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, self.features_out),\n",
    "        )\n",
    "        \n",
    "        self.linear_trainer_stack_ = nn.Sequential(\n",
    "            nn.Linear(self.features_out, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 1),   \n",
    "        )\n",
    "        \n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def forward(self, feature_tensor, h0_tensor=None):\n",
    "        if self.mode in [\"feature_generator\",\"train\"]:\n",
    "            if h0_tensor is None:\n",
    "                h_0_ = torch.rand(self.repeated_lstm_cells_, feature_tensor.size(0), self.hidden_size_, device=device) #hidden state\n",
    "            else:\n",
    "                h_0_ = h0_tensor\n",
    "            output_, hn_ = self.rnn_(feature_tensor, h_0_) #lstm with input, hidden, and internal state\n",
    "            hn_ = hn_.reshape(-1, self.hidden_size_*self.repeated_lstm_cells_) #reshaping the data for Dense layer next  \n",
    "            \n",
    "            out_ = self.linear_feature_stack_(hn_)\n",
    "            \n",
    "            if self.mode == \"train\":\n",
    "                out_ = self.linear_trainer_stack_(out_)\n",
    "            \n",
    "            return out_\n",
    "            \n",
    "            \n",
    "            \n",
    "class VolatilityBSModel(nn.Module):\n",
    "    def __init__(self, mode=\"hybrid\", use_stock_id=False):\n",
    "        \"\"\"various rnn features' fusion with fully connected nn\n",
    "        `mode`: hybrid|<feature_name>\n",
    "        \"\"\"\n",
    "        super(VolatilityBSModel, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.use_stock_id = use_stock_id\n",
    "#         self.feature_list = ['logrett_xs','trade_volume_xs','trade_ordercount_xs','trade_money_turnover_xs','trade_money_turnover_per_order_xs',\n",
    "#                              'logret1_xs','logret2_xs','book_directional_volume1_xs','book_directional_volume2_xs',\n",
    "#                              'book_price_spread1_xs','book_price_spread2_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "#                              'book_total_volume_xs','book_volume_imbalance_xs','book_money_turnover1_xs']\n",
    "        self.feature_list = ['logrett_xs','trade_volume_xs','trade_ordercount_xs','trade_money_turnover_xs','trade_money_turnover_per_order_xs',\n",
    "                             'logret1_xs','logret2_xs','book_directional_volume1_xs',\n",
    "                             'book_price_spread1_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "                             'book_total_volume_xs','book_volume_imbalance_xs']\n",
    "        self.feature_gen_feature_size = 32\n",
    "        self.feature_gen_models = {}\n",
    "        self.rnn_hidden_size = 64\n",
    "        self.rnn_layers = 2\n",
    "        self.hidden_generator_network = NeuralNetwork(feature_generator_mode_hidden_size=self.rnn_hidden_size*self.rnn_layers)\n",
    "        \n",
    "        for k in self.feature_list:\n",
    "            self.feature_gen_models[k]=SingleFetGRU(hidden_size=self.rnn_hidden_size, layers=self.rnn_layers, dropout=0.2, features_out=self.feature_gen_feature_size) \n",
    "            self.feature_gen_models[k].to(device)\n",
    "        \n",
    "        \n",
    "        self.linear_fusion = nn.Sequential(\n",
    "            nn.Linear(self.feature_gen_feature_size*len(self.feature_list) + (1 if self.use_stock_id else 0), 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "        self.set_mode(self.mode)\n",
    "    \n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return self.feature_list\n",
    "    \n",
    "    def set_mode(self, mode):\n",
    "        print(f\"------- set mode : {mode} -----------\")\n",
    "        self.mode = mode\n",
    "        for feature_gen_model in self.feature_gen_models.values():\n",
    "            feature_gen_model.set_mode('feature_generator' if self.mode in ['hybrid','hybrid_feature_out','hidden_generator'] else 'train')\n",
    "        if self.mode == 'hidden_generator':\n",
    "            self.hidden_generator_network.set_mode('train')\n",
    "        else:\n",
    "            self.hidden_generator_network.set_mode('hidden_generator')\n",
    "    \n",
    "    def parameters(self):\n",
    "        \n",
    "        generator_sources_map = {k:[v] for k,v in self.feature_gen_models.items()}\n",
    "        generator_sources_map['hybrid']= [self.linear_fusion]\n",
    "        generator_sources_map['hidden_generator'] = [self.hidden_generator_network]\n",
    "        params = []\n",
    "        if self.mode in generator_sources_map:\n",
    "            for generator_source in generator_sources_map[self.mode]:\n",
    "                for param in generator_source.parameters():\n",
    "                    params.append(param)\n",
    "        else:\n",
    "            return super(VolatilityBSModel,self).parameters()\n",
    "        return params\n",
    "    \n",
    "    def feature_transform(self, feature_x, feature_name):\n",
    "        if feature_name in ['logrett_xs',\n",
    "                             'logret1_xs','logret2_xs',\n",
    "                             'book_price_spread1_xs','book_price_spread2_xs','book_bid_spread_xs','book_ask_spread_xs']:\n",
    "            return feature_x * 10000\n",
    "        if feature_name in ['trade_ordercount_xs','trade_volume_xs','trade_money_turnover_xs','trade_money_turnover_per_order_xs',\n",
    "                             'book_total_volume_xs','book_volume_imbalance_xs','book_money_turnover1_xs']:\n",
    "            return torch.log(feature_x + 1)\n",
    "        return feature_x\n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "        \n",
    "        if self.mode in ['hidden_generator']:\n",
    "            return self.hidden_generator_network(feature_dict)\n",
    "        \n",
    "        h0_tensor = self.hidden_generator_network(feature_dict)\n",
    "        h0_tensor = h0_tensor.reshape(self.rnn_layers,-1,self.rnn_hidden_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.mode in self.feature_list:\n",
    "            feature_x = feature_dict[self.mode].to(device).reshape(-1, data_intervals_count ,1)\n",
    "            feature_x = self.feature_transform(feature_x, self.mode)\n",
    "            \n",
    "            # pass in some randomness to the initial hidden tensor to force it to learn some stuff on its own\n",
    "            # otherwise as the initial hidden layer contains solid infor to minimize the loss; it'll just use that hidden layer to minimize and instead\n",
    "            # learn to not learn and directly bypass initial hidden\n",
    "            h0_tensor.masked_fill_((torch.rand(h0_tensor.size()) > 0.5).to(device), 0.0)\n",
    "            \n",
    "            out = self.feature_gen_models[self.mode](feature_x, h0_tensor=h0_tensor)\n",
    "            return out\n",
    "        \n",
    "        if self.mode in ['hybrid','hybrid_feature_out']:\n",
    "            generated_features = []\n",
    "            for feature_name, feature_gen_model in self.feature_gen_models.items():\n",
    "                feature_x = feature_dict[feature_name].to(device).reshape(-1, data_intervals_count ,1)\n",
    "                feature_x = self.feature_transform(feature_x, feature_name)\n",
    "                features_out = feature_gen_model(feature_x, h0_tensor=h0_tensor)\n",
    "                generated_features.append(features_out)\n",
    "                \n",
    "                \n",
    "            combined_features = torch.cat(generated_features, 1)#.reshape(-1, self.feature_gen_feature_size*len(self.feature_list))\n",
    "            \n",
    "            if self.use_stock_id:\n",
    "                stock_id = feature_dict['stock_id'].to(device).reshape(-1,1)\n",
    "                combined_features = torch.cat([combined_features, stock_id], 1)\n",
    "                \n",
    "            if self.mode == 'hybrid_feature_out':\n",
    "                return combined_features\n",
    "            \n",
    "            out = self.linear_fusion(combined_features)\n",
    "            return out\n",
    "        \n",
    "#         input(\"--- out got\")\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- set mode : hybrid -----------\n",
      "------- set mode : hybrid -----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VolatilityBSModel(\n",
       "  (hidden_generator_network): NeuralNetwork(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (cnn_stack): Sequential(\n",
       "      (0): Conv1d(10, 16, kernel_size=(4,), stride=(2,))\n",
       "      (1): GELU()\n",
       "      (2): Conv1d(16, 24, kernel_size=(4,), stride=(2,))\n",
       "      (3): GELU()\n",
       "    )\n",
       "    (linear_stack): Sequential(\n",
       "      (0): Linear(in_features=672, out_features=512, bias=True)\n",
       "      (1): GELU()\n",
       "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (5): GELU()\n",
       "    )\n",
       "    (linear_hybrid): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (linear_fusion): Sequential(\n",
       "    (0): Linear(in_features=416, out_features=512, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): GELU()\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): GELU()\n",
       "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (7): GELU()\n",
       "    (8): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VolatilityBSModel(use_stock_id=False)\n",
    "model.set_mode('hybrid')\n",
    "modelpath = \"../input/optiver-realized-volatility-binarysentient-pytorch/07_1s_logret1n2_cnn_epoch_400_tloss_0.2393.pth\"\n",
    "modelpath = \"../output/models/16_ultimate_hybridEXP14_CNN_5s_StkFalse_1e-05_4_epoch_57_tloss_0.2282.pth\"\n",
    "checkpoint = torch.load(modelpath)\n",
    "model.load_state_dict(checkpoint['base'])\n",
    "for k,v in model.feature_gen_models.items():\n",
    "    v.load_state_dict(checkpoint[k])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=3,\n",
    "                        shuffle=True, num_workers=0, pin_memory=True)\n",
    "size = len(dataloader.dataset)\n",
    "submission_data = []\n",
    "data_interval_len = int(600/1)\n",
    "output_scaling = 10000\n",
    "data_ohlc_sample_len = 1 # 1 for each of open high low close\n",
    "for batch, (Feature_X, feature_y) in enumerate(dataloader):\n",
    "    row_ids = Feature_X['row_id']\n",
    "    y = feature_y.to(device) * output_scaling \n",
    "    \n",
    "    pred = model(Feature_X) \n",
    "#     print(pred)\n",
    "    predicted_volatility = (pred/output_scaling).tolist()\n",
    "    for idx, row_id in enumerate(row_ids):\n",
    "        submission_data.append({'row_id':row_id, 'target':predicted_volatility[idx][0]})\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df = dataset.main_df.merge(submission_df,on='row_id',how='left')\n",
    "submission_df = submission_df.rename(columns={'target_y':'target'})\n",
    "# submission_df\n",
    "# print(submission_df.columns)\n",
    "submission_df[['row_id','target']].to_csv(\"submission.csv\", index=False)\n",
    "# for idx, (X,y) in enumerate(dataset):\n",
    "#     print(idx, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.001254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.000647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.000531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.001254\n",
       "1   0-32  0.000647\n",
       "2   0-34  0.000531"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0+cu111'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
