{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import types\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "# from optiver_features_handler import get_features_map_for_stock, get_row_id, realized_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\n",
    "TRADE_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_train.parquet\")\n",
    "TRADE_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_test.parquet\")\n",
    "BOOK_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_train.parquet\")\n",
    "BOOK_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_test.parquet\")\n",
    "OUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\n",
    "os.makedirs(OUTPUT_DIRECTORY,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_row_id(stock_id, time_id):\n",
    "    if type(time_id) != int:\n",
    "        time_id = int(time_id)\n",
    "    return f\"{stock_id:.0f}-{time_id}\"\n",
    "    \n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "\n",
    "\n",
    "def get_features_map_for_stock(data_directory, mode, main_stock_id):\n",
    "        \"\"\"gets the `stock_id-row_id` wise feature map\n",
    "        `data_directory`: is where the train.csv and other parquet folders are present\n",
    "        `mode`: train|test\n",
    "        `main_stock_id`: the stock id! zlul\n",
    "        \"\"\"\n",
    "        interval_second = 6\n",
    "        intervals_count = 600//interval_second\n",
    "        \n",
    "        feature_map = {}\n",
    "        book_df = pd.read_parquet(os.path.join(data_directory, f\"book_{mode}.parquet\", f\"stock_id={main_stock_id}\"))\n",
    "        trade_df = pd.read_parquet(os.path.join(data_directory, f\"trade_{mode}.parquet\", f\"stock_id={main_stock_id}\"))\n",
    "        \n",
    "        book_df['wap1'] = (book_df['bid_price1'] * book_df['ask_size1'] + book_df['ask_price1'] * book_df['bid_size1'])/(book_df['bid_size1'] + book_df['ask_size1'])\n",
    "        book_df['logret1'] = np.log(book_df['wap1']).diff()\n",
    "        book_df['wap2'] = (book_df['bid_price2'] * book_df['ask_size2'] + book_df['ask_price2'] * book_df['bid_size2'])/(book_df['bid_size2'] + book_df['ask_size2'])\n",
    "        book_df['logret2'] = np.log(book_df['wap2']).diff()\n",
    "        book_df['wap_balance'] = abs(book_df['wap1'] - book_df['wap2'])\n",
    "        book_df['logret_bid_price1'] = np.log(book_df['bid_price1']).diff()\n",
    "        book_df['logret_ask_price1'] = np.log(book_df['ask_price1']).diff()\n",
    "        book_df['logret_bid_price2'] = np.log(book_df['bid_price2']).diff()\n",
    "        book_df['logret_ask_price2'] = np.log(book_df['ask_price2']).diff()\n",
    "        \n",
    "        book_df['price_spread1'] = (book_df['ask_price1'] - book_df['bid_price1']) / ((book_df['ask_price1'] + book_df['bid_price1'])/2)\n",
    "        book_df['bid_spread'] = abs(book_df['bid_price1'] - book_df['bid_price2']) / ((book_df['bid_price1'] + book_df['bid_price2'])/2)\n",
    "        book_df['ask_spread'] = abs(book_df['ask_price1'] - book_df['ask_price2']) / ((book_df['ask_price1'] + book_df['ask_price2'])/2)\n",
    "        book_df[\"bid_ask_spread\"] = abs(book_df['bid_spread'] - book_df['ask_spread'])\n",
    "        book_df['directional_volume1'] = (book_df['bid_size1'] - book_df['ask_size1'])\n",
    "        book_df['directional_volume2'] = (book_df['bid_size2'] - book_df['ask_size2'])\n",
    "        book_df['logret_directional_volume1'] = np.log(book_df['directional_volume1'] - book_df['directional_volume1'].min() + 1).diff()\n",
    "        book_df['logret_directional_volume2'] = np.log(book_df['directional_volume2'] - book_df['directional_volume2'].min() + 1).diff()\n",
    "        \n",
    "        book_df['total_volume'] = book_df['ask_size1'] + book_df['bid_size1'] + book_df['ask_size2'] + book_df['bid_size2']\n",
    "        book_df['volume_imbalance'] = abs(book_df['ask_size1'] - book_df['bid_size1'] + book_df['ask_size2'] - book_df['bid_size2'])\n",
    "        \n",
    "        trade_df['trade_money_turnover'] = trade_df['size'] * trade_df['price']\n",
    "        trade_df['logret_trade_money_turnover'] = np.log(trade_df['size'] * trade_df['price']).diff()\n",
    "        \n",
    "#         trade_df['trade_money_turnover_per_order'] = (trade_df['size'] * trade_df['price'] / trade_df['order_count'])\n",
    "        trade_df['logret_price'] = np.log(trade_df['price']).diff()\n",
    "#         trade_df['trade_tendancy'] = trade_df['logret_price'] * trade_df['size']\n",
    "        merged_df = book_df.merge(trade_df,how='left',on=['time_id','seconds_in_bucket']).reset_index(drop=False)\n",
    "        \n",
    "        merged_df['nwap1'] = (merged_df['ask_price1'] + merged_df['bid_price1'])/2\n",
    "        merged_df['trade_price_push_on_book'] = (merged_df['price'] - merged_df['nwap1'])/(merged_df['price'] + merged_df['nwap1'])/2\n",
    "#         merged_df['trade_volume_on_book'] = (merged_df['size']/(merged_df['bid_size1']+merged_df['ask_size1']+merged_df['bid_size2']+merged_df['ask_size2']))\n",
    "        \n",
    "        del book_df\n",
    "        del trade_df\n",
    "        \n",
    "        overview_aggregations = {\n",
    "        'wap1': ['sum', 'std'],\n",
    "        'wap2': ['sum', 'std'],\n",
    "        'logret1': [realized_volatility],\n",
    "        'logret2': [realized_volatility],\n",
    "        'logret_price': [realized_volatility],\n",
    "        'wap_balance': ['sum', 'max'],\n",
    "        'price_spread1': ['sum', 'max'],\n",
    "        'bid_spread': ['sum', 'max'],\n",
    "        'ask_spread': ['sum', 'max'],\n",
    "        'total_volume': ['sum', 'max'],\n",
    "        'volume_imbalance': ['sum', 'max'],\n",
    "        \"bid_ask_spread\": ['sum', 'max'],\n",
    "        'size':  ['sum', 'max','min'],\n",
    "        'order_count': ['sum', 'max'],\n",
    "        'trade_money_turnover': ['sum', 'max','min'],\n",
    "        }\n",
    "        aggregations = merged_df.groupby('time_id').agg(overview_aggregations).reset_index(drop=False)\n",
    "        aggregations = aggregations.fillna(-0.01)\n",
    "        aggregations.columns = ['_'.join(col).strip() for col in aggregations.columns.values]\n",
    "        for idx, row in aggregations.iterrows():\n",
    "            row = row.to_dict()\n",
    "#             print(row)\n",
    "#             input()\n",
    "            time_id = row['time_id_']\n",
    "#             print(int(time_id), type(time_id))\n",
    "#             input()\n",
    "            rowid = get_row_id(main_stock_id, time_id)\n",
    "            \n",
    "            if rowid not in feature_map:\n",
    "                feature_map[rowid] = {}\n",
    "            \n",
    "            for key, aggs in overview_aggregations.items():\n",
    "                for agg in aggs:\n",
    "                    if isinstance(agg, types.FunctionType):\n",
    "                        agg = agg.__name__\n",
    "                    feature_map[rowid][f'{key}_{agg}'] = row[f'{key}_{agg}']\n",
    "        del aggregations\n",
    "        aggregations = merged_df[merged_df['seconds_in_bucket']>=400].groupby('time_id').agg(overview_aggregations).reset_index(drop=False)\n",
    "        aggregations = aggregations.fillna(-0.01)\n",
    "        aggregations.columns = ['_'.join(col).strip() for col in aggregations.columns.values]\n",
    "        for idx, row in aggregations.iterrows():\n",
    "            row = row.to_dict()\n",
    "#             print(row)\n",
    "#             input()\n",
    "            time_id = row['time_id_']\n",
    "#             print(int(time_id), type(time_id))\n",
    "#             input()\n",
    "            rowid = get_row_id(main_stock_id, time_id)\n",
    "            \n",
    "            if rowid not in feature_map:\n",
    "                feature_map[rowid] = {}\n",
    "            \n",
    "            for key, aggs in overview_aggregations.items():\n",
    "                for agg in aggs:\n",
    "                    if isinstance(agg, types.FunctionType):\n",
    "                        agg = agg.__name__\n",
    "                    feature_map[rowid][f'{key}_{agg}_400'] = row[f'{key}_{agg}']\n",
    "        del aggregations\n",
    "        \n",
    "        merged_df['seconds_in_bucket'] = merged_df['seconds_in_bucket'] // interval_second\n",
    "        merge_prepared_df = merged_df.groupby(['time_id','seconds_in_bucket']).agg('sum').reset_index(drop=False)\n",
    "        del merged_df\n",
    "        for groupkey, groupdf in merge_prepared_df.groupby('time_id'):\n",
    "\n",
    "            rowid = get_row_id(main_stock_id, groupkey)\n",
    "            \n",
    "            if rowid not in feature_map:\n",
    "                feature_map[rowid] = {}\n",
    "            \n",
    "            sequence_length = len(groupdf['seconds_in_bucket'].to_numpy())\n",
    "                              \n",
    "            groupdf['has_trade_data'] = (~groupdf['price'].isnull()).astype(int)\n",
    "            \n",
    "            \n",
    "            feature_map[rowid]['sequence_mask_xs'] = [False]*sequence_length + [True]*(intervals_count-sequence_length)\n",
    "            feature_map[rowid]['has_trade_data_xs'] = np.concatenate([groupdf['has_trade_data'].to_numpy(),[0]*(intervals_count-sequence_length)])\n",
    "            feature_map[rowid]['seconds_in_bucket_xs'] = np.concatenate([groupdf['seconds_in_bucket'].to_numpy(),[0]*(intervals_count-sequence_length)])\n",
    "\n",
    "            for feature_name in ['logret1','logret_bid_price1','logret_ask_price1','logret_bid_price2','logret_ask_price2','price_spread1','bid_spread','ask_spread','logret_directional_volume1','logret_directional_volume2','logret_trade_money_turnover','trade_price_push_on_book','logret_price','order_count']: #'bid_price2','bid_size2','ask_price2','ask_size2'\n",
    "                nan_replace_val = -0.01\n",
    "                if feature_name in ['logret_bid_price1','logret_ask_price1','logret_bid_price2','logret_ask_price2','trade_price_push_on_book','logret_price']:\n",
    "                    nan_replace_val = 0.0\n",
    "                    \n",
    "                feature_map[rowid][f'{feature_name}_xs'] = np.concatenate([\n",
    "                                                        np.nan_to_num(groupdf[feature_name].to_numpy(),nan=nan_replace_val,neginf=nan_replace_val,posinf=nan_replace_val),\n",
    "                                                        [0]*(intervals_count-sequence_length)\n",
    "                                                                          ])\n",
    "#                 feature_map[rowid][f'{feature_name}_v_xs'] = np.concatenate([np.nan_to_num(1/(groupdf[feature_name].to_numpy()),nan=-0.01,neginf=-0.01,posinf=-0.01),[0]*(intervals_count-sequence_length)])\n",
    "#             groupdf = groupdf.fillna(-0.01) \n",
    "            # transformer mask ignores the True values,and False remains unchanged\n",
    "                \n",
    "#         print(merged_df)\n",
    "#         input()\n",
    "     \n",
    "#         import gc\n",
    "        del merge_prepared_df\n",
    "#         gc.collect()\n",
    "        return feature_map\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interval_seconds = 6\n",
    "data_intervals_count = int(600/data_interval_seconds)\n",
    "\n",
    "def transform_to_01_realized_volatility_linear_data(features_dict):\n",
    "        feature_x  = {\n",
    "                    'row_id':features_dict['row_id'],\n",
    "                    'stock_id':torch.tensor(features_dict['stock_id'], dtype=torch.float32),\n",
    "                    'sequence_mask_xs': torch.tensor(features_dict.get('sequence_mask_xs', [False]+[True]*(int(600/data_interval_seconds)-1)), dtype=torch.bool),\n",
    "                    'seconds_in_bucket_xs': torch.tensor(features_dict.get('seconds_in_bucket_xs', [(idx) for idx in range(0,int(data_intervals_count))]), dtype=torch.float32),\n",
    "                    'has_trade_data_xs': torch.tensor(features_dict.get('has_trade_data_xs', [0]*(int(600/data_interval_seconds))), dtype=torch.float32),\n",
    "                }\n",
    "        overview_aggregations = {\n",
    "        'wap1': ['sum', 'std'],\n",
    "        'wap2': ['sum', 'std'],\n",
    "        'logret1': [realized_volatility],\n",
    "        'logret2': [realized_volatility],\n",
    "        'logret_price': [realized_volatility],\n",
    "        'wap_balance': ['sum', 'max'],\n",
    "        'price_spread1': ['sum', 'max'],\n",
    "        'bid_spread': ['sum', 'max'],\n",
    "        'ask_spread': ['sum', 'max'],\n",
    "        'total_volume': ['sum', 'max'],\n",
    "        'volume_imbalance': ['sum', 'max'],\n",
    "        \"bid_ask_spread\": ['sum', 'max'],\n",
    "        'size':  ['sum', 'max','min'],\n",
    "        'order_count': ['sum', 'max'],\n",
    "        'trade_money_turnover': ['sum', 'max','min'],\n",
    "        }\n",
    "        \n",
    "        for key, aggs in overview_aggregations.items():\n",
    "            for agg in aggs:\n",
    "                if isinstance(agg, types.FunctionType):\n",
    "                    agg = agg.__name__\n",
    "                feature_x[f'{key}_{agg}'] = torch.tensor(features_dict.get(f'{key}_{agg}', -0.01), dtype=torch.float32)\n",
    "                feature_x[f'{key}_{agg}_400'] = torch.tensor(features_dict.get(f'{key}_{agg}_400', -0.01), dtype=torch.float32)\n",
    "                \n",
    "#         for feature_name in ['wap1','wap_balance','logret1','logret2','logrett',\n",
    "#                              'price_spread1','bid_spread','ask_spread','total_volume','volume_imbalance',\n",
    "#                             'size','order_count','trade_money_turnover','trade_book_price_spread']:\n",
    "            \n",
    "#             feature_x[f'{feature_name}_sum_xs'] = torch.tensor(features_dict.get(f'{feature_name}_sum_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "#             feature_x[f'{feature_name}_max_xs'] = torch.tensor(features_dict.get(f'{feature_name}_max_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "        for feature_name in ['logret1','logret_bid_price1','logret_ask_price1','logret_bid_price2','logret_ask_price2',\n",
    "                        'price_spread1','bid_spread','ask_spread','logret_directional_volume1','logret_directional_volume2','logret_trade_money_turnover','trade_price_push_on_book','logret_price','order_count']: #'bid_price2','bid_size2','ask_price2','ask_size2',\n",
    "            feature_x[f'{feature_name}_xs'] = torch.tensor(features_dict.get(f'{feature_name}_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "#             feature_x[f'{feature_name}_v_xs'] = torch.tensor(features_dict.get(f'{feature_name}_v_xs', [-0.01]*(int(600/data_interval_seconds))), dtype=torch.float32)\n",
    "#             np.concatenate([groupdf[feature_name].to_numpy(),[0]*(intervals_count-sequence_length)])\n",
    "        return (\n",
    "                feature_x,\n",
    "                {'target_realized_volatility':torch.tensor([features_dict['target_realized_volatility']])}\n",
    "#                 [features_dict['target']]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_optiver_dataset_for_evaluation(mode):\n",
    "    if mode not in ['train','test']:\n",
    "        raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n",
    "\n",
    "\n",
    "\n",
    "    main_df = pd.read_csv(os.path.join(DATA_DIRECTORY,f'{mode}.csv'))\n",
    "    #         if self.mode == 'train':\n",
    "    #             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n",
    "    if mode == 'test':\n",
    "        main_df['target'] = 0\n",
    "\n",
    "\n",
    "    main_df['row_id'] = main_df['stock_id'].astype(str) + \"-\" +main_df['time_id'].astype(str)\n",
    "    targets = main_df['target'].tolist()\n",
    "\n",
    "    for stock_id, stock_df in main_df.groupby(['stock_id']):\n",
    "        default_feature_set_list = {}\n",
    "\n",
    "        for idx, row_id in enumerate(stock_df['row_id'].tolist()):\n",
    "            stock_id = int(row_id.split('-')[0])\n",
    "            time_id = int(row_id.split('-')[1])\n",
    "            default_feature_set_list[row_id] = {'target_realized_volatility':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n",
    "\n",
    "        feature_set_list = get_features_map_for_stock(DATA_DIRECTORY, mode, stock_id)\n",
    "#         print(feature_set_list)\n",
    "\n",
    "        for rowid, features_dict in feature_set_list.items():\n",
    "            for fkey,fval in features_dict.items():\n",
    "                default_feature_set_list[rowid][fkey] = fval\n",
    "#         print(default_feature_set_list)\n",
    "        for row_id, datadict in default_feature_set_list.items():\n",
    "            x,y = transform_to_01_realized_volatility_linear_data(datadict)\n",
    "            \n",
    "            for fedict in [x,y]:\n",
    "                for k,v in fedict.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        fedict[k] = v.reshape(1,-1)\n",
    "                    else:\n",
    "                        fedict[k] = [v]\n",
    "            \n",
    "            \n",
    "            yield x,y\n",
    "\n",
    "\n",
    "#         for time_id, time_df in stock_df.groupby(['time_id']):\n",
    "#             row_id = time_df['row_id'].iloc[0]\n",
    "#             yield default_feature_set_list[row_id]\n",
    "    \n",
    "        del default_feature_set_list\n",
    "   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature_x, feature_y in iterate_optiver_dataset_for_evaluation('test'):\n",
    "#     for k,v in feature_x.items():\n",
    "#         print(k)\n",
    "#         input(v)\n",
    "#     print(feature_y)\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = None\n",
    "\n",
    "\n",
    "def loss_fn_mse(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)))\n",
    "\n",
    "def loss_fn_mspe(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)/y))\n",
    "\n",
    "def loss_fn_orig(y, pred):\n",
    "    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "realize_volatility_scale_factor = 1000\n",
    "def scale_optiver_feature(feature_name, feature_tensor):\n",
    "    standard_scaling_feature_map = {'stock_id': {'mean': 62.43794250488281, 'std': 37.12644958496094},\n",
    " 'seconds_in_bucket_xs': {'mean': 46.53627014160156,\n",
    "  'std': 30.269084930419922},\n",
    " 'has_trade_data_xs': {'mean': 0.9432891011238098, 'std': 0.23128946125507355},\n",
    " 'wap1_sum': {'mean': 389.92791748046875, 'std': 135.835205078125},\n",
    " 'wap1_sum_400': {'mean': 128.18423461914062, 'std': 47.224849700927734},\n",
    " 'wap1_std': {'mean': 0.0011102678254246712, 'std': 0.0010516541078686714},\n",
    " 'wap1_std_400': {'mean': 0.0006360253901220858, 'std': 0.0005713916034437716},\n",
    " 'wap2_sum': {'mean': 389.9277038574219, 'std': 135.83547973632812},\n",
    " 'wap2_sum_400': {'mean': 128.1841583251953, 'std': 47.2249641418457},\n",
    " 'wap2_std': {'mean': 0.0011489872122183442, 'std': 0.0010650980984792113},\n",
    " 'wap2_std_400': {'mean': 0.0006859369459562004, 'std': 0.0005950198974460363},\n",
    " 'logret1_realized_volatility': {'mean': 0.005850940477102995,\n",
    "  'std': 0.004778958857059479},\n",
    " 'logret1_realized_volatility_400': {'mean': 0.0022959925699979067,\n",
    "  'std': 0.0019109743880107999},\n",
    " 'logret2_realized_volatility': {'mean': 0.007210279814898968,\n",
    "  'std': 0.005797804333269596},\n",
    " 'logret2_realized_volatility_400': {'mean': 0.003152207238599658,\n",
    "  'std': 0.0026645385660231113},\n",
    " 'logret_price_realized_volatility': {'mean': 0.004610072355717421,\n",
    "  'std': 0.004089121241122484},\n",
    " 'logret_price_realized_volatility_400': {'mean': 0.0014677889412268996,\n",
    "  'std': 0.0011549298651516438},\n",
    " 'wap_balance_sum': {'mean': 0.09234429150819778, 'std': 0.08365236222743988},\n",
    " 'wap_balance_sum_400': {'mean': 0.029098447412252426,\n",
    "  'std': 0.026479562744498253},\n",
    " 'wap_balance_max': {'mean': 0.0010880399495363235,\n",
    "  'std': 0.0012415354140102863},\n",
    " 'wap_balance_max_400': {'mean': 0.0008600918226875365,\n",
    "  'std': 0.000884830835275352},\n",
    " 'price_spread1_sum': {'mean': 0.2226376235485077, 'std': 0.20077848434448242},\n",
    " 'price_spread1_sum_400': {'mean': 0.07035906612873077,\n",
    "  'std': 0.06335863471031189},\n",
    " 'price_spread1_max': {'mean': 0.0014055456267669797,\n",
    "  'std': 0.001588768558576703},\n",
    " 'price_spread1_max_400': {'mean': 0.0011413421016186476,\n",
    "  'std': 0.0011644071200862527},\n",
    " 'bid_spread_sum': {'mean': 0.07556163519620895, 'std': 0.06961646676063538},\n",
    " 'bid_spread_sum_400': {'mean': 0.024438833817839622,\n",
    "  'std': 0.02328931912779808},\n",
    " 'bid_spread_max': {'mean': 0.0007034957525320351,\n",
    "  'std': 0.0008316159946843982},\n",
    " 'bid_spread_max_400': {'mean': 0.0005391522427089512,\n",
    "  'std': 0.0005698652239516377},\n",
    " 'ask_spread_sum': {'mean': 0.07635528594255447, 'std': 0.06991078704595566},\n",
    " 'ask_spread_sum_400': {'mean': 0.024674607440829277,\n",
    "  'std': 0.023346172645688057},\n",
    " 'ask_spread_max': {'mean': 0.0007134961779229343,\n",
    "  'std': 0.0008447545696981251},\n",
    " 'ask_spread_max_400': {'mean': 0.0005458995583467185,\n",
    "  'std': 0.0005785721004940569},\n",
    " 'total_volume_sum': {'mean': 1629942.5, 'std': 9067553.0},\n",
    " 'total_volume_sum_400': {'mean': 547846.375, 'std': 3054834.25},\n",
    " 'total_volume_max': {'mean': 6443.4765625, 'std': 27511.509765625},\n",
    " 'total_volume_max_400': {'mean': 5374.6015625, 'std': 24850.201171875},\n",
    " 'volume_imbalance_sum': {'mean': 396575.46875, 'std': 2442117.5},\n",
    " 'volume_imbalance_sum_400': {'mean': 132286.90625, 'std': 828395.75},\n",
    " 'volume_imbalance_max': {'mean': 3689.99755859375, 'std': 13807.1123046875},\n",
    " 'volume_imbalance_max_400': {'mean': 2674.690185546875,\n",
    "  'std': 10599.6044921875},\n",
    " 'bid_ask_spread_sum': {'mean': 0.038354359567165375,\n",
    "  'std': 0.050127506256103516},\n",
    " 'bid_ask_spread_sum_400': {'mean': 0.011937146075069904,\n",
    "  'std': 0.01599636673927307},\n",
    " 'bid_ask_spread_max': {'mean': 0.000663700804580003,\n",
    "  'std': 0.0009456104598939419},\n",
    " 'bid_ask_spread_max_400': {'mean': 0.0004903593799099326,\n",
    "  'std': 0.000652861432172358},\n",
    " 'size_sum': {'mean': 31860.21484375, 'std': 70259.2109375},\n",
    " 'size_sum_400': {'mean': 10241.53515625, 'std': 23358.8828125},\n",
    " 'size_max': {'mean': 3035.258544921875, 'std': 8191.23974609375},\n",
    " 'size_max_400': {'mean': 1780.333984375, 'std': 4887.39892578125},\n",
    " 'size_min': {'mean': 6.389898777008057, 'std': 352.6045227050781},\n",
    " 'size_min_400': {'mean': 22.84056282043457, 'std': 639.7129516601562},\n",
    " 'order_count_sum': {'mean': 373.43682861328125, 'std': 608.5436401367188},\n",
    " 'order_count_sum_400': {'mean': 120.78227996826172, 'std': 201.4743194580078},\n",
    " 'order_count_max': {'mean': 26.246469497680664, 'std': 50.72904968261719},\n",
    " 'order_count_max_400': {'mean': 16.569400787353516,\n",
    "  'std': 30.438138961791992},\n",
    " 'trade_money_turnover_sum': {'mean': 31859.123046875, 'std': 70262.2265625},\n",
    " 'trade_money_turnover_sum_400': {'mean': 10240.9248046875,\n",
    "  'std': 23348.404296875},\n",
    " 'trade_money_turnover_max': {'mean': 3035.2578125, 'std': 8194.078125},\n",
    " 'trade_money_turnover_max_400': {'mean': 1780.24267578125,\n",
    "  'std': 4885.64453125},\n",
    " 'trade_money_turnover_min': {'mean': 6.387502670288086,\n",
    "  'std': 352.42071533203125},\n",
    " 'trade_money_turnover_min_400': {'mean': 22.84111213684082,\n",
    "  'std': 639.8861694335938},\n",
    " 'logret1_xs': {'mean': -2.4693693756461244e-09, 'std': 0.0006871781661175191},\n",
    " 'logret_bid_price1_xs': {'mean': -1.9815225016373006e-09,\n",
    "  'std': 0.0006457031704485416},\n",
    " 'logret_ask_price1_xs': {'mean': -2.840470081366675e-09,\n",
    "  'std': 0.0006457548006437719},\n",
    " 'logret_bid_price2_xs': {'mean': -2.0001771350308672e-09,\n",
    "  'std': 0.0006615926395170391},\n",
    " 'logret_ask_price2_xs': {'mean': -2.7945690206365725e-09,\n",
    "  'std': 0.0006625199457630515},\n",
    " 'price_spread1_xs': {'mean': 0.0022263764403760433,\n",
    "  'std': 0.0026683351024985313},\n",
    " 'bid_spread_xs': {'mean': 0.0007556164055131376,\n",
    "  'std': 0.0009447085903957486},\n",
    " 'ask_spread_xs': {'mean': 0.0007635528454557061,\n",
    "  'std': 0.0009570387774147093},\n",
    " 'logret_directional_volume1_xs': {'mean': -1.8204282525857707e-09,\n",
    "  'std': 0.022533627226948738},\n",
    " 'logret_directional_volume2_xs': {'mean': 1.4707172146799508e-09,\n",
    "  'std': 0.02200956828892231},\n",
    " 'logret_trade_money_turnover_xs': {'mean': -2.9323007311177207e-07,\n",
    "  'std': 1.6745622158050537},\n",
    " 'trade_price_push_on_book_xs': {'mean': -1.7589563583442214e-07,\n",
    "  'std': 5.7160297728842124e-05},\n",
    " 'logret_price_xs': {'mean': -2.801246123951273e-09,\n",
    "  'std': 0.0006271468009799719},\n",
    " 'order_count_xs': {'mean': 3.734368324279785, 'std': 10.645439147949219}}\n",
    "#     print(feature_name, feature_tensor.size())\n",
    "    \n",
    "#     if feature_name in ['book_realized_volatility_xs','trade_realized_volatility_xs']:\n",
    "#         # we expect feature_tensor to be log returns tensor\n",
    "#         feature_tensor = feature_tensor ** 2\n",
    "# #         print(feature_tensor)\n",
    "#         feature_tensor = torch.cumsum(feature_tensor,1)\n",
    "#         # scale it to make each step realize volatility extrapolatable to 10 min window\n",
    "# #         feature_tensor = feature_tensor * torch.tensor([data_intervals_count/idx for idx in range(1,data_intervals_count+1,1)])\n",
    "#         feature_tensor = torch.sqrt(feature_tensor) * realize_volatility_scale_factor\n",
    "        \n",
    "    if feature_name == 'sequence_mask_xs':\n",
    "        feature_tensor = feature_tensor.type(torch.float32)\n",
    "        return feature_tensor\n",
    "    if feature_name == 'has_trade_data_xs':\n",
    "        #TODO: we'll pre convert it so directly reutrn feature_tensor without converting\n",
    "        return feature_tensor.type(torch.float32)\n",
    "    if feature_name == 'seconds_in_bucket_xs':\n",
    "        return feature_tensor / standard_scaling_feature_map[feature_name]['std']/100\n",
    "    if feature_name in standard_scaling_feature_map:\n",
    "        return (feature_tensor - standard_scaling_feature_map[feature_name]['mean'])/standard_scaling_feature_map[feature_name]['std']\n",
    "#         return feature_tensor/standard_scaling_feature_map[feature_name]['std']/2\n",
    "    if feature_name in ['trade_price_local_standardized_xs','book_wap1_local_standardized_xs']:\n",
    "        #TODO: the kaggle version of pytorch dont have nan_to_num, do something here!\n",
    "        feature_tensor = torch.masked_fill(feature_tensor, torch.isinf(feature_tensor),0)\n",
    "#         feature_tensor = torch.nan_to_num(feature_tensor,nan=0, posinf=0, neginf=0)\n",
    "#     print(feature_tensor)\n",
    "#     print(torch.any(torch.isnan(feature_tensor)))\n",
    "#     input()\n",
    "    return feature_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockIdEmbedding(nn.Module):\n",
    "    def __init__(self,number_of_stock_embeddings=126+10, number_of_stock_embedding_dimention=2, mode='train'):\n",
    "        super(StockIdEmbedding, self).__init__()\n",
    "        \n",
    "        self.number_of_stock_embeddings = number_of_stock_embeddings\n",
    "        self.number_of_stock_embedding_dimention = number_of_stock_embedding_dimention\n",
    "        self.stock_embedding = nn.Embedding(self.number_of_stock_embeddings, self.number_of_stock_embedding_dimention)\n",
    "        self.mode = mode\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(self.number_of_stock_embedding_dimention, 32),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return []\n",
    "    \n",
    "    def set_mode(self,mode):\n",
    "        self.mode = mode\n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "        \n",
    "        stock_id_clamped = torch.clamp(feature_dict['stock_id'],0,self.number_of_stock_embeddings-1)\n",
    "        stock_id_clamped = stock_id_clamped.type(torch.int64)\n",
    "        stock_id_clamped = stock_id_clamped.to(device).reshape(-1,1)\n",
    "        embedding_logits = self.stock_embedding(stock_id_clamped)\n",
    "        embedding_logits = embedding_logits.reshape(-1,self.number_of_stock_embedding_dimention)\n",
    "        \n",
    "        if self.mode == 'stock_id_embedding':\n",
    "            return embedding_logits\n",
    "\n",
    "            \n",
    "        logits = self.linear_stack(embedding_logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len,1, d_model)\n",
    "#         print(pe.size())\n",
    "        pe[:,0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:,0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "#         print(\"x\",x.size())\n",
    "#         print(\"PE\",self.pe[:,:x.size(1)].size())\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class MultiFetTransformer(nn.Module):\n",
    "    def __init__(self,feature_names=None, overview_feature_names=None, mask_feature_name='sequence_mask_xs',sequence_feature_name='seconds_in_bucket_xs', mode=\"hybrid\"):\n",
    "        \"\"\"single feature, feature learner\n",
    "        `mode`: train|feature_generator\n",
    "        \"\"\"\n",
    "        super(MultiFetTransformer,self).__init__()\n",
    "        if feature_names is None:\n",
    "             \n",
    "#             feature_names = ['seconds_in_bucket_xs_group','has_trade_data_xs'] + [ f'{feature_name}_sum_xs' for feature_name in ['wap1','wap_balance','logret1','logret2','logrett',\n",
    "#                              'price_spread1','bid_spread','ask_spread','total_volume','volume_imbalance',\n",
    "#                             'size','order_count','trade_money_turnover','trade_book_price_spread']] + [ f'{feature_name}_max_xs' for feature_name in ['wap1','wap_balance','logret1','logret2','logrett',\n",
    "#                              'price_spread1','bid_spread','ask_spread','total_volume','volume_imbalance',\n",
    "#                             'size','order_count','trade_money_turnover','trade_book_price_spread']]\n",
    "            # 'bid_price2','bid_size2','ask_price2','ask_size2',\n",
    "    # 'seconds_in_bucket', 'logret1'\n",
    "            feature_names = [f'{feature_name}_xs' for feature_name in ['has_trade_data'] + \n",
    "                                                                         ['logret_bid_price1','logret_ask_price1','logret_bid_price2','logret_ask_price2','price_spread1',\n",
    "                                                        'bid_spread','ask_spread','logret_directional_volume1','logret_directional_volume2','logret_trade_money_turnover','trade_price_push_on_book','logret_price','order_count']]\n",
    "        \n",
    "            \n",
    "#             feature_names = ['seconds_in_bucket_xs_group','has_trade_data_xs']\n",
    "        \n",
    "        if overview_feature_names is None:\n",
    "            overview_feature_names = []\n",
    "            overview_aggregations = {\n",
    "            'wap1': ['sum', 'std'],\n",
    "            'wap2': ['sum', 'std'],\n",
    "            'logret1': [realized_volatility],\n",
    "            'logret2': [realized_volatility],\n",
    "            'logret_price': [realized_volatility],\n",
    "            'wap_balance': ['sum', 'max'],\n",
    "            'price_spread1': ['sum', 'max'],\n",
    "            'bid_spread': ['sum', 'max'],\n",
    "            'ask_spread': ['sum', 'max'],\n",
    "            'total_volume': ['sum', 'max'],\n",
    "            'volume_imbalance': ['sum', 'max'],\n",
    "            \"bid_ask_spread\": ['sum', 'max'],\n",
    "            'size':  ['sum', 'max','min'],\n",
    "            'order_count': ['sum', 'max'],\n",
    "            'trade_money_turnover': ['sum', 'max','min'],\n",
    "            }\n",
    "        \n",
    "            for key, aggs in overview_aggregations.items():\n",
    "                for agg in aggs:\n",
    "                    if isinstance(agg, types.FunctionType):\n",
    "                        agg = agg.__name__\n",
    "                    overview_feature_names.append(f'{key}_{agg}')\n",
    "                    overview_feature_names.append(f'{key}_{agg}_400')\n",
    "    #             overview_feature_names = ['wap1_sum', 'wap1_std', 'logret1_realized_volatility', 'logret2_realized_volatility', 'logrett_realized_volatility', 'wap_balance_sum', 'wap_balance_max', 'price_spread1_sum', 'price_spread1_max', 'bid_spread_sum', 'bid_spread_max', 'ask_spread_sum', 'ask_spread_max', 'total_volume_sum', 'total_volume_max', 'volume_imbalance_sum',\n",
    "    #                                       'volume_imbalance_max', 'bid_ask_spread_sum', 'bid_ask_spread_max', 'size_sum', 'size_max', 'size_min', 'order_count_sum', 'order_count_max', 'trade_money_turnover_sum', 'trade_money_turnover_max', 'trade_money_turnover_min']\n",
    "\n",
    "        \n",
    "        \n",
    "        if type(feature_names) == str:\n",
    "            feature_names = [feature_names]\n",
    "        \n",
    "        self.use_stock_embedding = True\n",
    "        self.use_overview_features = True\n",
    "        self.stock_id_embedding_dimension = 3\n",
    "        self.stock_id_embedding = StockIdEmbedding(number_of_stock_embedding_dimention=self.stock_id_embedding_dimension, mode='stock_id_embedding')\n",
    "        \n",
    "        self.feature_names = feature_names\n",
    "        self.overview_feature_names = overview_feature_names\n",
    "        self.mask_feature_name = mask_feature_name\n",
    "        self.sequence_feature_name = sequence_feature_name\n",
    "        self.features_count = len(self.feature_names)\n",
    "#         self.scaled_feature_dimension = 4\n",
    "#         self.feature_scalers = nn.ModuleList([nn.Sequential(\n",
    "#                     # one is original feature, then stock,\n",
    "#                     nn.Linear(1+self.stock_id_embedding_dimension, 32),\n",
    "#                     nn.GELU(),\n",
    "#                     nn.Linear(32, self.scaled_feature_dimension)\n",
    "#                 ) for _ in self.feature_names])\n",
    "#         for feature_name in self.feature_names:\n",
    "#             self.feature_scalers.append()\n",
    "        \n",
    "        self.output_dimensions = 128\n",
    "        self.transformer_input_dimension = 14\n",
    "#         self.positional_encoding = PositionalEncoding(d_model=self.transformer_input_dimension)\n",
    "        self.mode = mode\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.transformer_input_dimension, nhead=7, dropout=0.01, activation='gelu')\n",
    "        self.encoder_stack = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "        \n",
    "        self.feature_to_feature_embedding = nn.Sequential(\n",
    "            nn.Linear(self.features_count  + self.stock_id_embedding_dimension, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, self.transformer_input_dimension)\n",
    "        )\n",
    "        \n",
    "        self.transformer_output_feature = nn.Sequential(\n",
    "#             nn.Conv1d(self.transformer_input_dimension, 32, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(32, 24, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Flatten(),\n",
    "            nn.Linear(data_intervals_count*self.transformer_input_dimension,512),\n",
    "            nn.GELU(),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.Linear(512,self.output_dimensions),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, self.features_out),\n",
    "        )\n",
    "        \n",
    "        self.overviewff_output_feature = nn.Sequential(\n",
    "#             nn.Conv1d(self.transformer_input_dimension, 32, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(32, 24, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Flatten(),\n",
    "            nn.Linear(len(self.overview_feature_names)+self.stock_id_embedding_dimension,512),\n",
    "            nn.GELU(),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.Linear(512,self.output_dimensions),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, self.features_out),\n",
    "        )\n",
    "        \n",
    "        self.transformer_train = nn.Sequential(\n",
    "            nn.Linear(self.output_dimensions, 256),\n",
    "            nn.GELU(),\n",
    "#             nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),   \n",
    "        )\n",
    "        self.overviewff_train = nn.Sequential(\n",
    "            nn.Linear(self.output_dimensions, 256),\n",
    "            nn.GELU(),\n",
    "#             nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),   \n",
    "        )\n",
    "       \n",
    "        \n",
    "        self.hybrid_stack = nn.Sequential(\n",
    "\n",
    "            nn.Linear(self.output_dimensions*2, 256),\n",
    "            nn.GELU(),\n",
    "#             nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "#             nn.Linear(self.features_out, 1),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.Hardswish(),\n",
    "            nn.Linear(128, 1),   \n",
    "        )\n",
    "    \n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def get_possible_modes(self):\n",
    "        return ['transformer_train','overviewff_train','hybrid','full_train']\n",
    "    \n",
    "    def parameters(self):\n",
    "        generator_sources_map = {\n",
    "            'transformer_train': [self.encoder_stack, self.transformer_output_feature, self.transformer_train],\n",
    "            'overviewff_train': [self.stock_id_embedding, self.overviewff_output_feature, self.overviewff_train],\n",
    "            'hybrid': [self.hybrid_stack],\n",
    "            'full_train': [self.encoder_stack, self.transformer_output_feature, self.transformer_train] + [self.stock_id_embedding, self.overviewff_output_feature, self.overviewff_train] + [self.hybrid_stack]\n",
    "        }\n",
    "        params = []\n",
    "        if self.mode in generator_sources_map:\n",
    "            for generator_source in generator_sources_map[self.mode]:\n",
    "                for param in generator_source.parameters():\n",
    "                    params.append(param)\n",
    "            return params\n",
    "        \n",
    "        \n",
    "    def forward(self, feature_dict, h0_tensor=None):  \n",
    "        out_ = []\n",
    "        if self.mode in ['transformer_train','hybrid','full_train']:\n",
    "            feature_x = []     \n",
    "            for idx,feature_name in enumerate(self.feature_names):\n",
    "                feature_tensor = scale_optiver_feature(feature_name, feature_dict[feature_name]).to(device)\n",
    "                feature_tensor = feature_tensor.reshape(-1,feature_tensor.size(1),1)\n",
    "                feature_x.append(feature_tensor)\n",
    "            \n",
    "        \n",
    "            #combine all the features activated tensors\n",
    "            feature_x = torch.cat(feature_x,dim=2) #.reshape(-1, data_intervals_count, self.input_size_) #+[stock_embedding_logits]\n",
    "\n",
    "            #positional encoding\n",
    "            position_encodings = feature_dict[self.sequence_feature_name]\n",
    "            position_encodings = (position_encodings.to(device)+1)/601\n",
    "            position_encodings = position_encodings.reshape(-1,feature_x.size(1),1)\n",
    "            feature_x = torch.add(feature_x, position_encodings)\n",
    "\n",
    "            #Mask prepare\n",
    "            mask = feature_dict[self.mask_feature_name].to(device)\n",
    "\n",
    "\n",
    "            # make them sequence first!!\n",
    "            # RANT: all of this due to Kaggle using pytorch 1.7.0\n",
    "            feature_x = torch.stack(feature_x.unbind(0),dim=1)\n",
    "\n",
    "            transformer_features = self.encoder_stack(feature_x, src_key_padding_mask=mask)\n",
    "            # back to batch first!\n",
    "            transformer_features = torch.stack(transformer_features.unbind(0),dim=1)\n",
    "            transformer_features = transformer_features.reshape(-1, self.transformer_input_dimension*data_intervals_count)\n",
    "        \n",
    "\n",
    "            transformer_features = self.transformer_output_feature(transformer_features)\n",
    "            if self.mode == 'transformer_train':\n",
    "                return self.transformer_train(transformer_features)\n",
    "            out_.append(transformer_features)\n",
    "        # we add overview features here CAT them; that's why \n",
    "#         print(out_.size(),stock_embedding_logits.size())\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.mode in ['overviewff_train','hybrid','full_train']:\n",
    "        \n",
    "            stock_embedding_logits = self.stock_id_embedding(feature_dict)\n",
    "            \n",
    "        \n",
    "        \n",
    "            feature_x = []\n",
    "            for idx,feature_name in enumerate(self.overview_feature_names):\n",
    "                feature_tensor = scale_optiver_feature(feature_name, feature_dict[feature_name]).to(device)\n",
    "    #             print(feature_tensor.size())\n",
    "                feature_tensor = feature_tensor.reshape(-1,1)\n",
    "\n",
    "                feature_x.append(feature_tensor)\n",
    "            \n",
    "            \n",
    "            feature_x = torch.cat([stock_embedding_logits]+feature_x,dim=1)\n",
    "#             print(feature_x.size())\n",
    "            overview_features = self.overviewff_output_feature(feature_x)\n",
    "#             print(overview_features.size())\n",
    "            if self.mode == 'overviewff_train':\n",
    "                return self.overviewff_train(overview_features)\n",
    "            \n",
    "#             out_.append(stock_embedding_logits)\n",
    "            out_.append(overview_features)\n",
    "            \n",
    "#         print(feature_x.size())\n",
    "#         print(out_.size())\n",
    "        out_ = torch.cat(out_,dim=1)\n",
    "        \n",
    "        out_ = self.hybrid_stack(out_)\n",
    "\n",
    "        return out_\n",
    "\n",
    "# model = MultiFetTransformer()\n",
    "# model.to(device)\n",
    "# dataloader_train = DataLoader(dataset, batch_size=2,\n",
    "#                                 shuffle=True, num_workers=0, pin_memory=False)#, collate_fn=optiver_custom_collate_func)\n",
    "\n",
    "\n",
    " \n",
    "# >>> src = torch.rand(10, 32, 512)\n",
    "# >>> out = encoder_layer(src)   \n",
    "# for train_batch_idx, (Feature_X, feature_y) in enumerate(dataloader_train):\n",
    "#     model(Feature_X)\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiFetTransformer(\n",
       "  (stock_id_embedding): StockIdEmbedding(\n",
       "    (stock_embedding): Embedding(136, 3)\n",
       "    (linear_stack): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=32, bias=True)\n",
       "      (1): Hardswish()\n",
       "      (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (3): Hardswish()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=16, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=14, out_features=14, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=14, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.01, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=14, bias=True)\n",
       "    (norm1): LayerNorm((14,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((14,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.01, inplace=False)\n",
       "    (dropout2): Dropout(p=0.01, inplace=False)\n",
       "  )\n",
       "  (encoder_stack): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=14, out_features=14, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=14, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=14, bias=True)\n",
       "        (norm1): LayerNorm((14,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((14,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.01, inplace=False)\n",
       "        (dropout2): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=14, out_features=14, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=14, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=14, bias=True)\n",
       "        (norm1): LayerNorm((14,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((14,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.01, inplace=False)\n",
       "        (dropout2): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_to_feature_embedding): Sequential(\n",
       "    (0): Linear(in_features=17, out_features=128, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Linear(in_features=128, out_features=14, bias=True)\n",
       "  )\n",
       "  (transformer_output_feature): Sequential(\n",
       "    (0): Linear(in_features=1400, out_features=512, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (overviewff_output_feature): Sequential(\n",
       "    (0): Linear(in_features=61, out_features=512, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (transformer_train): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (overviewff_train): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (hybrid_stack): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiFetTransformer()\n",
    "# model.set_mode('ultimate')\n",
    "# modelpath = \"../input/optiver-realized-volatility-binarysentient-pytorch/07_1s_logret1n2_cnn_epoch_400_tloss_0.2393.pth\"\n",
    "modelpath = \"../output/models/18_MultiMod5_4klr_16Atten_2Layer_0.001_256_epoch_3_tloss_nan.pth\"\n",
    "checkpoint = torch.load(modelpath)\n",
    "model.load_state_dict(checkpoint['base'])\n",
    "# for k,v in model.feature_gen_models.items():\n",
    "#     v.load_state_dict(checkpoint[k])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'test'\n",
    "main_df = pd.read_csv(os.path.join(DATA_DIRECTORY,f'{mode}.csv'))\n",
    "submission_data = []\n",
    "\n",
    "output_scaling = realize_volatility_scale_factor\n",
    "\n",
    "for Feature_X, feature_y in iterate_optiver_dataset_for_evaluation(mode):\n",
    "    \n",
    "    row_ids = Feature_X['row_id']\n",
    "\n",
    "    pred = model(Feature_X) \n",
    "#     print(pred)\n",
    "#     print(Feature_X)\n",
    "#     print(feature_y)\n",
    "#     input()\n",
    "    predicted_volatility = (pred/realize_volatility_scale_factor).tolist()\n",
    "    for idx, row_id in enumerate(row_ids):\n",
    "        submission_data.append({'row_id':row_id, 'target':predicted_volatility[idx][0]})\n",
    "        \n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df = main_df.merge(submission_df,on='row_id',how='left')\n",
    "submission_df = submission_df.rename(columns={'target_y':'target'})\n",
    "# submission_df\n",
    "# print(submission_df.columns)\n",
    "submission_df[['row_id','target']].to_csv(\"submission.csv\", index=False)\n",
    "# for idx, (X,y) in enumerate(dataset):\n",
    "#     print(idx, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.001509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.001509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.000537\n",
       "1   0-32  0.001509\n",
       "2   0-34  0.001509"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0+cu111'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
