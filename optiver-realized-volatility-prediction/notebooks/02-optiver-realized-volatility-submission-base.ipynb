{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from optiver_features_handler import get_features_map_for_stock, get_row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\n",
    "TRADE_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_train.parquet\")\n",
    "TRADE_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_test.parquet\")\n",
    "BOOK_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_train.parquet\")\n",
    "BOOK_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_test.parquet\")\n",
    "OUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\n",
    "os.makedirs(OUTPUT_DIRECTORY,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIRECTORY,\"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIRECTORY,\"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interval_seconds = 5\n",
    "data_intervals_count = int(600/data_interval_seconds)\n",
    "class OptiverRealizedVolatilityDataset(Dataset):\n",
    "    def __init__(self, data_directory, mode=\"train\", lazy_load=True):\n",
    "        \"\"\"initializes Optiver Competition dataset\n",
    "        `mode`: train|test\n",
    "        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n",
    "        \"\"\"\n",
    "        print(\"INIT: OptiverRealizedVolatilityDataset\")\n",
    "        if mode.lower() not in ['train','test']:\n",
    "            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n",
    "        self.data_directory = data_directory\n",
    "        self.mode = mode.lower()\n",
    "        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n",
    "#         if self.mode == 'train':\n",
    "#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n",
    "        if self.mode == 'test':\n",
    "            self.main_df['target'] = 0\n",
    "        \n",
    "        self.cache_stocks_done_set = set()\n",
    "        # this is our final features lookup where we park all our features which can be addressed by row_id\n",
    "        # which is individual train/test.csv row id using 'stock_id`-`time_id`\n",
    "        self.cache_rowid_feature_map = {}\n",
    "        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n",
    "        targets = self.main_df['target'].tolist()\n",
    "        self.stock_possible_timeids_list = {}\n",
    "        for idx, row_id in enumerate(row_id_series.tolist()):\n",
    "            stock_id = int(row_id.split('-')[0])\n",
    "            time_id = int(row_id.split('-')[1])\n",
    "            self.cache_rowid_feature_map[row_id] = {'target':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n",
    "            \n",
    "            # below code is to make sure what timeids we expect from stock data extractor\n",
    "            # in case of missing parquet files we'll have to know the keys to fill default values into\n",
    "            if stock_id not in self.stock_possible_timeids_list:\n",
    "                self.stock_possible_timeids_list[stock_id] = []\n",
    "            self.stock_possible_timeids_list[stock_id].append(time_id)\n",
    "            \n",
    "        \n",
    "        if lazy_load == False:\n",
    "            worker_data = []\n",
    "            for gkey, gdf in self.main_df.groupby(['stock_id']):\n",
    "                worker_data.append((self.data_directory, self.mode, gkey))\n",
    "#             print(\"---------- CPU COUNG:\", multiprocessing.cpu_count())\n",
    "            # NOTE: this was hell of a hunt; this windows and pytorch and jupyter combination is too tedious\n",
    "            #       make sure the function that we distribute don't call pytorch\n",
    "            chunksize = multiprocessing.cpu_count() * 1\n",
    "            processed = 0\n",
    "            for worker_data_chunk in [worker_data[i * chunksize:(i + 1) * chunksize] for i in range((len(worker_data) + chunksize - 1) // chunksize )]:\n",
    "                with Pool(multiprocessing.cpu_count()) as p:\n",
    "                    \n",
    "                    feature_set_list = p.starmap(get_features_map_for_stock, worker_data_chunk)\n",
    "                    \n",
    "                    for feature_map in feature_set_list:\n",
    "                        for rowid, features_dict in feature_map.items():\n",
    "                            for fkey,fval in features_dict.items():\n",
    "                                self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                            self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                        # udpate the indications that we've already fetched this stock and the lazy loader code won't fetch this again\n",
    "                        self.cache_stocks_done_set.add(int(rowid.split('-')[0]))\n",
    "                    \n",
    "                    processed += chunksize\n",
    "                    print(f\"Processed and loaded {processed} stocks features.\")\n",
    "    \n",
    "    def __cache_generate_features(self, main_stock_id, main_time_id):\n",
    "            \n",
    "            main_row_id = get_row_id(main_stock_id, main_time_id)\n",
    "            if main_stock_id not in self.cache_stocks_done_set:\n",
    "#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))   \n",
    "                # we'll combine the featureset with the bigger feature set of all stocks\n",
    "                feature_map = get_features_map_for_stock(self.data_directory, self.mode, main_stock_id)\n",
    "                # NOTE: sometime we might now have parquet files in that case we'll have 3 entried in .csv while only 1 gets returned in feature map\n",
    "                # we need to cover for that disparity\n",
    "                for time_id in self.stock_possible_timeids_list[main_stock_id]:\n",
    "                    expected_row_id = get_row_id(main_stock_id, time_id)\n",
    "                    if expected_row_id not in feature_map:\n",
    "                        feature_map[expected_row_id] = {}\n",
    "                for rowid, features_dict in feature_map.items():\n",
    "                    for fkey,fval in features_dict.items():\n",
    "                        self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                    self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                self.cache_stocks_done_set.add(main_stock_id)\n",
    "#             print(self.cache_rowid_feature_map[main_row_id])\n",
    "#             print(torch.tensor([self.cache_rowid_feature_map[main_row_id].get('book_realized_volatility',0)]))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map[main_row_id].get('log_return1_2s', [0]*(int(600/2)))))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map.get('book_directional_volume1_2s', [0]*(int(600/2)))))\n",
    "            return self.cache_rowid_feature_map[main_row_id]\n",
    "        \n",
    "    @staticmethod\n",
    "    def transform_to_01_realized_volatility_linear_data(features_dict):\n",
    "        return (\n",
    "                {\n",
    "                    'row_id':features_dict['row_id'],\n",
    "                    'stock_id':torch.tensor(features_dict['stock_id'], dtype=torch.float32),\n",
    "#                     'book_realized_volatility':torch.tensor([features_dict.get('book_realized_volatility',0)]),\n",
    "                    # TRADE FEATURES\n",
    "                    'logrett_xs': torch.tensor(np.nan_to_num(features_dict.get('logrett_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_volume_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_volume_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_ordercount_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_ordercount_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    \n",
    "                    'trade_price_min': torch.tensor(np.nan_to_num(features_dict.get('trade_price_min', 0)), dtype=torch.float32),\n",
    "                    'trade_price_mean': torch.tensor(np.nan_to_num(features_dict.get('trade_price_mean', 0)), dtype=torch.float32),\n",
    "                    'trade_price_max': torch.tensor(np.nan_to_num(features_dict.get('trade_price_max', 0)), dtype=torch.float32),\n",
    "                    'trade_money_turnover_mean': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_mean', 0)), dtype=torch.float32),\n",
    "                    'trade_money_turnover_std': torch.tensor(np.nan_to_num(features_dict.get('trade_money_turnover_std', 0)), dtype=torch.float32),\n",
    "                    # BOOK FEATURES\n",
    "                    'logret1_xs': torch.tensor(np.nan_to_num(features_dict.get('logret1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'logret2_xs': torch.tensor(np.nan_to_num(features_dict.get('logret2_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_directional_volume1_xs': torch.tensor(np.nan_to_num(features_dict.get('book_directional_volume1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_directional_volume2_xs': torch.tensor(np.nan_to_num(features_dict.get('book_directional_volume2_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_price_spread1_xs': torch.tensor(np.nan_to_num(features_dict.get('book_price_spread1_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_price_spread2_xs': torch.tensor(np.nan_to_num(features_dict.get('book_price_spread2_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_bid_spread_xs': torch.tensor(np.nan_to_num(features_dict.get('book_bid_spread_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_ask_spread_xs': torch.tensor(np.nan_to_num(features_dict.get('book_ask_spread_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_total_volume_xs': torch.tensor(np.nan_to_num(features_dict.get('book_total_volume_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_volume_imbalance_xs': torch.tensor(np.nan_to_num(features_dict.get('book_volume_imbalance_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    \n",
    "                    'book_price_min': torch.tensor(np.nan_to_num(features_dict.get('book_price_min', 0)), dtype=torch.float32),\n",
    "                    'book_price_mean': torch.tensor(np.nan_to_num(features_dict.get('book_price_mean', 0)), dtype=torch.float32),\n",
    "                    'book_price_max': torch.tensor(np.nan_to_num(features_dict.get('book_price_max', 0)), dtype=torch.float32),\n",
    "                    'book_money_turnover_mean': torch.tensor(np.nan_to_num(features_dict.get('book_money_turnover_mean', 0)), dtype=torch.float32),\n",
    "                    'book_money_turnover_std': torch.tensor(np.nan_to_num(features_dict.get('book_money_turnover_std', 0)), dtype=torch.float32),\n",
    "                    \n",
    "#                     'askp2_1s':torch.tensor(features_dict.get('askp2_1s', [0]*(int(600/1)))),\n",
    "#                     'book_directional_volume1_1s':torch.tensor(features_dict.get('book_directional_volume1_1s', [0]*(int(600/1)))) \n",
    "                },\n",
    "                torch.tensor([features_dict['target']])\n",
    "#                 [features_dict['target']]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: handle for num_workers more than 0\n",
    "        #      using https://pytorch.org/docs/stable/data.html\n",
    "        #      using torch.util.data.get_worker_info()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        stock_id = self.main_df.at[idx, 'stock_id']\n",
    "        time_id = self.main_df.at[idx, 'time_id']\n",
    "        x,y = self.__cache_generate_features(stock_id,time_id)\n",
    "#         x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT: OptiverRealizedVolatilityDataset\n"
     ]
    }
   ],
   "source": [
    "# dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\")\n",
    "dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"test\", lazy_load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'row_id': '0-4',\n",
       "  'stock_id': tensor(0.),\n",
       "  'logrett_xs': tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7399e-05, -1.7280e-05,\n",
       "          -1.7399e-05, -1.7280e-05, -1.7400e-05, -1.7400e-05, -1.7281e-05,\n",
       "          -1.7401e-05, -1.7282e-05, -1.7401e-05, -1.7283e-05, -1.7402e-05,\n",
       "          -1.7402e-05, -1.7283e-05, -1.7403e-05, -1.7284e-05, -1.7404e-05,\n",
       "           3.3377e-06,  3.2185e-06,  3.3377e-06,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]),\n",
       "  'trade_volume_xs': tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          100.,   0.,   0., 100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  'trade_ordercount_xs': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 7., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'trade_price_min': tensor(1.0000),\n",
       "  'trade_price_mean': tensor(1.0001),\n",
       "  'trade_price_max': tensor(1.0003),\n",
       "  'trade_money_turnover_mean': tensor(8.3421),\n",
       "  'trade_money_turnover_std': tensor(69.3129),\n",
       "  'logret1_xs': tensor([0.0003, 0.0003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000]),\n",
       "  'logret2_xs': tensor([0.0003, 0.0003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000]),\n",
       "  'book_directional_volume1_xs': tensor([   9., -270.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]),\n",
       "  'book_directional_volume2_xs': tensor([-78., -86.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  'book_price_spread1_xs': tensor([0.0005, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "          0.0006, 0.0006, 0.0006]),\n",
       "  'book_price_spread2_xs': tensor([0.0010, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "          0.0012, 0.0012, 0.0012]),\n",
       "  'book_bid_spread_xs': tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
       "          0.0004, 0.0004, 0.0004]),\n",
       "  'book_ask_spread_xs': tensor([4.9203e-05, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04,\n",
       "          2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04, 2.4562e-04]),\n",
       "  'book_total_volume_xs': tensor([626., 426.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  'book_volume_imbalance_xs': tensor([ 69., 356.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  'book_price_min': tensor(1.0004),\n",
       "  'book_price_mean': tensor(1.0007),\n",
       "  'book_price_max': tensor(1.0007),\n",
       "  'book_money_turnover_mean': tensor(5.2272),\n",
       "  'book_money_turnover_std': tensor(51.2135)},\n",
       " tensor([0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = None\n",
    "\n",
    "\n",
    "def loss_fn_mse(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)))\n",
    "\n",
    "def loss_fn_mspe(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)/y))\n",
    "\n",
    "def loss_fn_orig(y, pred):\n",
    "    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, mode='both', use_stock_id = True):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.use_stock_id = use_stock_id\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mode = mode\n",
    "        self.cnn_stack = nn.Sequential(\n",
    "            nn.Conv1d(7, 24, kernel_size=6, stride=2, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(24, 32, kernel_size=4, stride=2, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(32, 48, kernel_size=3, stride=1, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(48, 64, kernel_size=2, stride=1, padding=0),\n",
    "\n",
    "        )\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(1600, 1024),\n",
    "            nn.GELU(),\n",
    "#             nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "\n",
    "        )\n",
    "        self.linear_hybrid = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, feature_dict):\n",
    "#         logits = self.basic_stack(x)\n",
    "#         x = self.flatten(x)\n",
    "        x = torch.cat([\n",
    "#                             feature_dict['logrett_xs'].to(device)*10000, \n",
    "#                                torch.log(feature_dict['trade_volume_xs'].to(device)+0.001),\n",
    "#                               torch.log(feature_dict['trade_ordercount_xs'].to(device)+0.001),\n",
    "#                             feature_dict['trade_volume_xs'].to(device),\n",
    "#                                         feature_dict['trade_ordercount_xs'].to(device),\n",
    "#                                         feature_dict['book_total_volume_xs'].to(device),\n",
    "#                                         feature_dict['book_volume_imbalance_xs'].to(device)\n",
    "#                             feature_dict['logrett_xs'].to(device)*10000, \n",
    "                               torch.log(feature_dict['trade_volume_xs'].to(device)+0.001),\n",
    "                              torch.log(feature_dict['trade_ordercount_xs'].to(device)+0.001),\n",
    "                                feature_dict['logret1_xs'].to(device)*10000,\n",
    "                            \n",
    "                                 feature_dict['logret2_xs'].to(device)*10000,\n",
    "                                    feature_dict['book_price_spread1_xs'].to(device)*10000, \n",
    "#                                 feature_dict['book_price_spread2_xs'].to(device)*10000, \n",
    "#                                 feature_dict['book_bid_spread_xs'].to(device)*10000, \n",
    "#                                 feature_dict['book_ask_spread_xs'].to(device)*10000, \n",
    "#                                  feature_dict['book_directional_volume1_xs'].to(device),\n",
    "#                                 feature_dict['book_price_spread1_xs'].to(device)*1000,\n",
    "                                torch.log(feature_dict['book_total_volume_xs'].to(device)+0.001),\n",
    "                                torch.log(feature_dict['book_volume_imbalance_xs'].to(device)+0.001),\n",
    "# #                              feature_dict['book_dirvolume_xs'],\n",
    "                          ], 1)\n",
    "\n",
    "#         x = torch.nan_to_num(feature_dict['logrett_xs']).type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        \n",
    "#         print(x)\n",
    "#         input()\n",
    "#         if torch.isnan(x).any():\n",
    "# #             print(x)\n",
    "#             print(feature_dict)\n",
    "#             input()\n",
    "        x = x.type(torch.cuda.FloatTensor)\n",
    "        x = x.to(device)\n",
    "        x = x.reshape(-1,7,data_intervals_count)\n",
    "        \n",
    "        logits = self.cnn_stack(x)\n",
    "        logits = self.flatten(logits)\n",
    "        \n",
    "       \n",
    "        \n",
    "        logits = self.linear_stack(logits)\n",
    "#         logits = torch.cat( [logits, torch.log(feature_dict['trade_money_turnover_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001), \n",
    "#                                        torch.log(feature_dict['trade_price_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                        torch.log(feature_dict['book_money_turnover_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                        torch.log(feature_dict['book_price_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001)\n",
    "#                                       ], 1)\n",
    "        \n",
    "        if self.use_stock_id:\n",
    "            stock_id = torch.tensor(feature_dict['stock_id']).reshape(-1,1)\n",
    "            stock_id = stock_id.to(device)\n",
    "            logits = torch.cat([logits, stock_id], 1)\n",
    "            \n",
    "        logits = self.linear_hybrid(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (cnn_stack): Sequential(\n",
       "    (0): Conv1d(7, 24, kernel_size=(6,), stride=(2,))\n",
       "    (1): GELU()\n",
       "    (2): Conv1d(24, 32, kernel_size=(4,), stride=(2,))\n",
       "    (3): GELU()\n",
       "    (4): Conv1d(32, 48, kernel_size=(3,), stride=(1,))\n",
       "    (5): GELU()\n",
       "    (6): Conv1d(48, 64, kernel_size=(2,), stride=(1,))\n",
       "  )\n",
       "  (linear_stack): Sequential(\n",
       "    (0): Linear(in_features=1600, out_features=1024, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (3): GELU()\n",
       "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (5): GELU()\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): GELU()\n",
       "  )\n",
       "  (linear_hybrid): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "modelpath = \"../input/optiver-realized-volatility-binarysentient-pytorch/07_1s_logret1n2_cnn_epoch_400_tloss_0.2393.pth\"\n",
    "modelpath = \"../output/models/08_ultimate_hybridEXPX5_1LGRU_5s_StkFalse_0.0001_2_epoch_1_tloss_0.4187.pth\"\n",
    "model.load_state_dict(torch.load(modelpath))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nisha\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\ipykernel_launcher.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=3,\n",
    "                        shuffle=True, num_workers=0, pin_memory=True)\n",
    "size = len(dataloader.dataset)\n",
    "submission_data = []\n",
    "data_interval_len = int(600/1)\n",
    "output_scaling = 10000\n",
    "data_ohlc_sample_len = 1 # 1 for each of open high low close\n",
    "for batch, (Feature_X, feature_y) in enumerate(dataloader):\n",
    "    row_ids = Feature_X['row_id']\n",
    "    y = feature_y.to(device) * output_scaling \n",
    "    \n",
    "    pred = model(Feature_X) \n",
    "#     print(pred)\n",
    "    predicted_volatility = (pred/1000).tolist()\n",
    "    for idx, row_id in enumerate(row_ids):\n",
    "        submission_data.append({'row_id':row_id, 'target':predicted_volatility[idx][0]})\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df = dataset.main_df.merge(submission_df,on='row_id',how='left')\n",
    "submission_df = submission_df.rename(columns={'target_y':'target'})\n",
    "# submission_df\n",
    "# print(submission_df.columns)\n",
    "submission_df[['row_id','target']].to_csv(\"submission.csv\", index=False)\n",
    "# for idx, (X,y) in enumerate(dataset):\n",
    "#     print(idx, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.020016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.007832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.007832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.020016\n",
       "1   0-32  0.007832\n",
       "2   0-34  0.007832"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0+cu111'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
