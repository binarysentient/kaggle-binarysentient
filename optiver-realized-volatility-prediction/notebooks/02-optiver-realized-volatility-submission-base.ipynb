{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom optiver_features_handler import get_features_map_for_stock, get_row_id","metadata":{"execution":{"iopub.status.busy":"2021-08-24T12:55:55.654654Z","iopub.execute_input":"2021-08-24T12:55:55.654964Z","iopub.status.idle":"2021-08-24T12:55:55.659810Z","shell.execute_reply.started":"2021-08-24T12:55:55.654934Z","shell.execute_reply":"2021-08-24T12:55:55.658589Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\nTRADE_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_train.parquet\")\nTRADE_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_test.parquet\")\nBOOK_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_train.parquet\")\nBOOK_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_test.parquet\")\nOUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\nos.makedirs(OUTPUT_DIRECTORY,exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T12:53:37.915400Z","iopub.execute_input":"2021-08-24T12:53:37.915740Z","iopub.status.idle":"2021-08-24T12:53:37.921422Z","shell.execute_reply.started":"2021-08-24T12:53:37.915713Z","shell.execute_reply":"2021-08-24T12:53:37.920346Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(DATA_DIRECTORY,\"train.csv\"))\ntest_df = pd.read_csv(os.path.join(DATA_DIRECTORY,\"test.csv\"))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T12:53:39.321887Z","iopub.execute_input":"2021-08-24T12:53:39.322195Z","iopub.status.idle":"2021-08-24T12:53:39.471571Z","shell.execute_reply.started":"2021-08-24T12:53:39.322165Z","shell.execute_reply":"2021-08-24T12:53:39.470744Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class OptiverRealizedVolatilityDataset(Dataset):\n    def __init__(self, data_directory, mode=\"train\", lazy_load=True):\n        \"\"\"initializes Optiver Competition dataset\n        `mode`: train|test\n        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n        \"\"\"\n        print(\"INIT: OptiverRealizedVolatilityDataset\")\n        if mode.lower() not in ['train','test']:\n            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n        self.data_directory = data_directory\n        self.mode = mode.lower()\n        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n#         if self.mode == 'train':\n#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n        if self.mode == 'test':\n            self.main_df['target'] = 0\n        \n        self.cache_stocks_done_set = set()\n        # this is our final features lookup where we park all our features which can be addressed by row_id\n        # which is individual train/test.csv row id using 'stock_id`-`time_id`\n        self.cache_rowid_feature_map = {}\n        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n        targets = self.main_df['target'].tolist()\n        self.stock_possible_timeids_list = {}\n        for idx, row_id in enumerate(row_id_series.tolist()):\n            stock_id = int(row_id.split('-')[0])\n            time_id = int(row_id.split('-')[1])\n            self.cache_rowid_feature_map[row_id] = {'target':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n            \n            # below code is to make sure what timeids we expect from stock data extractor\n            # in case of missing parquet files we'll have to know the keys to fill default values into\n            if stock_id not in self.stock_possible_timeids_list:\n                self.stock_possible_timeids_list[stock_id] = []\n            self.stock_possible_timeids_list[stock_id].append(time_id)\n            \n        \n        if lazy_load == False:\n            worker_data = []\n            for gkey, gdf in self.main_df.groupby(['stock_id']):\n                worker_data.append((self.data_directory, self.mode, gkey))\n#             print(\"---------- CPU COUNG:\", multiprocessing.cpu_count())\n            # NOTE: this was hell of a hunt; this windows and pytorch and jupyter combination is too tedious\n            #       make sure the function that we distribute don't call pytorch\n            with Pool(multiprocessing.cpu_count()) as p:\n                feature_set_list = p.starmap(get_features_map_for_stock, worker_data)\n                for feature_map in feature_set_list:\n                    for rowid, features_dict in feature_map.items():\n                        for fkey,fval in features_dict.items():\n                            self.cache_rowid_feature_map[rowid][fkey] = fval\n                        self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n                    # udpate the indications that we've already fetched this stock and the lazy loader code won't fetch this again\n                    self.cache_stocks_done_set.add(int(rowid.split('-')[0]))\n    \n    def __cache_generate_features(self, main_stock_id, main_time_id):\n            \n            \n            main_row_id = get_row_id(main_stock_id, main_time_id)\n            if main_stock_id not in self.cache_stocks_done_set:\n#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))   \n                # we'll combine the featureset with the bigger feature set of all stocks\n                feature_map = get_features_map_for_stock(self.data_directory, self.mode, main_stock_id)\n                # NOTE: sometime we might now have parquet files in that case we'll have 3 entried in .csv while only 1 gets returned in feature map\n                # we need to cover for that disparity\n                for time_id in self.stock_possible_timeids_list[main_stock_id]:\n                    expected_row_id = get_row_id(main_stock_id, time_id)\n                    if expected_row_id not in feature_map:\n                        feature_map[expected_row_id] = {}\n                for rowid, features_dict in feature_map.items():\n                    for fkey,fval in features_dict.items():\n                        self.cache_rowid_feature_map[rowid][fkey] = fval\n                    self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n                self.cache_stocks_done_set.add(main_stock_id)\n#             print(self.cache_rowid_feature_map[main_row_id])\n#             print(torch.tensor([self.cache_rowid_feature_map[main_row_id].get('book_realized_volatility',0)]))\n#             print(torch.tensor(self.cache_rowid_feature_map[main_row_id].get('log_return1_2s', [0]*(int(600/2)))))\n#             print(torch.tensor(self.cache_rowid_feature_map.get('book_directional_volume1_2s', [0]*(int(600/2)))))\n            return self.cache_rowid_feature_map[main_row_id]\n        \n    @staticmethod\n    def transform_to_01_realized_volatility_linear_data(features_dict):\n        return (\n                {\n                    'row_id':features_dict['row_id'],\n                    'book_realized_volatility':torch.tensor([features_dict.get('book_realized_volatility',0)]),\n                    'log_return1_2s':torch.tensor(features_dict.get('log_return1_2s', [0]*(int(600/2)))),\n                    'book_directional_volume1_2s':torch.tensor(features_dict.get('book_directional_volume1_2s', [0]*(int(600/2)))) \n                },\n                torch.tensor([features_dict['target']])\n#                 [features_dict['target']]\n        )\n    \n    def __len__(self):\n        return len(self.main_df)\n    \n    def __getitem__(self, idx):\n        #TODO: handle for num_workers more than 0\n        #      using https://pytorch.org/docs/stable/data.html\n        #      using torch.util.data.get_worker_info()\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        stock_id = self.main_df.at[idx, 'stock_id']\n        time_id = self.main_df.at[idx, 'time_id']\n        x,y = self.__cache_generate_features(stock_id,time_id)\n#         x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n        return x,y","metadata":{"execution":{"iopub.status.busy":"2021-08-24T13:13:28.809871Z","iopub.execute_input":"2021-08-24T13:13:28.810181Z","iopub.status.idle":"2021-08-24T13:13:28.830838Z","shell.execute_reply.started":"2021-08-24T13:13:28.810152Z","shell.execute_reply":"2021-08-24T13:13:28.830014Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\")\ndataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"test\", lazy_load=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T13:13:31.448363Z","iopub.execute_input":"2021-08-24T13:13:31.448679Z","iopub.status.idle":"2021-08-24T13:13:31.461982Z","shell.execute_reply.started":"2021-08-24T13:13:31.448651Z","shell.execute_reply":"2021-08-24T13:13:31.460999Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"INIT: OptiverRealizedVolatilityDataset\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset[1]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T13:13:32.558901Z","iopub.execute_input":"2021-08-24T13:13:32.559260Z","iopub.status.idle":"2021-08-24T13:13:32.591571Z","shell.execute_reply.started":"2021-08-24T13:13:32.559212Z","shell.execute_reply":"2021-08-24T13:13:32.590622Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"({'row_id': '0-32',\n  'book_realized_volatility': tensor([0]),\n  'log_return1_2s': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n  'book_directional_volume1_2s': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])},\n tensor([0]))"},"metadata":{}}]},{"cell_type":"code","source":"class NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.basic_stack = nn.Sequential(\n            nn.Linear(int(600/2),1024),\n            nn.ReLU(),\n            nn.Linear(1024,2048),\n            nn.ReLU(),\n            nn.Linear(2048,512),\n            nn.ReLU(),\n            nn.Linear(512,256),\n            nn.ReLU(),\n            nn.Linear(256,128),\n            nn.ReLU(),\n            nn.Linear(128,1)\n        )\n\n    def forward(self, x):\n        logits = self.basic_stack(x)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2021-08-24T13:13:34.505088Z","iopub.execute_input":"2021-08-24T13:13:34.505440Z","iopub.status.idle":"2021-08-24T13:13:34.511497Z","shell.execute_reply.started":"2021-08-24T13:13:34.505409Z","shell.execute_reply":"2021-08-24T13:13:34.510599Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\n# use_cuda = False\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\ntorch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2021-08-24T13:13:35.674091Z","iopub.execute_input":"2021-08-24T13:13:35.674445Z","iopub.status.idle":"2021-08-24T13:13:35.679181Z","shell.execute_reply.started":"2021-08-24T13:13:35.674414Z","shell.execute_reply":"2021-08-24T13:13:35.677944Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"model = NeuralNetwork()\n\nmodel.load_state_dict(torch.load(\"../input/optiver-realized-volatility-binarysentient-pytorch/05_2s_datanormalized_logreturn_sequencial.pth\"))\nmodel.to(device)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T13:13:36.462811Z","iopub.execute_input":"2021-08-24T13:13:36.463139Z","iopub.status.idle":"2021-08-24T13:13:36.525217Z","shell.execute_reply.started":"2021-08-24T13:13:36.463109Z","shell.execute_reply":"2021-08-24T13:13:36.524277Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"NeuralNetwork(\n  (basic_stack): Sequential(\n    (0): Linear(in_features=300, out_features=1024, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=1024, out_features=2048, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=2048, out_features=512, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=512, out_features=256, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=256, out_features=128, bias=True)\n    (9): ReLU()\n    (10): Linear(in_features=128, out_features=1, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"dataloader = DataLoader(dataset, batch_size=3,\n                        shuffle=True, num_workers=0, pin_memory=True)\nsize = len(dataloader.dataset)\nsubmission_data = []\nfor batch, (Feature_X, feature_y) in enumerate(dataloader):\n    row_ids = Feature_X['row_id']\n    X = Feature_X['log_return1_2s'] * 1000\n        \n    y = feature_y * 100\n#         y = Feature_X['book_realized_volatility'] * 100\n    X = X.type(torch.cuda.FloatTensor)    \n    y = y.type(torch.cuda.FloatTensor)\n\n\n    X = X.to(device)\n    y = y.to(device)\n    feature_y = feature_y.to(device)\n    pred = model(X)   \n#     print(pred)\n    predicted_volatility = (pred/100).tolist()\n    for idx, row_id in enumerate(row_ids):\n        submission_data.append({'row_id':row_id, 'target':predicted_volatility[idx][0]})\nsubmission_df = pd.DataFrame(submission_data)\nsubmission_df = dataset.main_df.merge(submission_df,on='row_id',how='left')\nsubmission_df = submission_df.rename(columns={'target_y':'target'})\n# submission_df\n# print(submission_df.columns)\nsubmission_df[['row_id','target']].to_csv(\"submission.csv\", index=False)\n# for idx, (X,y) in enumerate(dataset):\n#     print(idx, X)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T13:13:37.535227Z","iopub.execute_input":"2021-08-24T13:13:37.535622Z","iopub.status.idle":"2021-08-24T13:13:37.554453Z","shell.execute_reply.started":"2021-08-24T13:13:37.535591Z","shell.execute_reply":"2021-08-24T13:13:37.553377Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-24T13:13:40.604130Z","iopub.execute_input":"2021-08-24T13:13:40.604505Z","iopub.status.idle":"2021-08-24T13:13:40.621269Z","shell.execute_reply.started":"2021-08-24T13:13:40.604471Z","shell.execute_reply":"2021-08-24T13:13:40.620311Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"  row_id    target\n0    0-4  0.000645\n1   0-32  0.000362\n2   0-34  0.000362","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0-4</td>\n      <td>0.000645</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0-32</td>\n      <td>0.000362</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0-34</td>\n      <td>0.000362</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}