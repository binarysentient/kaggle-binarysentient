{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom optiver_features_handler import get_features_map_for_stock, get_row_id","metadata":{"execution":{"iopub.status.busy":"2021-08-26T05:56:50.070528Z","iopub.execute_input":"2021-08-26T05:56:50.070914Z","iopub.status.idle":"2021-08-26T05:56:50.076750Z","shell.execute_reply.started":"2021-08-26T05:56:50.070880Z","shell.execute_reply":"2021-08-26T05:56:50.075451Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\nTRADE_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_train.parquet\")\nTRADE_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_test.parquet\")\nBOOK_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_train.parquet\")\nBOOK_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_test.parquet\")\nOUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\nos.makedirs(OUTPUT_DIRECTORY,exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T05:56:50.079062Z","iopub.execute_input":"2021-08-26T05:56:50.079821Z","iopub.status.idle":"2021-08-26T05:56:50.090636Z","shell.execute_reply.started":"2021-08-26T05:56:50.079772Z","shell.execute_reply":"2021-08-26T05:56:50.089246Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(DATA_DIRECTORY,\"train.csv\"))\ntest_df = pd.read_csv(os.path.join(DATA_DIRECTORY,\"test.csv\"))","metadata":{"execution":{"iopub.status.busy":"2021-08-26T05:56:50.093822Z","iopub.execute_input":"2021-08-26T05:56:50.094713Z","iopub.status.idle":"2021-08-26T05:56:50.251746Z","shell.execute_reply.started":"2021-08-26T05:56:50.094662Z","shell.execute_reply":"2021-08-26T05:56:50.250593Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"class OptiverRealizedVolatilityDataset(Dataset):\n    def __init__(self, data_directory, mode=\"train\", lazy_load=True):\n        \"\"\"initializes Optiver Competition dataset\n        `mode`: train|test\n        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n        \"\"\"\n        print(\"INIT: OptiverRealizedVolatilityDataset\")\n        if mode.lower() not in ['train','test']:\n            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n        self.data_directory = data_directory\n        self.mode = mode.lower()\n        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n#         if self.mode == 'train':\n#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n        if self.mode == 'test':\n            self.main_df['target'] = 0\n        \n        self.cache_stocks_done_set = set()\n        # this is our final features lookup where we park all our features which can be addressed by row_id\n        # which is individual train/test.csv row id using 'stock_id`-`time_id`\n        self.cache_rowid_feature_map = {}\n        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n        targets = self.main_df['target'].tolist()\n        self.stock_possible_timeids_list = {}\n        for idx, row_id in enumerate(row_id_series.tolist()):\n            stock_id = int(row_id.split('-')[0])\n            time_id = int(row_id.split('-')[1])\n            self.cache_rowid_feature_map[row_id] = {'target':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n            \n            # below code is to make sure what timeids we expect from stock data extractor\n            # in case of missing parquet files we'll have to know the keys to fill default values into\n            if stock_id not in self.stock_possible_timeids_list:\n                self.stock_possible_timeids_list[stock_id] = []\n            self.stock_possible_timeids_list[stock_id].append(time_id)\n            \n        \n        if lazy_load == False:\n            worker_data = []\n            for gkey, gdf in self.main_df.groupby(['stock_id']):\n                worker_data.append((self.data_directory, self.mode, gkey))\n#             print(\"---------- CPU COUNG:\", multiprocessing.cpu_count())\n            # NOTE: this was hell of a hunt; this windows and pytorch and jupyter combination is too tedious\n            #       make sure the function that we distribute don't call pytorch\n            chunksize = multiprocessing.cpu_count() * 2\n            processed = 0\n            for worker_data_chunk in [worker_data[i * chunksize:(i + 1) * chunksize] for i in range((len(worker_data) + chunksize - 1) // chunksize )]:\n                with Pool(multiprocessing.cpu_count()) as p:\n                    \n                    feature_set_list = p.starmap(get_features_map_for_stock, worker_data_chunk)\n                    \n                    for feature_map in feature_set_list:\n                        for rowid, features_dict in feature_map.items():\n                            for fkey,fval in features_dict.items():\n                                self.cache_rowid_feature_map[rowid][fkey] = fval\n                            self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n                        # udpate the indications that we've already fetched this stock and the lazy loader code won't fetch this again\n                        self.cache_stocks_done_set.add(int(rowid.split('-')[0]))\n                    \n                    processed += chunksize\n                    print(f\"Processed and loaded {processed} stocks features.\")\n    \n    def __cache_generate_features(self, main_stock_id, main_time_id):\n            \n            \n            main_row_id = get_row_id(main_stock_id, main_time_id)\n            if main_stock_id not in self.cache_stocks_done_set:\n#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))   \n                # we'll combine the featureset with the bigger feature set of all stocks\n                feature_map = get_features_map_for_stock(self.data_directory, self.mode, main_stock_id)\n                # NOTE: sometime we might now have parquet files in that case we'll have 3 entried in .csv while only 1 gets returned in feature map\n                # we need to cover for that disparity\n                for time_id in self.stock_possible_timeids_list[main_stock_id]:\n                    expected_row_id = get_row_id(main_stock_id, time_id)\n                    if expected_row_id not in feature_map:\n                        feature_map[expected_row_id] = {}\n                for rowid, features_dict in feature_map.items():\n                    for fkey,fval in features_dict.items():\n                        self.cache_rowid_feature_map[rowid][fkey] = fval\n                    self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n                self.cache_stocks_done_set.add(main_stock_id)\n#             print(self.cache_rowid_feature_map[main_row_id])\n#             print(torch.tensor([self.cache_rowid_feature_map[main_row_id].get('book_realized_volatility',0)]))\n#             print(torch.tensor(self.cache_rowid_feature_map[main_row_id].get('log_return1_2s', [0]*(int(600/2)))))\n#             print(torch.tensor(self.cache_rowid_feature_map.get('book_directional_volume1_2s', [0]*(int(600/2)))))\n            return self.cache_rowid_feature_map[main_row_id]\n        \n    @staticmethod\n    def transform_to_01_realized_volatility_linear_data(features_dict):\n        return (\n                {\n                    'row_id':features_dict['row_id'],\n#                     'book_realized_volatility':torch.tensor([features_dict.get('book_realized_volatility',0)]),\n                    'log_return1_1s':torch.tensor(features_dict.get('log_return1_1s', [0]*(int(600/1)))),\n                    'log_return2_1s':torch.tensor(features_dict.get('log_return2_1s', [0]*(int(600/1)))),\n#                     'book_directional_volume1_1s':torch.tensor(features_dict.get('book_directional_volume1_1s', [0]*(int(600/1)))) \n                },\n                torch.tensor([features_dict['target']])\n#                 [features_dict['target']]\n        )\n    \n    def __len__(self):\n        return len(self.main_df)\n    \n    def __getitem__(self, idx):\n        #TODO: handle for num_workers more than 0\n        #      using https://pytorch.org/docs/stable/data.html\n        #      using torch.util.data.get_worker_info()\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        stock_id = self.main_df.at[idx, 'stock_id']\n        time_id = self.main_df.at[idx, 'time_id']\n        x,y = self.__cache_generate_features(stock_id,time_id)\n#         x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n        return x,y","metadata":{"execution":{"iopub.status.busy":"2021-08-26T05:56:50.253827Z","iopub.execute_input":"2021-08-26T05:56:50.254321Z","iopub.status.idle":"2021-08-26T05:56:50.280491Z","shell.execute_reply.started":"2021-08-26T05:56:50.254273Z","shell.execute_reply":"2021-08-26T05:56:50.278836Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\")\ndataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"test\", lazy_load=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T05:56:50.282329Z","iopub.execute_input":"2021-08-26T05:56:50.282846Z","iopub.status.idle":"2021-08-26T05:56:50.303247Z","shell.execute_reply.started":"2021-08-26T05:56:50.282798Z","shell.execute_reply":"2021-08-26T05:56:50.301860Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"INIT: OptiverRealizedVolatilityDataset\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset[1]","metadata":{"execution":{"iopub.status.busy":"2021-08-26T05:56:50.317787Z","iopub.execute_input":"2021-08-26T05:56:50.318272Z","iopub.status.idle":"2021-08-26T05:56:50.364518Z","shell.execute_reply.started":"2021-08-26T05:56:50.318226Z","shell.execute_reply":"2021-08-26T05:56:50.363261Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"({'row_id': '0-32',\n  'log_return1_1s': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n  'log_return2_1s': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])},\n tensor([0]))"},"metadata":{}}]},{"cell_type":"code","source":"class NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.cnn_stack = nn.Sequential(\n            nn.Conv1d(2, 8, kernel_size=10, stride=2, padding=0),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv1d(8, 8, kernel_size=8, stride=2, padding=0),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n#             nn.Conv1d(8, 8, kernel_size=4, stride=2, padding=0), \n#             nn.ReLU(),\n#             nn.Dropout(0.1),\n        )\n        self.linear_stack = nn.Sequential(\n            nn.Linear(1160, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n#             nn.Dropout(0.3),\n#             nn.Linear(256, 64),\n#             nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n#         self.basic_stack = nn.Sequential(\n#             nn.Linear(int(600/2)*1,512),\n#             nn.ReLU(),\n#             nn.Dropout(0.4),\n#             nn.Linear(512,1024),\n#             nn.ReLU(),\n#             nn.Dropout(0.4),\n# #             nn.Linear(2048,1024),\n# #             nn.ReLU(),\n# #             nn.Dropout(),\n#             nn.Linear(1024,512),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),\n#             nn.Linear(512,128),\n#             nn.ReLU(),\n#             nn.Dropout(0.2),\n#             nn.Linear(128,128),\n#             nn.ReLU(),\n#             nn.Linear(128,1),\n#         )\n        \n    def forward(self, x):\n#         logits = self.basic_stack(x)\n#         x = self.flatten(x)\n        logits = self.cnn_stack(x)\n        logits = self.flatten(logits)\n        logits = self.linear_stack(logits)\n        return logits\n\n\n\ndef loss_fn_mse(y, pred):\n    return torch.mean(torch.square((y-pred)))\n\ndef loss_fn_mspe(y, pred):\n    return torch.mean(torch.square((y-pred)/y))\n\ndef loss_fn_orig(y, pred):\n    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))","metadata":{"execution":{"iopub.status.busy":"2021-08-26T05:56:50.367369Z","iopub.execute_input":"2021-08-26T05:56:50.367854Z","iopub.status.idle":"2021-08-26T05:56:50.379514Z","shell.execute_reply.started":"2021-08-26T05:56:50.367812Z","shell.execute_reply":"2021-08-26T05:56:50.378233Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\n# use_cuda = False\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\ntorch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2021-08-26T05:56:50.382087Z","iopub.execute_input":"2021-08-26T05:56:50.382966Z","iopub.status.idle":"2021-08-26T05:56:50.395672Z","shell.execute_reply.started":"2021-08-26T05:56:50.382918Z","shell.execute_reply":"2021-08-26T05:56:50.394460Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model = NeuralNetwork()\nmodelpath = \"../input/optiver-realized-volatility-binarysentient-pytorch/07_1s_logret1n2_cnn_epoch_400_tloss_0.2393.pth\"\nmodel.load_state_dict(torch.load(modelpath))\nmodel.to(device)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T05:56:50.397688Z","iopub.execute_input":"2021-08-26T05:56:50.398294Z","iopub.status.idle":"2021-08-26T05:56:50.429745Z","shell.execute_reply.started":"2021-08-26T05:56:50.398241Z","shell.execute_reply":"2021-08-26T05:56:50.428399Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"NeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (cnn_stack): Sequential(\n    (0): Conv1d(2, 8, kernel_size=(10,), stride=(2,))\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Conv1d(8, 8, kernel_size=(8,), stride=(2,))\n    (4): ReLU()\n    (5): Dropout(p=0.1, inplace=False)\n  )\n  (linear_stack): Sequential(\n    (0): Linear(in_features=1160, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.3, inplace=False)\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=1, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"dataloader = DataLoader(dataset, batch_size=3,\n                        shuffle=True, num_workers=0, pin_memory=True)\nsize = len(dataloader.dataset)\nsubmission_data = []\ndata_interval_len = int(600/1)\ndata_ohlc_sample_len = 1 # 1 for each of open high low close\nfor batch, (Feature_X, feature_y) in enumerate(dataloader):\n    row_ids = Feature_X['row_id']\n#     print(row_ids)\n    X = Feature_X['log_return1_1s'] * 10000\n    X = torch.cat([X, Feature_X['log_return2_1s']*10000], 1)\n#         X = torch.cat([X, Feature_X['book_directional_volume1_1s']/100], 1)\n    X = X.reshape(-1,2,data_interval_len*data_ohlc_sample_len*1)\n        \n    y = feature_y * 1000\n#         y = Feature_X['book_realized_volatility'] * 100\n    X = X.type(torch.cuda.FloatTensor)    \n    y = y.type(torch.cuda.FloatTensor)\n\n\n    X = X.to(device)\n    y = y.to(device)\n    \n    pred = model(X)   \n#     print(pred)\n    predicted_volatility = (pred/1000).tolist()\n    for idx, row_id in enumerate(row_ids):\n        submission_data.append({'row_id':row_id, 'target':predicted_volatility[idx][0]})\nsubmission_df = pd.DataFrame(submission_data)\nsubmission_df = dataset.main_df.merge(submission_df,on='row_id',how='left')\nsubmission_df = submission_df.rename(columns={'target_y':'target'})\n# submission_df\n# print(submission_df.columns)\nsubmission_df[['row_id','target']].to_csv(\"submission.csv\", index=False)\n# for idx, (X,y) in enumerate(dataset):\n#     print(idx, X)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T05:56:50.431647Z","iopub.execute_input":"2021-08-26T05:56:50.432128Z","iopub.status.idle":"2021-08-26T05:56:50.457458Z","shell.execute_reply.started":"2021-08-26T05:56:50.432061Z","shell.execute_reply":"2021-08-26T05:56:50.456436Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-26T05:56:50.458933Z","iopub.execute_input":"2021-08-26T05:56:50.459644Z","iopub.status.idle":"2021-08-26T05:56:50.474726Z","shell.execute_reply.started":"2021-08-26T05:56:50.459581Z","shell.execute_reply":"2021-08-26T05:56:50.473306Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"  row_id    target\n0    0-4  0.000374\n1   0-32  0.000357\n2   0-34  0.000357","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0-4</td>\n      <td>0.000374</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0-32</td>\n      <td>0.000357</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0-34</td>\n      <td>0.000357</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}