{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from optiver_features_handler import get_features_map_for_stock, get_row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\n",
    "TRADE_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_train.parquet\")\n",
    "TRADE_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_test.parquet\")\n",
    "BOOK_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_train.parquet\")\n",
    "BOOK_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_test.parquet\")\n",
    "OUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\n",
    "os.makedirs(OUTPUT_DIRECTORY,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIRECTORY,\"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIRECTORY,\"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interval_seconds = 24\n",
    "data_intervals_count = int(600/data_interval_seconds)\n",
    "class OptiverRealizedVolatilityDataset(Dataset):\n",
    "    def __init__(self, data_directory, mode=\"train\", lazy_load=True):\n",
    "        \"\"\"initializes Optiver Competition dataset\n",
    "        `mode`: train|test\n",
    "        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n",
    "        \"\"\"\n",
    "        print(\"INIT: OptiverRealizedVolatilityDataset\")\n",
    "        if mode.lower() not in ['train','test']:\n",
    "            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n",
    "        self.data_directory = data_directory\n",
    "        self.mode = mode.lower()\n",
    "        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n",
    "#         if self.mode == 'train':\n",
    "#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n",
    "        if self.mode == 'test':\n",
    "            self.main_df['target'] = 0\n",
    "        \n",
    "        self.cache_stocks_done_set = set()\n",
    "        # this is our final features lookup where we park all our features which can be addressed by row_id\n",
    "        # which is individual train/test.csv row id using 'stock_id`-`time_id`\n",
    "        self.cache_rowid_feature_map = {}\n",
    "        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n",
    "        targets = self.main_df['target'].tolist()\n",
    "        self.stock_possible_timeids_list = {}\n",
    "        for idx, row_id in enumerate(row_id_series.tolist()):\n",
    "            stock_id = int(row_id.split('-')[0])\n",
    "            time_id = int(row_id.split('-')[1])\n",
    "            self.cache_rowid_feature_map[row_id] = {'target_realized_volatility':targets[idx], 'stock_id':stock_id,'time_id':time_id,'row_id':row_id}\n",
    "            \n",
    "            # below code is to make sure what timeids we expect from stock data extractor\n",
    "            # in case of missing parquet files we'll have to know the keys to fill default values into\n",
    "            if stock_id not in self.stock_possible_timeids_list:\n",
    "                self.stock_possible_timeids_list[stock_id] = []\n",
    "            self.stock_possible_timeids_list[stock_id].append(time_id)\n",
    "            \n",
    "        \n",
    "        if lazy_load == False:\n",
    "            worker_data = []\n",
    "            for gkey, gdf in self.main_df.groupby(['stock_id']):\n",
    "                worker_data.append((self.data_directory, self.mode, gkey))\n",
    "#             print(\"---------- CPU COUNG:\", multiprocessing.cpu_count())\n",
    "            # NOTE: this was hell of a hunt; this windows and pytorch and jupyter combination is too tedious\n",
    "            #       make sure the function that we distribute don't call pytorch\n",
    "            chunksize = multiprocessing.cpu_count() * 1\n",
    "            processed = 0\n",
    "            for worker_data_chunk in [worker_data[i * chunksize:(i + 1) * chunksize] for i in range((len(worker_data) + chunksize - 1) // chunksize )]:\n",
    "                with Pool(multiprocessing.cpu_count()) as p:\n",
    "                    \n",
    "                    feature_set_list = p.starmap(get_features_map_for_stock, worker_data_chunk)\n",
    "                    \n",
    "                    for feature_map in feature_set_list:\n",
    "                        for rowid, features_dict in feature_map.items():\n",
    "                            for fkey,fval in features_dict.items():\n",
    "                                self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                            self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                        # udpate the indications that we've already fetched this stock and the lazy loader code won't fetch this again\n",
    "                        self.cache_stocks_done_set.add(int(rowid.split('-')[0]))\n",
    "                    \n",
    "                    processed += chunksize\n",
    "                    print(f\"Processed and loaded {processed} stocks features.\")\n",
    "    \n",
    "    def __cache_generate_features(self, main_stock_id, main_time_id):\n",
    "            \n",
    "            main_row_id = get_row_id(main_stock_id, main_time_id)\n",
    "            if main_stock_id not in self.cache_stocks_done_set:\n",
    "#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))   \n",
    "                # we'll combine the featureset with the bigger feature set of all stocks\n",
    "                feature_map = get_features_map_for_stock(self.data_directory, self.mode, main_stock_id)\n",
    "                # NOTE: sometime we might now have parquet files in that case we'll have 3 entried in .csv while only 1 gets returned in feature map\n",
    "                # we need to cover for that disparity\n",
    "                for time_id in self.stock_possible_timeids_list[main_stock_id]:\n",
    "                    expected_row_id = get_row_id(main_stock_id, time_id)\n",
    "                    if expected_row_id not in feature_map:\n",
    "                        feature_map[expected_row_id] = {}\n",
    "                for rowid, features_dict in feature_map.items():\n",
    "                    for fkey,fval in features_dict.items():\n",
    "                        self.cache_rowid_feature_map[rowid][fkey] = fval\n",
    "                    self.cache_rowid_feature_map[rowid]  = OptiverRealizedVolatilityDataset.transform_to_01_realized_volatility_linear_data(self.cache_rowid_feature_map[rowid])\n",
    "                self.cache_stocks_done_set.add(main_stock_id)\n",
    "#             print(self.cache_rowid_feature_map[main_row_id])\n",
    "#             print(torch.tensor([self.cache_rowid_feature_map[main_row_id].get('book_realized_volatility',0)]))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map[main_row_id].get('log_return1_2s', [0]*(int(600/2)))))\n",
    "#             print(torch.tensor(self.cache_rowid_feature_map.get('book_directional_volume1_2s', [0]*(int(600/2)))))\n",
    "            return self.cache_rowid_feature_map[main_row_id]\n",
    "        \n",
    "    @staticmethod\n",
    "    def transform_to_01_realized_volatility_linear_data(features_dict):\n",
    "        return (\n",
    "                {\n",
    "                    'row_id':features_dict['row_id'],\n",
    "                    'stock_id':torch.tensor(features_dict['stock_id'], dtype=torch.float32),\n",
    "                    'seconds_in_bucket_xs': torch.tensor(np.nan_to_num(features_dict.get('seconds_in_bucket_xs',  [(idx*data_interval_seconds)+data_interval_seconds for idx in range(0,int(data_intervals_count))])), dtype=torch.float32),\n",
    "                    'book_realized_volatility':torch.tensor([features_dict.get('book_realized_volatility',0)], dtype=torch.float32),\n",
    "                    # TRADE FEATURES\n",
    "                    'trade_logrett_sum_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_logrett_sum_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_logrett_realized_volatility_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_logrett_realized_volatility_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_logrett_std_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_logrett_std_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_logrett_mean_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_logrett_mean_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_size_sum_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_size_sum_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_size_std_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_size_std_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_order_count_sum_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_order_count_sum_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_order_count_std_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_order_count_std_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_trade_money_turnover_sum_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_trade_money_turnover_sum_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'trade_trade_money_turnover_std_xs': torch.tensor(np.nan_to_num(features_dict.get('trade_trade_money_turnover_std_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    \n",
    "                    \n",
    "                    'book_logret1_sum_xs': torch.tensor(np.nan_to_num(features_dict.get('book_logret1_sum_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_logret1_realized_volatility_xs': torch.tensor(np.nan_to_num(features_dict.get('book_logret1_realized_volatility_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_logret1_std_xs': torch.tensor(np.nan_to_num(features_dict.get('book_logret1_std_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_logret1_mean_xs': torch.tensor(np.nan_to_num(features_dict.get('book_logret1_mean_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_logret2_sum_xs': torch.tensor(np.nan_to_num(features_dict.get('book_logret2_sum_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_logret2_realized_volatility_xs': torch.tensor(np.nan_to_num(features_dict.get('book_logret2_realized_volatility_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_logret2_std_xs': torch.tensor(np.nan_to_num(features_dict.get('book_logret2_std_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_logret2_mean_xs': torch.tensor(np.nan_to_num(features_dict.get('book_logret2_mean_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_price_spread1_sum_xs': torch.tensor(np.nan_to_num(features_dict.get('book_price_spread1_sum_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_price_spread1_std_xs': torch.tensor(np.nan_to_num(features_dict.get('book_price_spread1_std_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_bid_spread_sum_xs': torch.tensor(np.nan_to_num(features_dict.get('book_bid_spread_sum_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_bid_spread_std_xs': torch.tensor(np.nan_to_num(features_dict.get('book_bid_spread_std_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_ask_spread_sum_xs': torch.tensor(np.nan_to_num(features_dict.get('book_ask_spread_sum_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_ask_spread_std_xs': torch.tensor(np.nan_to_num(features_dict.get('book_ask_spread_std_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_total_volume_sum_xs': torch.tensor(np.nan_to_num(features_dict.get('book_total_volume_sum_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_total_volume_std_xs': torch.tensor(np.nan_to_num(features_dict.get('book_total_volume_std_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_volume_imbalance_sum_xs': torch.tensor(np.nan_to_num(features_dict.get('book_volume_imbalance_sum_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "                    'book_volume_imbalance_std_xs': torch.tensor(np.nan_to_num(features_dict.get('book_volume_imbalance_std_xs', [0]*(int(600/data_interval_seconds)))), dtype=torch.float32),\n",
    "#                   \n",
    "                },\n",
    "                {'target_realized_volatility':torch.tensor([features_dict['target_realized_volatility']])}\n",
    "#                 [features_dict['target']]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: handle for num_workers more than 0\n",
    "        #      using https://pytorch.org/docs/stable/data.html\n",
    "        #      using torch.util.data.get_worker_info()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        stock_id = self.main_df.at[idx, 'stock_id']\n",
    "        time_id = self.main_df.at[idx, 'time_id']\n",
    "        x,y = self.__cache_generate_features(stock_id,time_id)\n",
    "#         x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT: OptiverRealizedVolatilityDataset\n"
     ]
    }
   ],
   "source": [
    "# dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\")\n",
    "dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"test\", lazy_load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'row_id': '0-4',\n",
       "  'stock_id': tensor(0.),\n",
       "  'seconds_in_bucket_xs': tensor([ 24.,  48.,  72.,  96., 120., 144., 168., 192., 216., 240., 264., 288.,\n",
       "          312., 336., 360., 384., 408., 432., 456., 480., 504., 528., 552., 576.,\n",
       "          600.]),\n",
       "  'book_realized_volatility': tensor([0.0003]),\n",
       "  'trade_logrett_sum_xs': tensor([ 0.0000, -0.0003,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000]),\n",
       "  'trade_logrett_realized_volatility_xs': tensor([0.0000, 0.0003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'trade_logrett_std_xs': tensor([0.0000, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'trade_logrett_mean_xs': tensor([ 0.0000, -0.0001,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000]),\n",
       "  'trade_size_sum_xs': tensor([  1., 200.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.]),\n",
       "  'trade_size_std_xs': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.]),\n",
       "  'trade_order_count_sum_xs': tensor([ 1., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  'trade_order_count_std_xs': tensor([0.0000, 2.8284, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'trade_trade_money_turnover_sum_xs': tensor([  1.0003, 200.0108,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000]),\n",
       "  'trade_trade_money_turnover_std_xs': tensor([0.0000, 0.0007, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'book_logret1_sum_xs': tensor([0.0003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'book_logret1_realized_volatility_xs': tensor([0.0003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'book_logret1_std_xs': tensor([0.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'book_logret1_mean_xs': tensor([9.8066e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]),\n",
       "  'book_logret2_sum_xs': tensor([0.0003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'book_logret2_realized_volatility_xs': tensor([0.0003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'book_logret2_std_xs': tensor([0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'book_logret2_mean_xs': tensor([9.2343e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]),\n",
       "  'book_price_spread1_sum_xs': tensor([0.0017, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'book_price_spread1_std_xs': tensor([2.8407e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]),\n",
       "  'book_bid_spread_sum_xs': tensor([0.0012, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'book_bid_spread_std_xs': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.]),\n",
       "  'book_ask_spread_sum_xs': tensor([0.0003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'book_ask_spread_std_xs': tensor([0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       "  'book_total_volume_sum_xs': tensor([1052.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "             0.,    0.,    0.,    0.,    0.]),\n",
       "  'book_total_volume_std_xs': tensor([65.2712,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000]),\n",
       "  'book_volume_imbalance_sum_xs': tensor([494.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.]),\n",
       "  'book_volume_imbalance_std_xs': tensor([165.7116,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000])},\n",
       " {'target_realized_volatility': tensor([0])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = None\n",
    "\n",
    "\n",
    "def loss_fn_mse(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)))\n",
    "\n",
    "def loss_fn_mspe(y, pred):\n",
    "    return torch.mean(torch.square((y-pred)/y))\n",
    "\n",
    "def loss_fn_orig(y, pred):\n",
    "    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "realize_volatility_scale_factor = 1000\n",
    "def scale_optiver_feature(feature_name, feature_tensor):\n",
    "    standard_scaling_feature_map ={'seconds_in_bucket_xs': {'mean': 312.0, 'std': 173.06646728515625},\n",
    "             'book_realized_volatility': {'mean': 0.005850940477102995,\n",
    "              'std': 0.004778958857059479},\n",
    "             'trade_logrett_sum_xs': {'mean': -1.1226925167306945e-08,\n",
    "              'std': 0.0012633935548365116},\n",
    "             'trade_logrett_realized_volatility_xs': {'mean': 0.0005547573091462255,\n",
    "              'std': 0.0011005407432094216},\n",
    "             'trade_logrett_std_xs': {'mean': 0.0002125719329342246,\n",
    "              'std': 0.000520365487318486},\n",
    "             'trade_logrett_mean_xs': {'mean': 2.2360411549016135e-06,\n",
    "              'std': 0.0006506441859528422},\n",
    "             'trade_size_sum_xs': {'mean': 1274.408447265625, 'std': 3957.27392578125},\n",
    "             'trade_size_std_xs': {'mean': 201.2566680908203, 'std': 809.9418334960938},\n",
    "             'trade_order_count_sum_xs': {'mean': 14.93747329711914,\n",
    "              'std': 31.093172073364258},\n",
    "             'trade_order_count_std_xs': {'mean': 1.9885706901550293,\n",
    "              'std': 3.986957311630249},\n",
    "             'trade_trade_money_turnover_sum_xs': {'mean': 1274.364990234375,\n",
    "              'std': 3957.194580078125},\n",
    "             'trade_trade_money_turnover_std_xs': {'mean': 201.25364685058594,\n",
    "              'std': 809.8814697265625},\n",
    "             'book_logret1_sum_xs': {'mean': -9.877491713439213e-09,\n",
    "              'std': 0.0013098383788019419},\n",
    "             'book_logret1_realized_volatility_xs': {'mean': 0.0008679572492837906,\n",
    "              'std': 0.001236740150488913},\n",
    "             'book_logret1_std_xs': {'mean': 0.00023386265092995018,\n",
    "              'std': 0.0003532674163579941},\n",
    "             'book_logret1_mean_xs': {'mean': 9.141682255631167e-08,\n",
    "              'std': 0.00013150273298379034},\n",
    "             'book_logret2_sum_xs': {'mean': -9.768747588623228e-09,\n",
    "              'std': 0.0013809562660753727},\n",
    "             'book_logret2_realized_volatility_xs': {'mean': 0.0011465881252661347,\n",
    "              'std': 0.00145239126868546},\n",
    "             'book_logret2_std_xs': {'mean': 0.00031500737532041967,\n",
    "              'std': 0.00042466234299354255},\n",
    "             'book_logret2_mean_xs': {'mean': 9.769648556812172e-08,\n",
    "              'std': 0.0001457101752748713},\n",
    "             'book_price_spread1_sum_xs': {'mean': 0.008905505761504173,\n",
    "              'std': 0.00932735949754715},\n",
    "             'book_price_spread1_std_xs': {'mean': 0.00013038843462709337,\n",
    "              'std': 0.00018358806846663356},\n",
    "             'book_bid_spread_sum_xs': {'mean': 0.0030224656220525503,\n",
    "              'std': 0.0032615496311336756},\n",
    "             'book_bid_spread_std_xs': {'mean': 7.655585795873776e-05,\n",
    "              'std': 0.0001423279318260029},\n",
    "             'book_ask_spread_sum_xs': {'mean': 0.0030542113818228245,\n",
    "              'std': 0.003295465372502804},\n",
    "             'book_ask_spread_std_xs': {'mean': 7.811487739672884e-05,\n",
    "              'std': 0.00014516572991851717},\n",
    "             'book_total_volume_sum_xs': {'mean': 65197.703125, 'std': 372891.21875},\n",
    "             'book_total_volume_std_xs': {'mean': 394.8986511230469,\n",
    "              'std': 1568.0399169921875},\n",
    "             'book_volume_imbalance_sum_xs': {'mean': 15863.017578125,\n",
    "              'std': 113432.6953125},\n",
    "             'book_volume_imbalance_std_xs': {'mean': 385.7958984375,\n",
    "              'std': 1644.90966796875}}\n",
    "    \n",
    "#     if feature_name in ['book_realized_volatility_xs','trade_realized_volatility_xs']:\n",
    "#         # we expect feature_tensor to be log returns tensor\n",
    "#         feature_tensor = feature_tensor ** 2\n",
    "# #         print(feature_tensor)\n",
    "#         feature_tensor = torch.cumsum(feature_tensor,1)\n",
    "#         # scale it to make each step realize volatility extrapolatable to 10 min window\n",
    "# #         feature_tensor = feature_tensor * torch.tensor([data_intervals_count/idx for idx in range(1,data_intervals_count+1,1)])\n",
    "#         feature_tensor = torch.sqrt(feature_tensor) * realize_volatility_scale_factor\n",
    "        \n",
    "        \n",
    "    if feature_name in standard_scaling_feature_map:\n",
    "        return (feature_tensor - standard_scaling_feature_map[feature_name]['mean'])/standard_scaling_feature_map[feature_name]['std']\n",
    "    if feature_name in ['trade_price_local_standardized_xs','book_wap1_local_standardized_xs']:\n",
    "        #TODO: the kaggle version of pytorch dont have nan_to_num, do something here!\n",
    "        feature_tensor = torch.masked_fill(feature_tensor, torch.isinf(feature_tensor),0)\n",
    "#         feature_tensor = torch.nan_to_num(feature_tensor,nan=0, posinf=0, neginf=0)\n",
    "#     print(feature_tensor)\n",
    "#     print(torch.any(torch.isnan(feature_tensor)))\n",
    "#     input()\n",
    "    return feature_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "class StockIdEmbedding(nn.Module):\n",
    "    def __init__(self,number_of_stock_embeddings=126+10, number_of_stock_embedding_dimention=2, mode='train'):\n",
    "        super(StockIdEmbedding, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.number_of_stock_embeddings = number_of_stock_embeddings\n",
    "        self.number_of_stock_embedding_dimention = number_of_stock_embedding_dimention\n",
    "        self.stock_embedding = nn.Embedding(self.number_of_stock_embeddings, self.number_of_stock_embedding_dimention)\n",
    "        self.mode = mode\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(self.number_of_stock_embedding_dimention, 32),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return []\n",
    "    \n",
    "    def set_mode(self,mode):\n",
    "        self.mode = mode\n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "        \n",
    "        stock_id_clamped = torch.clamp(feature_dict['stock_id'],0,self.number_of_stock_embeddings-1)\n",
    "        stock_id_clamped = stock_id_clamped.type(torch.cuda.IntTensor)\n",
    "        stock_id_clamped = stock_id_clamped.to(device).reshape(-1,1)\n",
    "        embedding_logits = self.stock_embedding(stock_id_clamped)\n",
    "        embedding_logits = embedding_logits.reshape(-1,self.number_of_stock_embedding_dimention)\n",
    "        \n",
    "        if self.mode == 'stock_id_embedding':\n",
    "            return embedding_logits\n",
    "\n",
    "            \n",
    "        logits = self.linear_stack(embedding_logits)\n",
    "        return logits\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, feature_generator_mode_hidden_size=64, mode='train'):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mode = mode\n",
    "        self.feature_generator_mode_hidden_size = feature_generator_mode_hidden_size\n",
    "        self.stock_id_embedding = StockIdEmbedding(number_of_stock_embedding_dimention=2, mode='stock_id_embedding')\n",
    "        self.cnn_stack = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(28, 14, kernel_size=10, stride=5, padding=0),\n",
    "            nn.Hardswish(),\n",
    "#             nn.BatchNorm1d(16),\n",
    "            \n",
    "            nn.Conv1d(14, 7, kernel_size=2, stride=1, padding=0),\n",
    "            nn.Hardswish(),\n",
    "#             nn.Dropout(0.07),\n",
    "#             nn.Conv1d(16, 24, kernel_size=4, stride=1, padding=0),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Dropout(0.1),\n",
    "\n",
    "        )\n",
    "        self.linear_stack = nn.Sequential(\n",
    "#             nn.LazyLinear(256),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Dropout(0.05),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Dropout(0.05),\n",
    "            nn.LazyLinear(self.feature_generator_mode_hidden_size),\n",
    "#             nn.Hardswish(),\n",
    "\n",
    "        )\n",
    "        self.linear_hybrid = nn.Sequential(\n",
    "            nn.Linear(self.feature_generator_mode_hidden_size, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 64),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return []\n",
    "    \n",
    "    def set_mode(self,mode):\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train_stock_id_embedding':\n",
    "            self.stock_id_embedding.set_mode('train')\n",
    "        else:\n",
    "            self.stock_id_embedding.set_mode('stock_id_embedding')\n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "#         logits = self.basic_stack(x)\n",
    "#         x = self.flatten(x)\n",
    "        x = torch.cat([\n",
    "            \n",
    "            scale_optiver_feature('trade_logrett_sum_xs',feature_dict['trade_logrett_sum_xs']).to(device),\n",
    "            scale_optiver_feature('trade_logrett_realized_volatility_xs',feature_dict['trade_logrett_realized_volatility_xs']).to(device),\n",
    "            scale_optiver_feature('trade_logrett_std_xs',feature_dict['trade_logrett_std_xs']).to(device),\n",
    "            scale_optiver_feature('trade_logrett_mean_xs',feature_dict['trade_logrett_mean_xs']).to(device),\n",
    "            \n",
    "            scale_optiver_feature('book_logret1_sum_xs',feature_dict['book_logret1_sum_xs']).to(device),\n",
    "            scale_optiver_feature('book_logret1_realized_volatility_xs',feature_dict['book_logret1_realized_volatility_xs']).to(device),\n",
    "            scale_optiver_feature('book_logret1_std_xs',feature_dict['book_logret1_std_xs']).to(device),\n",
    "            scale_optiver_feature('book_logret1_mean_xs',feature_dict['book_logret1_mean_xs']).to(device),\n",
    "            \n",
    "            scale_optiver_feature('book_logret2_sum_xs',feature_dict['book_logret2_sum_xs']).to(device),\n",
    "            scale_optiver_feature('book_logret2_realized_volatility_xs',feature_dict['book_logret2_realized_volatility_xs']).to(device),\n",
    "            scale_optiver_feature('book_logret2_std_xs',feature_dict['book_logret2_std_xs']).to(device),\n",
    "            scale_optiver_feature('book_logret2_mean_xs',feature_dict['book_logret2_mean_xs']).to(device),\n",
    "            \n",
    "            scale_optiver_feature('trade_size_sum_xs',feature_dict['trade_size_sum_xs']).to(device),\n",
    "            scale_optiver_feature('trade_size_std_xs',feature_dict['trade_size_std_xs']).to(device),\n",
    "            scale_optiver_feature('trade_order_count_sum_xs',feature_dict['trade_order_count_sum_xs']).to(device),\n",
    "            scale_optiver_feature('trade_order_count_std_xs',feature_dict['trade_order_count_std_xs']).to(device),\n",
    "            scale_optiver_feature('trade_trade_money_turnover_sum_xs',feature_dict['trade_trade_money_turnover_sum_xs']).to(device),\n",
    "            scale_optiver_feature('trade_trade_money_turnover_std_xs',feature_dict['trade_trade_money_turnover_std_xs']).to(device),\n",
    "            scale_optiver_feature('book_price_spread1_sum_xs',feature_dict['book_price_spread1_sum_xs']).to(device),\n",
    "            scale_optiver_feature('book_price_spread1_std_xs',feature_dict['book_price_spread1_std_xs']).to(device),\n",
    "            scale_optiver_feature('book_bid_spread_sum_xs',feature_dict['book_bid_spread_sum_xs']).to(device),\n",
    "            scale_optiver_feature('book_bid_spread_std_xs',feature_dict['book_bid_spread_std_xs']).to(device),\n",
    "            scale_optiver_feature('book_ask_spread_sum_xs',feature_dict['book_ask_spread_sum_xs']).to(device),\n",
    "            scale_optiver_feature('book_ask_spread_std_xs',feature_dict['book_ask_spread_std_xs']).to(device),\n",
    "            scale_optiver_feature('book_total_volume_sum_xs',feature_dict['book_total_volume_sum_xs']).to(device),\n",
    "            scale_optiver_feature('book_total_volume_std_xs',feature_dict['book_total_volume_std_xs']).to(device),\n",
    "            scale_optiver_feature('book_volume_imbalance_sum_xs',feature_dict['book_volume_imbalance_sum_xs']).to(device),\n",
    "            scale_optiver_feature('book_volume_imbalance_std_xs',feature_dict['book_volume_imbalance_std_xs']).to(device),\n",
    "                            \n",
    "#                             scale_optiver_feature('book_logret1_sum_xs',feature_dict['book_logret1_sum_xs']).to(device),\n",
    "#                             scale_optiver_feature('book_logret1_realized_volatility_xs',feature_dict['book_logret1_realized_volatility_xs']).to(device),\n",
    "                            \n",
    "            \n",
    "#                             scale_optiver_feature('book_logret1_std_xs',feature_dict['book_logret1_std_xs']).to(device),\n",
    "#                             scale_optiver_feature('book_logret1_mean_xs',feature_dict['book_logret1_mean_xs']).to(device),\n",
    "                            \n",
    "               \n",
    "            \n",
    "#                             scale_optiver_feature('logrett_xs',feature_dict['logrett_xs']).to(device),\n",
    "\n",
    "#                                 feature_dict['logret1_xs'].to(device)*10000,\n",
    "                            \n",
    "#                                     feature_dict['book_price_spread1_xs'].to(device)*1000, \n",
    "#                                 feature_dict['book_bid_spread_xs'].to(device)*10000, \n",
    "#                                 feature_dict['book_ask_spread_xs'].to(device)*10000, \n",
    "\n",
    "#                                 torch.log(feature_dict['book_total_volume_xs'].to(device)+1),\n",
    "#                                 torch.log(feature_dict['book_volume_imbalance_xs'].to(device)+1),\n",
    "\n",
    "#                                 torch.log(feature_dict['trade_money_turnover_per_order_xs'].to(device)+1),\n",
    "                          ], 1)\n",
    "\n",
    "#         x = torch.nan_to_num(feature_dict['logrett_xs']).type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        \n",
    "#         print(x)\n",
    "#         input()\n",
    "#         if torch.isnan(x).any():\n",
    "# #             print(x)\n",
    "#             print(feature_dict)\n",
    "#             input()\n",
    "        x = x.to(device)\n",
    "        x = x.reshape(-1,28,data_intervals_count)\n",
    "        \n",
    "        logits = self.cnn_stack(x)\n",
    "        logits = self.flatten(logits)\n",
    "        \n",
    "        #         if self.use_stock_id:\n",
    "#         embedding_logits = self.stock_id_embedding(feature_dict)\n",
    "        \n",
    "#         if self.mode == 'train_stock_id_embedding':\n",
    "            # in that case embedding logits are predicted volatility\n",
    "#             return embedding_logits\n",
    "        \n",
    "#         print('cat',logits.size(), embedding_logits.size())\n",
    "#         logits = torch.cat([logits,embedding_logits], 1)\n",
    "#         logits = embedding_logits\n",
    "        \n",
    "        \n",
    "        logits = self.linear_stack(logits)\n",
    "        \n",
    "        if self.mode == 'hidden_generator':\n",
    "            return logits\n",
    "#         logits = torch.cat( [logits, \n",
    "#                              torch.log(feature_dict['trade_money_turnover_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001), \n",
    "#                                            torch.log(feature_dict['trade_money_turnover_std'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                            torch.log(feature_dict['trade_price_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                            torch.log(feature_dict['book_money_turnover_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                            torch.log(feature_dict['book_money_turnover_std'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001),\n",
    "#                                            torch.log(feature_dict['book_price_mean'].type(torch.cuda.FloatTensor).to(device).reshape(-1,1)+0.001)\n",
    "#                                       ], 1)\n",
    "        \n",
    "#         if self.use_stock_id:\n",
    "#             stock_id = torch.tensor(feature_dict['stock_id']).reshape(-1,1)\n",
    "#             stock_id = stock_id.to(device)\n",
    "#             logits = torch.cat([logits, stock_id], 1)\n",
    "            \n",
    "        logits = self.linear_hybrid(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VolatilityGRU(nn.Module):\n",
    "#     def __init__(self, input_size=1, hidden_size=64, repeated_cells=1):\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "class MultiFetGRU(nn.Module):\n",
    "    def __init__(self,feature_names, hidden_size=64, layers=1, dropout=0, features_out=32, mode=\"train\"):\n",
    "        \"\"\"single feature, feature learner\n",
    "        `mode`: train|feature_generator\n",
    "        \"\"\"\n",
    "        super(MultiFetGRU,self).__init__()\n",
    "        if type(feature_names) == str:\n",
    "            feature_names = [feature_names]\n",
    "        self.feature_names = feature_names\n",
    "        self.input_size_ = len(self.feature_names)\n",
    "        \n",
    "        self.hidden_size_ = hidden_size\n",
    "        self.repeated_lstm_cells_ = layers\n",
    "        self.dropout_ = dropout\n",
    "        self.features_out = features_out\n",
    "        self.initial_dropout_ = nn.Dropout(0.1)\n",
    "        self.rnn_ = nn.GRU(self.input_size_, self.hidden_size_, self.repeated_lstm_cells_, batch_first=True, dropout=self.dropout_)\n",
    "        \n",
    "        self.linear_feature_stack_ = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size_, self.features_out),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(128, self.features_out),\n",
    "        )\n",
    "        \n",
    "        self.linear_trainer_stack_ = nn.Sequential(\n",
    "            nn.Linear(self.features_out, 128),\n",
    "#             nn.Linear(self.features_out, 1),\n",
    "            nn.Hardswish(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.Hardswish(),\n",
    "            nn.Linear(128, 1),   \n",
    "        )\n",
    "        \n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def forward(self, feature_dict, h0_tensor=None):\n",
    "        \n",
    "            \n",
    "        feature_x = []\n",
    "        for feature_name in self.feature_names:\n",
    "            if feature_name == 'book_realized_volatility_xs':\n",
    "                feature_tensor = feature_dict['logret1_xs']\n",
    "            elif feature_name == 'trade_realized_volatility_xs':\n",
    "                feature_tensor = feature_dict['logrett_xs']\n",
    "            else:\n",
    "                feature_tensor = feature_dict[feature_name]\n",
    "                \n",
    "            feature_x.append(scale_optiver_feature(feature_name, feature_tensor).to(device))\n",
    "            \n",
    "        feature_x = torch.stack(feature_x,dim=2) #.reshape(-1, data_intervals_count, self.input_size_)\n",
    "        feature_x = self.initial_dropout_(feature_x)\n",
    "        if self.mode in [\"feature_generator\",\"train\"]:\n",
    "#             if h0_tensor is None:\n",
    "# #                 h_0_ = torch.rand(self.repeated_lstm_cells_, feature_x.size(0), self.hidden_size_, device=device) #hidden state\n",
    "#                 h_0_ = torch.zeros(self.repeated_lstm_cells_, feature_x.size(0), self.hidden_size_, device=device) #hidden state\n",
    "#             else:\n",
    "#                 h_0_ = h0_tensor\n",
    "            output_, hn_ = self.rnn_(feature_x)#, h_0_) #lstm with input, hidden, and internal state\n",
    "#             print('output',output_.size(), 'hn', hn_.size())\n",
    "#             print(output_[:,-1].size())\n",
    "#             input()\n",
    "#             hn_ = hn_.reshape(-1, self.hidden_size_*self.repeated_lstm_cells_) #reshaping the data for Dense layer next  \n",
    "#             output_ = output_.reshape(-1, self.hidden_size_*self.repeated_lstm_cells_) #reshaping the data for Dense layer next  \n",
    "            \n",
    "            out_ = self.linear_feature_stack_(output_[:,-1])\n",
    "            \n",
    "            if self.mode == \"train\":\n",
    "                out_ = self.linear_trainer_stack_(out_)\n",
    "            \n",
    "            return out_\n",
    "            \n",
    "            \n",
    "            \n",
    "class VolatilityBSModel(nn.Module):\n",
    "    def __init__(self, mode=\"hybrid\"):\n",
    "        \"\"\"various rnn features' fusion with fully connected nn\n",
    "        `mode`: hybrid|<feature_name>\n",
    "        \"\"\"\n",
    "        super(VolatilityBSModel, self).__init__()\n",
    "        self.mode = mode\n",
    "#         self.feature_list = ['logrett_xs','trade_volume_xs','trade_ordercount_xs','trade_money_turnover_xs','trade_money_turnover_per_order_xs',\n",
    "#                              'logret1_xs',\n",
    "#                              'book_price_spread1_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "#                              'book_total_volume_xs','book_volume_imbalance_xs','book_money_turnover_intention1_xs','book_wap1_local_standardized_xs','trade_price_local_standardized_xs']\n",
    "#         self.feature_list = [[x] for x in self.feature_list]\n",
    "        # 'trade_volume_xs','trade_ordercount_xs','trade_money_turnover_xs','trade_money_turnover_per_order_xs',\n",
    "#                             #realized_volatility_xs  \n",
    "        self.feature_list = [\n",
    "            \n",
    "            ['book_logret1_sum_xs','book_logret1_realized_volatility_xs','book_logret1_std_xs','book_logret1_mean_xs',\n",
    "            'book_logret2_sum_xs','book_logret2_realized_volatility_xs','book_logret2_std_xs','book_logret2_mean_xs',\n",
    "            'trade_size_sum_xs','trade_size_std_xs','trade_order_count_sum_xs','trade_order_count_std_xs','trade_order_count_std_xs','trade_trade_money_turnover_sum_xs','trade_trade_money_turnover_std_xs',\n",
    "            'trade_logrett_sum_xs','trade_logrett_realized_volatility_xs','trade_logrett_std_xs','trade_logrett_mean_xs',\n",
    "            'book_price_spread1_sum_xs','book_price_spread1_std_xs','book_bid_spread_sum_xs','book_bid_spread_std_xs','book_ask_spread_sum_xs',\n",
    "                    'book_ask_spread_std_xs','book_total_volume_sum_xs','book_total_volume_std_xs','book_volume_imbalance_sum_xs','book_volume_imbalance_std_xs']]\n",
    "        # ['trade_price_local_standardized_xs','trade_money_turnover_xs','book_money_turnover_intention1_xs','book_wap1_local_standardized_xs']\n",
    "#         self.feature_list = [['logrett_xs','logret1_xs'],['trade_volume_xs','trade_ordercount_xs','trade_money_turnover_per_order_xs','book_money_turnover_intention1_xs','trade_price_local_standardized_xs','book_wap1_local_standardized_xs'],['book_price_spread1_xs','book_bid_spread_xs','book_ask_spread_xs',\n",
    "#                              'book_total_volume_xs','book_volume_imbalance_xs']]\n",
    "         \n",
    "        self.feature_gen_feature_size = 128\n",
    "        self.feature_gen_models = {}\n",
    "        self.rnn_hidden_size = 128\n",
    "        self.rnn_layers = 4\n",
    "        self.stock_embedding_dimention = 6\n",
    "        self.stock_id_embedding = StockIdEmbedding(number_of_stock_embedding_dimention=self.stock_embedding_dimention, mode='stock_id_embedding')\n",
    "        self.hidden_generator_network = NeuralNetwork(feature_generator_mode_hidden_size=self.feature_gen_feature_size)\n",
    "        \n",
    "        for k in self.feature_list:\n",
    "            self.feature_gen_models[str(k)]=MultiFetGRU(k, hidden_size=self.rnn_hidden_size, layers=self.rnn_layers, dropout=0.0, features_out=self.feature_gen_feature_size) \n",
    "            self.feature_gen_models[str(k)].to(device)\n",
    "        \n",
    "        \n",
    "        self.linear_fusion = nn.Sequential(\n",
    "            #self.feature_gen_feature_size*len(self.feature_list) + self.rnn_hidden_size*self.rnn_layers + 2 + 1\n",
    "            nn.LazyLinear(256),\n",
    "            nn.Hardswish(),\n",
    "#             nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.1),\n",
    "#             nn.Linear(256,256),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Dropout(0.05),\n",
    "            nn.Linear(256,64),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Dropout(0.05),\n",
    "#             nn.Linear(128,64),\n",
    "#             nn.Hardswish(),\n",
    "#             nn.Dropout(0.05),\n",
    "#             nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "        self.set_mode(self.mode)\n",
    "    \n",
    "    def get_feature_gen_train_modes(self):\n",
    "        return self.feature_list\n",
    "    \n",
    "    def set_mode(self, mode):\n",
    "        print(f\"------- set mode : {mode} -----------\")\n",
    "        self.mode = mode\n",
    "        for feature_gen_model in self.feature_gen_models.values():\n",
    "            feature_gen_model.set_mode('feature_generator' if self.mode in ['hybrid','hybrid_feature_out','hidden_generator','ultimate'] else 'train')\n",
    "        if self.mode == 'hidden_generator':\n",
    "            self.hidden_generator_network.set_mode('train')\n",
    "        else:\n",
    "            self.hidden_generator_network.set_mode('hidden_generator')\n",
    "        self.stock_id_embedding.set_mode('stock_id_embedding')\n",
    "    \n",
    "    def parameters(self):\n",
    "        \n",
    "        generator_sources_map = {str(k):[v] for k,v in self.feature_gen_models.items()}\n",
    "        generator_sources_map['hybrid']= [self.linear_fusion, self.stock_id_embedding]\n",
    "        generator_sources_map['hidden_generator'] = [self.hidden_generator_network] \n",
    "        generator_sources_map['ultimate']= [self.linear_fusion, self.hidden_generator_network, self.stock_id_embedding] + list(self.feature_gen_models.values())\n",
    "        params = []\n",
    "        # mode and key is actually str version of array of strings with feature name as values e.g. ['logret1_xs','volume_xs']\n",
    "        if str(self.mode) in generator_sources_map:\n",
    "            for generator_source in generator_sources_map[str(self.mode)]:\n",
    "                for param in generator_source.parameters():\n",
    "                    params.append(param)\n",
    "        else:\n",
    "            return super(VolatilityBSModel,self).parameters()\n",
    "        return params\n",
    "    \n",
    "    \n",
    "    def forward(self, feature_dict):\n",
    "        \n",
    "        \n",
    "        if self.mode in self.feature_list:\n",
    "            \n",
    "            \n",
    "            # pass in some randomness to the initial hidden tensor to force it to learn some stuff on its own\n",
    "            # otherwise as the initial hidden layer contains solid infor to minimize the loss; it'll just use that hidden layer to minimize and instead\n",
    "            # learn to not learn and directly bypass initial hidden\n",
    "#             h0_tensor.masked_fill_((torch.rand(h0_tensor.size()) > 0.5).to(device), 0.0)\n",
    "#             h0_tensor = torch.zeros(self.rnn_layers, -1, self.rnn_hidden_size)\n",
    "            out = self.feature_gen_models[str(self.mode)](feature_dict, h0_tensor=None)\n",
    "            return out\n",
    "        \n",
    "        if self.mode in ['hidden_generator']:\n",
    "            out = self.hidden_generator_network(feature_dict)\n",
    "            return out\n",
    "        \n",
    "        if self.mode in ['hybrid','hybrid_feature_out','ultimate']:\n",
    "            generated_features = []\n",
    "            for feature_name, feature_gen_model in self.feature_gen_models.items():\n",
    "#                 h0_tensor = torch.zeros(self.rnn_layers, -1, self.rnn_hidden_size)\n",
    "                features_out = feature_gen_model(feature_dict, h0_tensor=None)\n",
    "                generated_features.append(features_out)\n",
    "                \n",
    "                \n",
    "            combined_features = torch.cat(generated_features, 1)#.reshape(-1, self.feature_gen_feature_size*len(self.feature_list))\n",
    "            \n",
    "            cnn_features = self.hidden_generator_network(feature_dict)\n",
    "#             cnn_features = cnn_features.reshape(self.rnn_layers,-1,self.rnn_hidden_size)\n",
    "            combined_features = torch.cat([combined_features,cnn_features],1)\n",
    "    \n",
    "            embedding_logits = self.stock_id_embedding(feature_dict)\n",
    "            combined_features = torch.cat([combined_features,embedding_logits],1)\n",
    "            \n",
    "            realized_volatility_logits = feature_dict['book_realized_volatility'].to(device).reshape(-1,1) * realize_volatility_scale_factor\n",
    "#             realized_volatility_logits = scale_optiver_feature('book_realized_volatility',feature_dict['book_realized_volatility']).to(device).reshape(-1,1)\n",
    "#             realized_volatility_logits = realized_volatility_logits # * realize_volatility_scale_factor\n",
    "            combined_features = torch.cat([combined_features,realized_volatility_logits],1)\n",
    "            \n",
    "            if self.mode == 'hybrid_feature_out':\n",
    "                return combined_features\n",
    "            \n",
    "            out = self.linear_fusion(combined_features)\n",
    "            return out\n",
    "        \n",
    "#         input(\"--- out got\")\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nisha\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- set mode : hybrid -----------\n",
      "------- set mode : ultimate -----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VolatilityBSModel:\n\tMissing key(s) in state_dict: \"hidden_generator_network.cnn_stack.3.weight\", \"hidden_generator_network.cnn_stack.3.bias\", \"linear_fusion.0.weight\", \"linear_fusion.0.bias\", \"linear_fusion.3.weight\", \"linear_fusion.3.bias\", \"linear_fusion.5.weight\", \"linear_fusion.5.bias\". \n\tUnexpected key(s) in state_dict: \"hidden_generator_network.cnn_stack.4.weight\", \"hidden_generator_network.cnn_stack.4.bias\", \"linear_fusion.6.weight\", \"linear_fusion.6.bias\", \"linear_fusion.1.weight\", \"linear_fusion.1.bias\", \"linear_fusion.4.weight\", \"linear_fusion.4.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7212/2669167392.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodelpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../output/models/17_gg_hybridEXP47_CNNRNN_5s_StkFalse_0.001_256_epoch_49_tloss_nan.pth\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'base'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_gen_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1407\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VolatilityBSModel:\n\tMissing key(s) in state_dict: \"hidden_generator_network.cnn_stack.3.weight\", \"hidden_generator_network.cnn_stack.3.bias\", \"linear_fusion.0.weight\", \"linear_fusion.0.bias\", \"linear_fusion.3.weight\", \"linear_fusion.3.bias\", \"linear_fusion.5.weight\", \"linear_fusion.5.bias\". \n\tUnexpected key(s) in state_dict: \"hidden_generator_network.cnn_stack.4.weight\", \"hidden_generator_network.cnn_stack.4.bias\", \"linear_fusion.6.weight\", \"linear_fusion.6.bias\", \"linear_fusion.1.weight\", \"linear_fusion.1.bias\", \"linear_fusion.4.weight\", \"linear_fusion.4.bias\". "
     ]
    }
   ],
   "source": [
    "model = VolatilityBSModel()\n",
    "model.set_mode('ultimate')\n",
    "modelpath = \"../input/optiver-realized-volatility-binarysentient-pytorch/07_1s_logret1n2_cnn_epoch_400_tloss_0.2393.pth\"\n",
    "modelpath = \"../output/models/17_gg_hybridEXP47_CNNRNN_5s_StkFalse_0.001_256_epoch_49_tloss_nan.pth\"\n",
    "checkpoint = torch.load(modelpath)\n",
    "model.load_state_dict(checkpoint['base'])\n",
    "for k,v in model.feature_gen_models.items():\n",
    "    v.load_state_dict(checkpoint[k])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=3,\n",
    "                        shuffle=False, num_workers=0, pin_memory=True)\n",
    "size = len(dataloader.dataset)\n",
    "submission_data = []\n",
    "\n",
    "output_scaling = realize_volatility_scale_factor\n",
    "data_ohlc_sample_len = 1 # 1 for each of open high low close\n",
    "for batch, (Feature_X, feature_y) in enumerate(dataloader):\n",
    "    row_ids = Feature_X['row_id']\n",
    "#     y = feature_y.to(device) * output_scaling \n",
    "    \n",
    "    pred = model(Feature_X) \n",
    "#     print(pred)\n",
    "    predicted_volatility = (pred/realize_volatility_scale_factor).tolist()\n",
    "    for idx, row_id in enumerate(row_ids):\n",
    "        submission_data.append({'row_id':row_id, 'target':predicted_volatility[idx][0]})\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df = dataset.main_df.merge(submission_df,on='row_id',how='left')\n",
    "submission_df = submission_df.rename(columns={'target_y':'target'})\n",
    "# submission_df\n",
    "# print(submission_df.columns)\n",
    "submission_df[['row_id','target']].to_csv(\"submission.csv\", index=False)\n",
    "# for idx, (X,y) in enumerate(dataset):\n",
    "#     print(idx, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
