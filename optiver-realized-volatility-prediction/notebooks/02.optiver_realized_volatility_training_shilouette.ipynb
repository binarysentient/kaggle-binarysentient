{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c4e895-fd66-490d-a8b8-d28b324d3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9dad9958-4061-4195-b9d8-77f142bc7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750eb0e3-2860-490b-bc3c-2a512485a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(\"..\",\"input\",\"optiver-realized-volatility-prediction\")\n",
    "TRADE_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_train.parquet\")\n",
    "TRADE_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"trade_test.parquet\")\n",
    "BOOK_TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_train.parquet\")\n",
    "BOOK_TEST_DIRECTORY = os.path.join(DATA_DIRECTORY,\"book_test.parquet\")\n",
    "OUTPUT_DIRECTORY = os.path.join(\"..\",\"output\")\n",
    "os.makedirs(OUTPUT_DIRECTORY,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ae2df8-a69e-4cfe-b678-7ffd14c2eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIRECTORY,\"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIRECTORY,\"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "da0a879f-e679-492a-b7a0-f7fe9880c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptiverRealizedVolatilityDataset(Dataset):\n",
    "    def __init__(self, data_directory, mode=\"train\"):\n",
    "        \"\"\"initializes Optiver Competition dataset\n",
    "        `mode`: train|test\n",
    "        `data_directory`: the datadirectory of the input data, where there are test.csv, train.csv, and parquet folders for trade_train.parquet and other relevant folders\n",
    "        \"\"\"\n",
    "        if mode.lower() not in ['train','test']:\n",
    "            raise Exception(\"Invalid mode passed for Optiver dataset. Valid values:train|test\")\n",
    "        self.data_directory = data_directory\n",
    "        self.mode = mode.lower()\n",
    "        self.main_df = pd.read_csv(os.path.join(self.data_directory,f'{self.mode}.csv'))\n",
    "#         if self.mode == 'train':\n",
    "#             self.main_df['row_id'] = self.main_df.apply(lambda x: f\"{x['stock_id']:.0f}-{x['time_id']:.0f}\", axis=1)\n",
    "        if self.mode == 'test':\n",
    "            self.main_df['target'] = 0\n",
    "        \n",
    "        self.cache_stocks_done_set = set()\n",
    "        self.cache_rowid_feature_map = {}\n",
    "        row_id_series = self.main_df['stock_id'].astype(str) + \"-\" +self.main_df['time_id'].astype(str)\n",
    "        targets = self.main_df['target'].tolist()\n",
    "        for idx, row_id in enumerate(row_id_series.tolist()):\n",
    "            self.cache_rowid_feature_map[row_id] = {'target':targets[idx], 'stock_id':row_id.split('-')[0],'time_id':row_id.split('-')[1],'row_id':row_id}\n",
    "    \n",
    "    \n",
    "    def __get_row_id(self, stock_id, time_id):\n",
    "        return f\"{stock_id:.0f}-{time_id:.0f}\"\n",
    "    \n",
    "    def __generate_realized_volatility_of_book_df(self, book_df):\n",
    "        \"\"\" expects the wap1 to be present and log_return to be present\"\"\"\n",
    "        return np.sqrt(np.sum(book_df['log_return']**2))\n",
    "    \n",
    "    def __generate_per2min_wap1_book_df0(self, book_df):\n",
    "        \"\"\"`seconds_in_bucket_120s_groupkey` needs to be present\"\"\"\n",
    "        \n",
    "        prices_120s = []\n",
    "        \n",
    "        for gkey, gdata in book_df.groupby('seconds_in_bucket_120s_groupkey'):\n",
    "            prices_120s.append(gdata['wap1'].mean())\n",
    "        \n",
    "#              \n",
    "        if len(prices_120s) == 0:\n",
    "            prices_120s += [0] * 5\n",
    "        if len(prices_120s) < 5:\n",
    "            prices_120s += ([prices_120s[0]] * (5-len(prices_120s)))\n",
    "        return prices_120s\n",
    "        \n",
    "    \n",
    "    def __cache_generate_features(self, main_stock_id, main_time_id):\n",
    "            \n",
    "            \n",
    "            main_row_id = self.__get_row_id(main_stock_id, main_time_id)\n",
    "            if main_stock_id not in self.cache_stocks_done_set:\n",
    "#                 trade_df = pd.read_parquet(os.path.join(self.data_directory, f\"trade_{self.mode}.parquet\", f\"stock_id={stock_id}\"))\n",
    "                \n",
    "                book_df = pd.read_parquet(os.path.join(self.data_directory, f\"book_{self.mode}.parquet\", f\"stock_id={main_stock_id}\"))\n",
    "                book_df['wap1'] = (book_df['bid_price1'] * book_df['ask_size1'] + book_df['ask_price1'] * book_df['bid_size1'])/(book_df['bid_size1'] + book_df['ask_size1'])\n",
    "    #             book_df['wap2'] = (book_df['bid_price2'] * book_df['ask_size2'] + book_df['ask_price2'] * book_df['bid_size2'])/(book_df['bid_size2'] + book_df['ask_size2'])\n",
    "                #NOTE: use wap1 ; until we figure out in 01. study which price wap1 closely resembles the trade price, or maybe wap1&wap2 mean\n",
    "                book_df['log_return'] = book_df.groupby('time_id')['wap1'].apply(lambda x: np.log(x).diff())\n",
    "                \n",
    "                book_df['seconds_in_bucket_120s_groupkey'] = (book_df['seconds_in_bucket']/120).astype(int)\n",
    "                \n",
    "#                 print(book_df)\n",
    "                # ACTUAL FEATURES HERE!\n",
    "                for groupkey, groupdf in book_df.groupby('time_id'):\n",
    "                    rowid = self.__get_row_id(main_stock_id, groupkey)\n",
    "                    self.cache_rowid_feature_map[rowid]['realized_volatility'] = self.__generate_realized_volatility_of_book_df(groupdf)\n",
    "                    \n",
    "                    self.cache_rowid_feature_map[rowid]['wap_120s_interval'] = self.__generate_per2min_wap1_book_df0(groupdf)\n",
    "                \n",
    "                    \n",
    "                self.cache_stocks_done_set.add(main_stock_id)\n",
    "                \n",
    "            return self.cache_rowid_feature_map[main_row_id]\n",
    "        \n",
    "    \n",
    "    def __transform_to_01_realized_volatility_linear_data(self, features_dict):\n",
    "        return {'row_id':features_dict['row_id'],\n",
    "                'x_realized_volatility':torch.tensor([features_dict.get('realized_volatility',0)]),\n",
    "                'x_wap_120s':torch.tensor(features_dict.get('wap_120s_interval',[0]*int(600/120)))\n",
    "               }, torch.tensor([features_dict['target']])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: handle for num_workers more than 0\n",
    "        #      using https://pytorch.org/docs/stable/data.html\n",
    "        #      using torch.util.data.get_worker_info()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        stock_id = self.main_df.at[idx, 'stock_id']\n",
    "        time_id = self.main_df.at[idx, 'time_id']\n",
    "        features_dict = self.__cache_generate_features(stock_id,time_id)\n",
    "        x, y = self.__transform_to_01_realized_volatility_linear_data(features_dict)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "efcee691-c9ce-481f-a4e8-dbf97b66b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OptiverRealizedVolatilityDataset(DATA_DIRECTORY, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "f1e82ed5-3d4d-45eb-abac-a8e468bac1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'row_id': '0-5',\n",
       "  'x_realized_volatility': tensor([0.0045], dtype=torch.float64),\n",
       "  'x_wap_120s': tensor([1.0030, 1.0041, 1.0039, 1.0040, 1.0034], dtype=torch.float64)},\n",
       " tensor([0.0041]))"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "202d58ff-26aa-4b02-b2ac-991ccb366c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for x in range(0,9):\n",
    "#     print(dataset[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "a01a7450-a6f4-49ac-861b-81912176aed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for key, val in dataset.cache_rowid_feature_map.items():\n",
    "#     dataset.main_df.at[0,'time_id']\n",
    "#     dataset.main_df.at[0,'stock_id']\n",
    "# for idx in range(0, len(dataset)):\n",
    "#     if dataset[idx] is None:\n",
    "#         print(\"______________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "af8004ac-aa73-40f0-aa93-bdf701fa3d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'row_id': '33-16526',\n",
       "  'x_realized_volatility': tensor([0.0058], dtype=torch.float64),\n",
       "  'x_wap_120s': tensor([1.0000, 0.9977, 0.9993, 1.0002, 1.0000], dtype=torch.float64)},\n",
       " tensor([0.0087]))"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data where only 4 values for x_wap_120 was there\n",
    "dataset[10000*11+6888]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "751ca2da-bcb6-470a-ba88-45fdbe7aa534",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3441, dtype=torch.float64)\n",
      "tensor(0.3484, dtype=torch.float64)\n",
      "tensor(0.3244, dtype=torch.float64)\n",
      "tensor(0.3237, dtype=torch.float64)\n",
      "tensor(0.3159, dtype=torch.float64)\n",
      "tensor(0.3361, dtype=torch.float64)\n",
      "tensor(0.3728, dtype=torch.float64)\n",
      "tensor(0.3382, dtype=torch.float64)\n",
      "tensor(0.3129, dtype=torch.float64)\n",
      "tensor(0.3626, dtype=torch.float64)\n",
      "tensor(0.5738, dtype=torch.float64)\n",
      "tensor(0.5318, dtype=torch.float64)\n",
      "tensor(0.2776, dtype=torch.float64)\n",
      "tensor(0.3827, dtype=torch.float64)\n",
      "tensor(0.3319, dtype=torch.float64)\n",
      "tensor(0.2900, dtype=torch.float64)\n",
      "tensor(0.3247, dtype=torch.float64)\n",
      "tensor(0.2969, dtype=torch.float64)\n",
      "tensor(0.3073, dtype=torch.float64)\n",
      "tensor(0.3155, dtype=torch.float64)\n",
      "tensor(0.3413, dtype=torch.float64)\n",
      "tensor(0.2933, dtype=torch.float64)\n",
      "tensor(0.3229, dtype=torch.float64)\n",
      "tensor(0.3178, dtype=torch.float64)\n",
      "tensor(0.3267, dtype=torch.float64)\n",
      "tensor(0.3283, dtype=torch.float64)\n",
      "tensor(0.2959, dtype=torch.float64)\n",
      "tensor(0.3582, dtype=torch.float64)\n",
      "tensor(0.3142, dtype=torch.float64)\n",
      "tensor(0.3279, dtype=torch.float64)\n",
      "tensor(0.3911, dtype=torch.float64)\n",
      "tensor(0.3140, dtype=torch.float64)\n",
      "tensor(0.2985, dtype=torch.float64)\n",
      "tensor(0.3395, dtype=torch.float64)\n",
      "tensor(0.3169, dtype=torch.float64)\n",
      "tensor(0.3614, dtype=torch.float64)\n",
      "tensor(0.3080, dtype=torch.float64)\n",
      "tensor(0.3483, dtype=torch.float64)\n",
      "tensor(0.3180, dtype=torch.float64)\n",
      "tensor(0.3557, dtype=torch.float64)\n",
      "tensor(0.2923, dtype=torch.float64)\n",
      "tensor(0.2907, dtype=torch.float64)\n",
      "tensor(0.3177, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# buffer data for performance\n",
    "dataloader = DataLoader(dataset, batch_size=10000,\n",
    "                        shuffle=False, num_workers=0, pin_memory=True)\n",
    "size = len(dataloader.dataset)\n",
    "for batch, (X, y) in enumerate(dataloader):\n",
    "#     continue\n",
    "#     print(X['row_id'][0])\n",
    "    print(loss_fn(y, X['x_realized_volatility']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "15e35973-37f1-408a-b47f-3a8fa6c9879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(6, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "def loss_fn(y, pred):\n",
    "    return torch.sqrt(torch.mean(torch.square((y-pred)/y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128498b-e124-4862-990c-a9ae5bb0fc8c",
   "metadata": {},
   "source": [
    "### LEarning rate: our base line is 0.34 loss as that's what the optiver guys have when they use current 10 min realize vol and use it as target (copy to prediction). We create simplest neural network and work with learning rates to figure out what's best and when we see something in range of 0.35 then we've found good Learning rate\n",
    "- #### GBM: 1e-7 works best\n",
    "- #### ADAM: 1e-3 works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "5cb3871f-5215-4afb-92fc-47ae2d385fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "cef34a31-7317-47c0-a94f-9bebe2fd2e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 117.719505  [    0/428932]\n",
      "loss: 0.627800  [85760/428932]\n",
      "loss: 0.409063  [171520/428932]\n",
      "loss: 0.380505  [257280/428932]\n",
      "loss: 0.280221  [343040/428932]\n",
      "LOSSES TRAIN: 1.1016393478255315 LOSSES TEST: 0.28767515236677116\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.293525  [    0/428932]\n",
      "loss: 0.402002  [85760/428932]\n",
      "loss: 0.274830  [171520/428932]\n",
      "loss: 0.502549  [257280/428932]\n",
      "loss: 0.278513  [343040/428932]\n",
      "LOSSES TRAIN: 0.36125285879294405 LOSSES TEST: 0.26691711317861727\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.298576  [    0/428932]\n",
      "loss: 0.310002  [85760/428932]\n",
      "loss: 0.402453  [171520/428932]\n",
      "loss: 0.263494  [257280/428932]\n",
      "loss: 0.401027  [343040/428932]\n",
      "LOSSES TRAIN: 0.33885335577363535 LOSSES TEST: 0.34954684589399854\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.325481  [    0/428932]\n",
      "loss: 0.324930  [85760/428932]\n",
      "loss: 0.270447  [171520/428932]\n",
      "loss: 0.903530  [257280/428932]\n",
      "loss: 0.262508  [343040/428932]\n",
      "LOSSES TRAIN: 0.340659904581693 LOSSES TEST: 0.2902145672320075\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.254146  [    0/428932]\n",
      "loss: 0.290942  [85760/428932]\n",
      "loss: 0.359652  [171520/428932]\n",
      "loss: 1.127457  [257280/428932]\n",
      "loss: 0.238312  [343040/428932]\n",
      "LOSSES TRAIN: 0.33203416593659063 LOSSES TEST: 0.28630261826568537\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.282283  [    0/428932]\n",
      "loss: 0.297852  [85760/428932]\n",
      "loss: 0.264034  [171520/428932]\n",
      "loss: 0.430172  [257280/428932]\n",
      "loss: 0.195793  [343040/428932]\n",
      "LOSSES TRAIN: 0.32280152145160396 LOSSES TEST: 0.475342647230154\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.463650  [    0/428932]\n",
      "loss: 0.349556  [85760/428932]\n",
      "loss: 0.325086  [171520/428932]\n",
      "loss: 0.229478  [257280/428932]\n",
      "loss: 0.335485  [343040/428932]\n",
      "LOSSES TRAIN: 0.3200053932789278 LOSSES TEST: 0.3612307921707586\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.393656  [    0/428932]\n",
      "loss: 0.316132  [85760/428932]\n",
      "loss: 0.422856  [171520/428932]\n",
      "loss: 0.724275  [257280/428932]\n",
      "loss: 0.343686  [343040/428932]\n",
      "LOSSES TRAIN: 0.315285505387197 LOSSES TEST: 0.2600528130017493\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.236242  [    0/428932]\n",
      "loss: 0.248767  [85760/428932]\n",
      "loss: 0.237916  [171520/428932]\n",
      "loss: 0.269439  [257280/428932]\n",
      "loss: 0.261257  [343040/428932]\n",
      "LOSSES TRAIN: 0.3049871757159656 LOSSES TEST: 0.25652890645195353\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.277748  [    0/428932]\n",
      "loss: 0.216479  [85760/428932]\n",
      "loss: 0.286653  [171520/428932]\n",
      "loss: 0.242700  [257280/428932]\n",
      "loss: 0.269216  [343040/428932]\n",
      "LOSSES TRAIN: 0.3097112864444879 LOSSES TEST: 0.2992094151236778\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.297854  [    0/428932]\n",
      "loss: 0.308436  [85760/428932]\n",
      "loss: 0.276784  [171520/428932]\n",
      "loss: 0.238326  [257280/428932]\n",
      "loss: 0.280076  [343040/428932]\n",
      "LOSSES TRAIN: 0.3089609740413823 LOSSES TEST: 0.25649539135135346\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.258438  [    0/428932]\n",
      "loss: 0.236873  [85760/428932]\n",
      "loss: 0.329499  [171520/428932]\n",
      "loss: 0.306049  [257280/428932]\n",
      "loss: 0.301775  [343040/428932]\n",
      "LOSSES TRAIN: 0.30383964878649733 LOSSES TEST: 0.2587512056610639\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.258903  [    0/428932]\n",
      "loss: 0.632180  [85760/428932]\n",
      "loss: 0.260790  [171520/428932]\n",
      "loss: 0.319757  [257280/428932]\n",
      "loss: 0.274917  [343040/428932]\n",
      "LOSSES TRAIN: 0.30027083806062715 LOSSES TEST: 0.26160432244973963\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.229937  [    0/428932]\n",
      "loss: 0.287860  [85760/428932]\n",
      "loss: 0.282317  [171520/428932]\n",
      "loss: 0.317264  [257280/428932]\n",
      "loss: 0.240503  [343040/428932]\n",
      "LOSSES TRAIN: 0.29894515377612135 LOSSES TEST: 0.3232133315069119\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.340646  [    0/428932]\n",
      "loss: 0.240333  [85760/428932]\n",
      "loss: 0.524265  [171520/428932]\n",
      "loss: 0.230832  [257280/428932]\n",
      "loss: 0.737001  [343040/428932]\n",
      "LOSSES TRAIN: 0.2942887922468936 LOSSES TEST: 0.25766215566198475\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.229701  [    0/428932]\n",
      "loss: 0.270844  [85760/428932]\n",
      "loss: 0.455879  [171520/428932]\n",
      "loss: 0.347664  [257280/428932]\n",
      "loss: 0.304175  [343040/428932]\n",
      "LOSSES TRAIN: 0.2978109384746927 LOSSES TEST: 0.29595515980905424\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.326051  [    0/428932]\n",
      "loss: 0.365383  [85760/428932]\n",
      "loss: 0.320674  [171520/428932]\n",
      "loss: 0.281576  [257280/428932]\n",
      "loss: 0.281429  [343040/428932]\n",
      "LOSSES TRAIN: 0.2971328351839804 LOSSES TEST: 0.2695704412873693\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.286648  [    0/428932]\n",
      "loss: 0.315860  [85760/428932]\n",
      "loss: 0.244449  [171520/428932]\n",
      "loss: 0.301712  [257280/428932]\n",
      "loss: 0.367992  [343040/428932]\n",
      "LOSSES TRAIN: 0.29393521936159445 LOSSES TEST: 0.2933112770252313\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.312264  [    0/428932]\n",
      "loss: 0.435921  [85760/428932]\n",
      "loss: 0.254646  [171520/428932]\n",
      "loss: 0.276588  [257280/428932]\n",
      "loss: 0.343888  [343040/428932]\n",
      "LOSSES TRAIN: 0.2953624332525845 LOSSES TEST: 0.3003451040349373\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.266181  [    0/428932]\n",
      "loss: 0.299116  [85760/428932]\n",
      "loss: 0.264065  [171520/428932]\n",
      "loss: 0.345822  [257280/428932]\n",
      "loss: 0.293883  [343040/428932]\n",
      "LOSSES TRAIN: 0.2925918058772578 LOSSES TEST: 0.3086650153645644\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.294765  [    0/428932]\n",
      "loss: 0.264604  [85760/428932]\n",
      "loss: 0.224915  [171520/428932]\n",
      "loss: 0.224899  [257280/428932]\n",
      "loss: 0.283144  [343040/428932]\n",
      "LOSSES TRAIN: 0.28880882023075366 LOSSES TEST: 0.3141924586649887\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.261783  [    0/428932]\n",
      "loss: 0.326049  [85760/428932]\n",
      "loss: 0.206894  [171520/428932]\n",
      "loss: 0.278496  [257280/428932]\n",
      "loss: 0.266336  [343040/428932]\n",
      "LOSSES TRAIN: 0.2917095716543849 LOSSES TEST: 0.2872365473656402\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.309555  [    0/428932]\n",
      "loss: 0.246441  [85760/428932]\n",
      "loss: 0.233489  [171520/428932]\n",
      "loss: 0.254529  [257280/428932]\n",
      "loss: 0.270404  [343040/428932]\n",
      "LOSSES TRAIN: 0.28715457302872166 LOSSES TEST: 0.2889694079790073\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.281465  [    0/428932]\n",
      "loss: 0.269528  [85760/428932]\n",
      "loss: 0.434699  [171520/428932]\n",
      "loss: 0.242287  [257280/428932]\n",
      "loss: 0.289582  [343040/428932]\n",
      "LOSSES TRAIN: 0.28619249741706954 LOSSES TEST: 0.2673182885074331\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.355001  [    0/428932]\n",
      "loss: 0.275494  [85760/428932]\n",
      "loss: 0.280297  [171520/428932]\n",
      "loss: 0.213677  [257280/428932]\n",
      "loss: 0.190688  [343040/428932]\n",
      "LOSSES TRAIN: 0.2848684392183052 LOSSES TEST: 0.2608854153723436\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.273661  [    0/428932]\n",
      "loss: 0.261311  [85760/428932]\n",
      "loss: 0.313398  [171520/428932]\n",
      "loss: 0.250268  [257280/428932]\n",
      "loss: 0.266409  [343040/428932]\n",
      "LOSSES TRAIN: 0.2879864138115791 LOSSES TEST: 0.29869759552830816\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.321275  [    0/428932]\n",
      "loss: 0.270829  [85760/428932]\n",
      "loss: 0.232570  [171520/428932]\n",
      "loss: 0.247636  [257280/428932]\n",
      "loss: 0.267168  [343040/428932]\n",
      "LOSSES TRAIN: 0.28646974601838093 LOSSES TEST: 0.30102632840918925\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.304223  [    0/428932]\n",
      "loss: 0.265243  [85760/428932]\n",
      "loss: 0.267991  [171520/428932]\n",
      "loss: 0.259548  [257280/428932]\n",
      "loss: 0.289834  [343040/428932]\n",
      "LOSSES TRAIN: 0.28660268940695716 LOSSES TEST: 0.25375750243085965\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.226525  [    0/428932]\n",
      "loss: 0.287394  [85760/428932]\n",
      "loss: 0.231141  [171520/428932]\n",
      "loss: 0.273436  [257280/428932]\n",
      "loss: 0.230533  [343040/428932]\n",
      "LOSSES TRAIN: 0.2873015600475123 LOSSES TEST: 0.2683984408020351\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.281602  [    0/428932]\n",
      "loss: 0.258082  [85760/428932]\n",
      "loss: 0.416280  [171520/428932]\n",
      "loss: 0.300523  [257280/428932]\n",
      "loss: 0.264893  [343040/428932]\n",
      "LOSSES TRAIN: 0.28590972506479734 LOSSES TEST: 0.2546224252978516\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.230370  [    0/428932]\n",
      "loss: 0.275354  [85760/428932]\n",
      "loss: 0.488910  [171520/428932]\n",
      "loss: 0.295455  [257280/428932]\n",
      "loss: 0.267825  [343040/428932]\n",
      "LOSSES TRAIN: 0.283823405476881 LOSSES TEST: 0.29301083284603135\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.242219  [    0/428932]\n",
      "loss: 0.250955  [85760/428932]\n",
      "loss: 0.236033  [171520/428932]\n",
      "loss: 0.228046  [257280/428932]\n",
      "loss: 0.318520  [343040/428932]\n",
      "LOSSES TRAIN: 0.2839281017551979 LOSSES TEST: 0.40007662790911486\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.364363  [    0/428932]\n",
      "loss: 0.277370  [85760/428932]\n",
      "loss: 0.247377  [171520/428932]\n",
      "loss: 0.328713  [257280/428932]\n",
      "loss: 0.339450  [343040/428932]\n",
      "LOSSES TRAIN: 0.2863628585273674 LOSSES TEST: 0.27935188135311373\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.239669  [    0/428932]\n",
      "loss: 0.303795  [85760/428932]\n",
      "loss: 0.233295  [171520/428932]\n",
      "loss: 0.224244  [257280/428932]\n",
      "loss: 0.295434  [343040/428932]\n",
      "LOSSES TRAIN: 0.28176585182753217 LOSSES TEST: 0.3152975125000781\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.422726  [    0/428932]\n",
      "loss: 0.272539  [85760/428932]\n",
      "loss: 0.282971  [171520/428932]\n",
      "loss: 0.285017  [257280/428932]\n",
      "loss: 0.240054  [343040/428932]\n",
      "LOSSES TRAIN: 0.2852781356653808 LOSSES TEST: 0.2642691553883549\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.234294  [    0/428932]\n",
      "loss: 0.272748  [85760/428932]\n",
      "loss: 0.259024  [171520/428932]\n",
      "loss: 0.283935  [257280/428932]\n",
      "loss: 0.231878  [343040/428932]\n",
      "LOSSES TRAIN: 0.28460858050276655 LOSSES TEST: 0.26219176877176115\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.219085  [    0/428932]\n",
      "loss: 0.253107  [85760/428932]\n",
      "loss: 0.354267  [171520/428932]\n",
      "loss: 0.256097  [257280/428932]\n",
      "loss: 0.260242  [343040/428932]\n",
      "LOSSES TRAIN: 0.2857332511157831 LOSSES TEST: 0.2555637037371807\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.247176  [    0/428932]\n",
      "loss: 0.274818  [85760/428932]\n",
      "loss: 0.240031  [171520/428932]\n",
      "loss: 0.229240  [257280/428932]\n",
      "loss: 0.347910  [343040/428932]\n",
      "LOSSES TRAIN: 0.2820899380410576 LOSSES TEST: 0.3504232472443563\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.360898  [    0/428932]\n",
      "loss: 0.256866  [85760/428932]\n",
      "loss: 0.265771  [171520/428932]\n",
      "loss: 0.272464  [257280/428932]\n",
      "loss: 0.307938  [343040/428932]\n",
      "LOSSES TRAIN: 0.28220077590993964 LOSSES TEST: 0.27675521497091726\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.262554  [    0/428932]\n",
      "loss: 0.247127  [85760/428932]\n",
      "loss: 0.251970  [171520/428932]\n",
      "loss: 0.213921  [257280/428932]\n",
      "loss: 0.250922  [343040/428932]\n",
      "LOSSES TRAIN: 0.28406183172332755 LOSSES TEST: 0.2543673484984127\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.244541  [    0/428932]\n",
      "loss: 0.389051  [85760/428932]\n",
      "loss: 0.280689  [171520/428932]\n",
      "loss: 0.260297  [257280/428932]\n",
      "loss: 0.285575  [343040/428932]\n",
      "LOSSES TRAIN: 0.2819778521235022 LOSSES TEST: 0.25914947715248304\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.236643  [    0/428932]\n",
      "loss: 0.298855  [85760/428932]\n",
      "loss: 0.344112  [171520/428932]\n",
      "loss: 0.288055  [257280/428932]\n",
      "loss: 0.268329  [343040/428932]\n",
      "LOSSES TRAIN: 0.28362606240437127 LOSSES TEST: 0.25559401701049606\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.230093  [    0/428932]\n",
      "loss: 0.296645  [85760/428932]\n",
      "loss: 0.242594  [171520/428932]\n",
      "loss: 0.251475  [257280/428932]\n",
      "loss: 0.261897  [343040/428932]\n",
      "LOSSES TRAIN: 0.28166651742443227 LOSSES TEST: 0.30670055965938114\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.309711  [    0/428932]\n",
      "loss: 0.358905  [85760/428932]\n",
      "loss: 0.288339  [171520/428932]\n",
      "loss: 0.218591  [257280/428932]\n",
      "loss: 0.302746  [343040/428932]\n",
      "LOSSES TRAIN: 0.28369755712252687 LOSSES TEST: 0.4042450028022308\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.309805  [    0/428932]\n",
      "loss: 0.316419  [85760/428932]\n",
      "loss: 0.372167  [171520/428932]\n",
      "loss: 0.372819  [257280/428932]\n",
      "loss: 0.288914  [343040/428932]\n",
      "LOSSES TRAIN: 0.283204969359129 LOSSES TEST: 0.26300319797018407\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.266834  [    0/428932]\n",
      "loss: 0.262041  [85760/428932]\n",
      "loss: 0.244499  [171520/428932]\n",
      "loss: 0.269647  [257280/428932]\n",
      "loss: 0.228548  [343040/428932]\n",
      "LOSSES TRAIN: 0.2840890370417735 LOSSES TEST: 0.253837004381716\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.226536  [    0/428932]\n",
      "loss: 0.331925  [85760/428932]\n",
      "loss: 0.266437  [171520/428932]\n",
      "loss: 0.219607  [257280/428932]\n",
      "loss: 0.375336  [343040/428932]\n",
      "LOSSES TRAIN: 0.2831222589132235 LOSSES TEST: 0.2562696147240604\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.250701  [    0/428932]\n",
      "loss: 0.218521  [85760/428932]\n",
      "loss: 0.259852  [171520/428932]\n",
      "loss: 0.221382  [257280/428932]\n",
      "loss: 0.250207  [343040/428932]\n",
      "LOSSES TRAIN: 0.28352875925297616 LOSSES TEST: 0.255324772473683\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.286935  [    0/428932]\n",
      "loss: 0.281087  [85760/428932]\n",
      "loss: 0.271442  [171520/428932]\n",
      "loss: 0.234458  [257280/428932]\n",
      "loss: 0.226075  [343040/428932]\n",
      "LOSSES TRAIN: 0.2811873003712611 LOSSES TEST: 0.2586712167389263\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.272837  [    0/428932]\n",
      "loss: 0.267483  [85760/428932]\n",
      "loss: 0.352658  [171520/428932]\n",
      "loss: 0.246879  [257280/428932]\n",
      "loss: 0.205594  [343040/428932]\n",
      "LOSSES TRAIN: 0.28099780620055964 LOSSES TEST: 0.27866326410276154\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.277914  [    0/428932]\n",
      "loss: 0.283633  [85760/428932]\n",
      "loss: 0.253453  [171520/428932]\n",
      "loss: 0.229376  [257280/428932]\n",
      "loss: 0.361571  [343040/428932]\n",
      "LOSSES TRAIN: 0.282734948017623 LOSSES TEST: 0.27193359417269963\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.237534  [    0/428932]\n",
      "loss: 0.417930  [85760/428932]\n",
      "loss: 0.210764  [171520/428932]\n",
      "loss: 0.204017  [257280/428932]\n",
      "loss: 0.258613  [343040/428932]\n",
      "LOSSES TRAIN: 0.27957388326520133 LOSSES TEST: 0.2949480020026499\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.250097  [    0/428932]\n",
      "loss: 0.253723  [85760/428932]\n",
      "loss: 0.234178  [171520/428932]\n",
      "loss: 0.297938  [257280/428932]\n",
      "loss: 0.332958  [343040/428932]\n",
      "LOSSES TRAIN: 0.28340515091387203 LOSSES TEST: 0.26086737199962184\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.284435  [    0/428932]\n",
      "loss: 0.256391  [85760/428932]\n",
      "loss: 0.231532  [171520/428932]\n",
      "loss: 0.315621  [257280/428932]\n",
      "loss: 0.278821  [343040/428932]\n",
      "LOSSES TRAIN: 0.2803571953541394 LOSSES TEST: 0.2540508232549622\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.238726  [    0/428932]\n",
      "loss: 0.254229  [85760/428932]\n",
      "loss: 0.299139  [171520/428932]\n",
      "loss: 0.290391  [257280/428932]\n",
      "loss: 0.321090  [343040/428932]\n",
      "LOSSES TRAIN: 0.28293872293097366 LOSSES TEST: 0.3694937117814484\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.382680  [    0/428932]\n",
      "loss: 0.271049  [85760/428932]\n",
      "loss: 0.223053  [171520/428932]\n",
      "loss: 0.228250  [257280/428932]\n",
      "loss: 0.399443  [343040/428932]\n",
      "LOSSES TRAIN: 0.2808182218328468 LOSSES TEST: 0.25396291227104234\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.222349  [    0/428932]\n",
      "loss: 0.225523  [85760/428932]\n",
      "loss: 0.288537  [171520/428932]\n",
      "loss: 0.225241  [257280/428932]\n",
      "loss: 0.884150  [343040/428932]\n",
      "LOSSES TRAIN: 0.2797446922773925 LOSSES TEST: 0.8487924878318127\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.921629  [    0/428932]\n",
      "loss: 1.530382  [85760/428932]\n",
      "loss: 0.374932  [171520/428932]\n",
      "loss: 0.280352  [257280/428932]\n",
      "loss: 0.271465  [343040/428932]\n",
      "LOSSES TRAIN: 0.2809292503459958 LOSSES TEST: 0.2544510309305945\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.263872  [    0/428932]\n",
      "loss: 0.281775  [85760/428932]\n",
      "loss: 0.269122  [171520/428932]\n",
      "loss: 0.295278  [257280/428932]\n",
      "loss: 0.259282  [343040/428932]\n",
      "LOSSES TRAIN: 0.28267701003521245 LOSSES TEST: 0.26360009025051734\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.273076  [    0/428932]\n",
      "loss: 0.193292  [85760/428932]\n",
      "loss: 0.292997  [171520/428932]\n",
      "loss: 0.276009  [257280/428932]\n",
      "loss: 0.306362  [343040/428932]\n",
      "LOSSES TRAIN: 0.28246809853152616 LOSSES TEST: 0.2540506329067957\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.271729  [    0/428932]\n",
      "loss: 0.253893  [85760/428932]\n",
      "loss: 0.265232  [171520/428932]\n",
      "loss: 0.317378  [257280/428932]\n",
      "loss: 0.298474  [343040/428932]\n",
      "LOSSES TRAIN: 0.2799129963604784 LOSSES TEST: 0.3331816983058504\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.302933  [    0/428932]\n",
      "loss: 0.241165  [85760/428932]\n",
      "loss: 0.239533  [171520/428932]\n",
      "loss: 0.234085  [257280/428932]\n",
      "loss: 0.228943  [343040/428932]\n",
      "LOSSES TRAIN: 0.2817633343316512 LOSSES TEST: 0.2545520170778592\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.289328  [    0/428932]\n",
      "loss: 0.339675  [85760/428932]\n",
      "loss: 0.387509  [171520/428932]\n",
      "loss: 0.257419  [257280/428932]\n",
      "loss: 0.575047  [343040/428932]\n",
      "LOSSES TRAIN: 0.2826757553307613 LOSSES TEST: 0.2541713694486931\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.238691  [    0/428932]\n",
      "loss: 0.304033  [85760/428932]\n",
      "loss: 0.251508  [171520/428932]\n",
      "loss: 0.294392  [257280/428932]\n",
      "loss: 0.479199  [343040/428932]\n",
      "LOSSES TRAIN: 0.2794804342621878 LOSSES TEST: 0.3186837615560543\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.308838  [    0/428932]\n",
      "loss: 0.288153  [85760/428932]\n",
      "loss: 0.432190  [171520/428932]\n",
      "loss: 0.249423  [257280/428932]\n",
      "loss: 0.267634  [343040/428932]\n",
      "LOSSES TRAIN: 0.2820809687620132 LOSSES TEST: 0.2780288522875549\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.252173  [    0/428932]\n",
      "loss: 0.212082  [85760/428932]\n",
      "loss: 0.262811  [171520/428932]\n",
      "loss: 0.216223  [257280/428932]\n",
      "loss: 0.217233  [343040/428932]\n",
      "LOSSES TRAIN: 0.28004065596459293 LOSSES TEST: 0.3294046447069942\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.325018  [    0/428932]\n",
      "loss: 0.394767  [85760/428932]\n",
      "loss: 0.245870  [171520/428932]\n",
      "loss: 0.233436  [257280/428932]\n",
      "loss: 0.340887  [343040/428932]\n",
      "LOSSES TRAIN: 0.28355925399025 LOSSES TEST: 0.28226529301812986\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.321260  [    0/428932]\n",
      "loss: 0.222582  [85760/428932]\n",
      "loss: 0.287989  [171520/428932]\n",
      "loss: 0.521034  [257280/428932]\n",
      "loss: 0.312231  [343040/428932]\n",
      "LOSSES TRAIN: 0.28132901075325634 LOSSES TEST: 0.25493644863522646\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.259388  [    0/428932]\n",
      "loss: 0.349767  [85760/428932]\n",
      "loss: 0.201002  [171520/428932]\n",
      "loss: 0.290820  [257280/428932]\n",
      "loss: 0.222954  [343040/428932]\n",
      "LOSSES TRAIN: 0.2789468473238271 LOSSES TEST: 0.2653587939444093\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.247492  [    0/428932]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8624/1903526697.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mlosses_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\bsstonks\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    dataloader_train = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0, pin_memory=True)\n",
    "    dataset_size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    losses_train = []\n",
    "    for batch, (X, y) in enumerate(dataloader_train):\n",
    "        X = torch.cat((X['x_realized_volatility'], X['x_wap_120s']), 1)\n",
    "        \n",
    "        \n",
    "        X = X.type(torch.cuda.FloatTensor)\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(y, pred)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses_train.append(loss.item())\n",
    "        # we want 5 spread out output per epoch\n",
    "        if batch % int(dataset_size/5/batch_size) == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            \n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{dataset_size:>5d}]\")\n",
    "            \n",
    "    dataloader_test = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0, pin_memory=True)\n",
    "    dataset_size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "\n",
    "    losses_test = []\n",
    "    for batch, (X, y) in enumerate(dataloader_test):\n",
    "        with torch.no_grad():\n",
    "            X = torch.cat((X['x_realized_volatility'], X['x_wap_120s']), 1)\n",
    "            X = X.type(torch.cuda.FloatTensor)\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(y, pred)\n",
    "            losses_test.append(loss.item())\n",
    "            \n",
    "    print(\"LOSSES TRAIN:\", np.mean(losses_train), \"LOSSES TEST:\", np.mean(losses_test))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "92f6420b-e415-4c48-8e43-cb3c407b33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(OUTPUT_DIRECTORY,\"03_wap120s_model_2hidden_layer.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800f3dd-258a-4873-ba48-98a26bca027a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
