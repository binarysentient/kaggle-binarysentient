{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4dade30-94ed-4d49-bb1e-553b91996bb0",
   "metadata": {},
   "source": [
    "## IMPUTE INSANITY!:\n",
    "### before we move onto neural networks we need to impute the NANs\n",
    "#### How do we even know we've correctly imputed?\n",
    "#### **Try out every impute methods and per feature set train the GBM and see which one got most importance**\n",
    "\n",
    "### TODO\n",
    "- #### [KNN , DeepLearner, other novel ways to impute](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)\n",
    "  - KNN thoughts: we can perhaps use subset of features with no nans, but then the records will become similar, and decision trees already predict the same result for similar looking rows? then what's the point of imputing value same as similar looking rows?\n",
    "- ### DON't Impute rather provide the Flag wether the feature has value or not, kinda like mask; we did this in Optiver (not the transformer sequence mask , but we have mask about weather the second/grouped seconds is missing or not)\n",
    "  - #### [Results on passing information about missing values to neural networks](https://machinelearningmastery.com/binary-flags-for-missing-values-for-machine-learning/)\n",
    "- ### AMEND ABOVE: Impute as well as pass the missing Flag too, so neural network learns better\n",
    "- ### Categorical Imputation: train on data with missing categorical values as target variable! **We can even use TEST set to do this!!!! yay!**\n",
    "  - #### [Training to find missing categorical values](https://www.analyticsvidhya.com/blog/2021/04/how-to-handle-missing-values-of-categorical-variables/)\n",
    "\n",
    "### IMPORTANT FLOAT16 aggregations return NONE!!! mean, sum most of it doesn't work :-/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d809121-a984-441c-8885-a94938548420",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- ### MULTILEVEL IMPUTATION DEFAULT\n",
    "- ### LEVEL 1: out of all the variant+original; select the one with best amex scores DEFAULT_EMP1\n",
    "    - #### This is partial nans\n",
    "- ### Level 2: now that we have partial none df with best variants(including original with nans) ; TRAIN ML MODEL to have PREDICTED/BS variant\n",
    "    - #### In the order of lowest missing values to highest missing values\n",
    "    - #### we Target the feature with lowest amount on NAN as we have maximum training rows; then slowly cascade to more missing row features\n",
    "    - #### once we have computed PREDICTED/BS variants for all of the features with nans\n",
    "- ### Level 3: from DEFAULT_EMP1 vs PREDICTED variant select the one with maximum amex score (this is kinda like back to stage 1 but with improved train_df_wt)\n",
    "    - #### repetetively select and choose the best varint out of the two\n",
    "    - #### DEFAULT_EMP2 is generated!\n",
    "    - #### Now there is a scope of repeating Level 2 again with using base_df of EMP2 and then going to EMP3,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799d3de7-7230-42ee-b699-26d02974f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHERE_THIS = \"local\" # local|kaggle\n",
    "\n",
    "BS_DATASET = \"amex-default-prediction-binarysentient\"\n",
    "if WHERE_THIS == \"kaggle\":\n",
    "    INPUT_PATH = \"/kaggle/input/amex-default-prediction/\"\n",
    "    OUTPUT_PATH = \"/kaggle/working/\"\n",
    "    TEMP_PATH = \"/kaggle/temp/\"\n",
    "elif WHERE_THIS == \"local\":\n",
    "    INPUT_PATH = \"input/amex-default-prediction/\"\n",
    "    OUTPUT_PATH = \"working/\"\n",
    "    TEMP_PATH = \"temp/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b8e156e-282d-4a57-8738-46695bcc46cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/amex-default-prediction/sample_submission.csv\n",
      "input/amex-default-prediction/test_data.csv\n",
      "input/amex-default-prediction/train_data.csv\n",
      "input/amex-default-prediction/train_labels.csv\n",
      "input/amex-default-prediction/.ipynb_checkpoints\\sample_submission-checkpoint.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from IPython.display import display, HTML\n",
    "from lightgbm import LGBMClassifier, log_evaluation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import plotly.express as pltex\n",
    "\n",
    "import impute_insanity\n",
    "from impute_insanity import load_prepare_amex_dataset\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "for dirname, _, filenames in os.walk(INPUT_PATH):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f240c1ff-1841-49f9-b7af-c868c9dd855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_features = ['customer_ID']\n",
    "datetime_features = ['S_2']\n",
    "target_features = ['target']\n",
    "cat_features = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "numeric_features = ['R_2', 'S_25', 'D_118', 'B_5', 'D_60', 'B_21', 'D_115', 'S_15', 'D_84', 'D_122', 'B_9', 'B_33', 'D_53', 'R_24', 'D_94', 'D_56', 'D_139', 'R_28', 'B_3', 'S_20', 'B_31', 'D_133', 'B_2', 'S_12', 'D_71', 'D_96', 'S_7', 'D_72', 'B_36', 'B_41', 'S_5', 'D_41', 'R_22', 'R_8', 'D_140', 'D_47', 'D_89', 'P_2', 'R_19', 'D_59', 'B_23', 'S_3', 'D_145', 'D_103', 'B_19', 'R_20', 'D_73', 'D_136', 'D_141', 'D_142', 'B_22', 'D_46', 'B_29', 'B_25', 'D_128', 'B_18', 'D_86', 'D_109', 'B_8', 'B_17', 'R_17', 'B_12', 'D_54', 'D_74', 'S_16', 'B_6', 'D_49', 'D_80', 'S_8', 'B_7', 'D_144', 'B_27', 'B_26', 'R_25', 'R_23', 'D_82', 'D_111', 'B_10', 'D_113', 'R_4', 'D_48', 'R_26', 'S_23', 'B_11', 'D_104', 'D_134', 'D_79', 'P_3', 'D_132', 'D_137', 'D_135', 'B_28', 'D_88', 'S_13', 'D_51', 'D_61', 'D_75', 'D_69', 'R_16', 'S_6', 'S_17', 'D_93', 'B_20', 'D_112', 'D_123', 'D_130', 'B_1', 'D_78', 'D_92', 'S_27', 'D_44', 'B_16', 'R_5', 'D_43', 'S_18', 'B_15', 'D_39', 'D_50', 'D_55', 'S_9', 'D_105', 'D_70', 'R_18', 'D_125', 'D_58', 'S_24', 'D_110', 'D_42', 'R_6', 'D_81', 'R_7', 'D_138', 'D_52', 'R_27', 'D_124', 'D_45', 'D_91', 'D_108', 'S_22', 'B_14', 'D_83', 'R_13', 'D_87', 'S_19', 'D_131', 'R_21', 'B_40', 'R_3', 'D_65', 'B_13', 'D_129', 'D_119', 'B_32', 'R_9', 'B_24', 'D_127', 'D_106', 'D_102', 'R_1', 'R_14', 'B_37', 'D_107', 'R_15', 'R_12', 'P_4', 'R_10', 'customer_ID', 'D_121', 'R_11', 'S_11', 'D_76', 'D_143', 'B_39', 'B_42', 'D_62', 'B_4', 'S_26', 'D_77']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e62d91-78a3-44d5-a4dc-3697f03206a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "## TODO: FAST AMEX implementation is not accurate; convert the dataframe accurate version to directly work with numpy arrays\n",
    "# @yunchonggan's fast metric implementation\n",
    "# From https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "def fast_amex_metric(y_true: np.array, y_pred: np.array) -> float:\n",
    "\n",
    "    # count of positives and negatives\n",
    "    n_pos = y_true.sum()\n",
    "    n_neg = y_true.shape[0] - n_pos\n",
    "\n",
    "    # sorting by descring prediction values\n",
    "    indices = np.argsort(y_pred)[::-1]\n",
    "    preds, target = y_pred[indices], y_true[indices]\n",
    "\n",
    "    # filter the top 4% by cumulative row weights\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_filter = cum_norm_weight <= 0.04\n",
    "\n",
    "    # default rate captured at 4%\n",
    "    d = target[four_pct_filter].sum() / n_pos\n",
    "\n",
    "    # weighted gini coefficient\n",
    "    lorentz = (target / n_pos).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    # max weighted gini coefficient\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    # normalized weighted gini coefficient\n",
    "    g = gini / gini_max\n",
    "\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "# Need Lightgbm supported eval metric\n",
    "#Custom eval function expects a callable with following signatures: func(y_true, y_pred), func(y_true, y_pred, weight) or func(y_true, y_pred, weight, group) and returns (eval_name, eval_result, is_higher_better) or list of (eval_name, eval_result, is_higher_better):\n",
    "def lgbm_eval_metric_amex(y_true, y_pred):\n",
    "    amex_metric = fast_amex_metric(y_true, y_pred)\n",
    "    return ('amex', amex_metric, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ccf182-f698-4fec-bd9b-d386027d57ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458908</th>\n",
       "      <td>ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458909</th>\n",
       "      <td>ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458910</th>\n",
       "      <td>ffff9984b999fccb2b6127635ed0736dda94e544e67e02...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458911</th>\n",
       "      <td>ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458912</th>\n",
       "      <td>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458913 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              customer_ID  target\n",
       "0       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...     0.0\n",
       "1       00000fd6641609c6ece5454664794f0340ad84dddce9a2...     0.0\n",
       "2       00001b22f846c82c51f6e3958ccd81970162bae8b007e8...     0.0\n",
       "3       000041bdba6ecadd89a52d11886e8eaaec9325906c9723...     0.0\n",
       "4       00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...     0.0\n",
       "...                                                   ...     ...\n",
       "458908  ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...     0.0\n",
       "458909  ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...     0.0\n",
       "458910  ffff9984b999fccb2b6127635ed0736dda94e544e67e02...     0.0\n",
       "458911  ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...     1.0\n",
       "458912  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...     0.0\n",
       "\n",
       "[458913 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_df = load_prepare_amex_dataset('train_labels')#, index_col='customer_ID')\n",
    "train_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09052bcc-5d49-4107-a017-2b27a1624677",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels = train_label_df.copy()\n",
    "# prediction_labels['prediction'] = prediction_labels['target'] \n",
    "# prediction_labels['prediction'] = prediction_labels['target'] - 3\n",
    "prediction_labels['prediction'] = 1000\n",
    "del prediction_labels['target'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd1a55-95ff-4d87-9a34-c65d1dd30564",
   "metadata": {},
   "source": [
    "### Predictions impact on Evaluation Metric\n",
    "- #### all predictions 0 :   score 0.018829875288988086\n",
    "- #### all predictions 1 :   score 0.018829875288988086\n",
    "- #### all predictions 0.4 : score 0.018829875288988086\n",
    "- #### same as target      : score 1.0\n",
    "- #### target * 10         : score 1.0\n",
    "- #### target * 0.001      : score 1.0\n",
    "- #### target - 3          : score 1.0\n",
    "- #### target + 3          : score 1.0\n",
    "\n",
    "## The evaluation metrics is solid! amazing, need to understand more\n",
    "## TODO: FAST AMEX implementation is not accurate; convert the dataframe accurate version to directly work with numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf09d96f-a3aa-4382-a65e-1a58c4588135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02260958442726859\n"
     ]
    }
   ],
   "source": [
    "## TODO: FAST AMEX implementation is not accurate; convert the dataframe accurate version to directly work with numpy arrays\n",
    "# print(amex_metric(train_labels, prediction_labels))\n",
    "print(fast_amex_metric(train_label_df['target'], prediction_labels['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78c037fc-0aaf-4c09-8807-0aae76e5c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_with_config(random_state=1, n_estimators=1200, importance_type=None, early_stopping_rounds=None):\n",
    "    \"\"\"\n",
    "    Creates model with our desired and some default hyper params\n",
    "    importance_type: split|gain\n",
    "    \"\"\"\n",
    "    return LGBMClassifier(n_estimators=n_estimators,\n",
    "                          learning_rate=0.03, reg_lambda=50,\n",
    "                          min_child_samples=2400,\n",
    "                          num_leaves=95,\n",
    "                          colsample_bytree=0.19,early_stopping_rounds=early_stopping_rounds,\n",
    "                          max_bins=511, random_state=random_state, importance_type=importance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe33434f-4986-4a7b-8ac4-de48eb0052ed",
   "metadata": {},
   "source": [
    "## Precision impact on scores:\n",
    "\n",
    "### float32: 0.788541\n",
    "### float16: 0.788748, 0.788649, 0.788821\n",
    "### float64: 0.788276, 0.789053, 0.789025, 0.788543\n",
    "\n",
    "\n",
    "## Default\n",
    "\n",
    "## EMP1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "959bc1be-77fe-42db-8dde-de902027aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_score_track = {}\n",
    "def track_score(key, score):\n",
    "    key = str(key)\n",
    "    if key not in fold_score_track:\n",
    "        fold_score_track[key] = []\n",
    "    fold_score_track[key].append(score)\n",
    "\n",
    "def show_score(key):\n",
    "    key = str(key)\n",
    "    display(HTML(f\"<h3>{key} OVERALL SCORE : {np.mean(fold_score_track[key]):0.6f}</h3>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da9083-1a02-4478-9a90-2c24146cc1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6368f18-9663-418e-84eb-413299e659fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lgbm_model_with_config(random_state=1, n_estimators=2400, importance_type=None, early_stopping_rounds=None):\n",
    "    \"\"\"\n",
    "    Creates model with our desired and some default hyper params\n",
    "    importance_type: split|gain\n",
    "    \"\"\"\n",
    "    if early_stopping_rounds is None:\n",
    "        early_stopping_rounds = n_estimators//10\n",
    "    return LGBMClassifier(n_estimators=n_estimators,\n",
    "                          learning_rate=0.03, reg_lambda=50,\n",
    "                          min_child_samples=2400,\n",
    "                          num_leaves=95, num_threads=12,\n",
    "                          colsample_bytree=0.19,early_stopping_rounds=early_stopping_rounds,\n",
    "                          max_bins=511, random_state=random_state, importance_type=importance_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "381bb1be-dea6-48e3-adac-21b57db064fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read test\n",
      "Computed avg test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:26: FutureWarning: Dropping invalid columns in DataFrameGroupBy.min is deprecated. In a future version, a TypeError will be raised. Before calling .min, select only columns which should be valid for the function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed min test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:33: FutureWarning: Dropping invalid columns in DataFrameGroupBy.max is deprecated. In a future version, a TypeError will be raised. Before calling .max, select only columns which should be valid for the function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed max test\n",
      "Computed last test\n",
      "test shape: (924621, 456)\n",
      "Read train\n",
      "Computed avg train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:26: FutureWarning: Dropping invalid columns in DataFrameGroupBy.min is deprecated. In a future version, a TypeError will be raised. Before calling .min, select only columns which should be valid for the function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed min train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:33: FutureWarning: Dropping invalid columns in DataFrameGroupBy.max is deprecated. In a future version, a TypeError will be raised. Before calling .max, select only columns which should be valid for the function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed max train\n",
      "Computed last train\n",
      "train shape: (458913, 456)\n",
      "target shape: (458913,)\n",
      "CPU times: total: 2min 54s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_avg = ['B_1', 'B_2', 'B_3', 'B_4', 'B_5', 'B_6', 'B_8', 'B_9', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_28', 'B_29', 'B_30', 'B_32', 'B_33', 'B_37', 'B_38', 'B_39', 'B_40', 'B_41', 'B_42', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_50', 'D_51', 'D_53', 'D_54', 'D_55', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_65', 'D_66', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_76', 'D_77', 'D_78', 'D_80', 'D_82', 'D_84', 'D_86', 'D_91', 'D_92', 'D_94', 'D_96', 'D_103', 'D_104', 'D_108', 'D_112', 'D_113', 'D_114', 'D_115', 'D_117', 'D_118', 'D_119', 'D_120', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_126', 'D_128', 'D_129', 'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_140', 'D_141', 'D_142', 'D_144', 'D_145', 'P_2', 'P_3', 'P_4', 'R_1', 'R_2', 'R_3', 'R_7', 'R_8', 'R_9', 'R_10', 'R_11', 'R_14', 'R_15', 'R_16', 'R_17', 'R_20', 'R_21', 'R_22', 'R_24', 'R_26', 'R_27', 'S_3', 'S_5', 'S_6', 'S_7', 'S_9', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_18', 'S_22', 'S_23', 'S_25', 'S_26']\n",
    "features_avg = list(set(features_avg).difference(set(cat_features+['D_64', 'D_66', 'D_68'])))\n",
    "features_min = ['B_2', 'B_4', 'B_5', 'B_9', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_19', 'B_20', 'B_28', 'B_29', 'B_33', 'B_36', 'B_42', 'D_39', 'D_41', 'D_42', 'D_45', 'D_46', 'D_48', 'D_50', 'D_51', 'D_53', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_62', 'D_70', 'D_71', 'D_74', 'D_75', 'D_78', 'D_83', 'D_102', 'D_112', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_128', 'D_132', 'D_140', 'D_141', 'D_144', 'D_145', 'P_2', 'P_3', 'R_1', 'R_27', 'S_3', 'S_5', 'S_7', 'S_9', 'S_11', 'S_12', 'S_23', 'S_25']\n",
    "features_min = list(set(features_min).difference(set(cat_features+['D_64', 'D_66', 'D_68'])))\n",
    "features_max = ['B_1', 'B_2', 'B_3', 'B_4', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'B_10', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_21', 'B_23', 'B_24', 'B_25', 'B_29', 'B_30', 'B_33', 'B_37', 'B_38', 'B_39', 'B_40', 'B_42', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_52', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_63', 'D_64', 'D_65', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_76', 'D_77', 'D_78', 'D_80', 'D_82', 'D_84', 'D_91', 'D_102', 'D_105', 'D_107', 'D_110', 'D_111', 'D_112', 'D_115', 'D_116', 'D_117', 'D_118', 'D_119', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_126', 'D_128', 'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_138', 'D_140', 'D_141', 'D_142', 'D_144', 'D_145', 'P_2', 'P_3', 'P_4', 'R_1', 'R_3', 'R_5', 'R_6', 'R_7', 'R_8', 'R_10', 'R_11', 'R_14', 'R_17', 'R_20', 'R_26', 'R_27', 'S_3', 'S_5', 'S_7', 'S_8', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27']\n",
    "features_max = list(set(features_max).difference(set(cat_features+['D_64', 'D_66', 'D_68'])))\n",
    "features_last = ['B_1', 'B_2', 'B_3', 'B_4', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_26', 'B_28', 'B_29', 'B_30', 'B_32', 'B_33', 'B_36', 'B_37', 'B_38', 'B_39', 'B_40', 'B_41', 'B_42', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_63', 'D_64', 'D_65', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_75', 'D_76', 'D_77', 'D_78', 'D_79', 'D_80', 'D_81', 'D_82', 'D_83', 'D_86', 'D_91', 'D_96', 'D_105', 'D_106', 'D_112', 'D_114', 'D_119', 'D_120', 'D_121', 'D_122', 'D_124', 'D_125', 'D_126', 'D_127', 'D_130', 'D_131', 'D_132', 'D_133', 'D_134', 'D_138', 'D_140', 'D_141', 'D_142', 'D_145', 'P_2', 'P_3', 'P_4', 'R_1', 'R_2', 'R_3', 'R_4', 'R_5', 'R_6', 'R_7', 'R_8', 'R_9', 'R_10', 'R_11', 'R_12', 'R_13', 'R_14', 'R_15', 'R_19', 'R_20', 'R_26', 'R_27', 'S_3', 'S_5', 'S_6', 'S_7', 'S_8', 'S_9', 'S_11', 'S_12', 'S_13', 'S_16', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27']\n",
    "features_last = list(set(features_last).union(set(cat_features)).difference(set(['D_64', 'D_68'])))\n",
    "for i in ['test', 'train']:\n",
    "    df = load_prepare_amex_dataset(f\"{i}_data_emp1\")\n",
    "    cid = pd.Categorical(df.pop('customer_ID'), ordered=True)\n",
    "    last = (cid != np.roll(cid, -1)) # mask for last statement of every customer\n",
    "    if 'target' in df.columns:\n",
    "        df.drop(columns=['target'], inplace=True)\n",
    "    gc.collect()\n",
    "    print('Read', i)\n",
    "    df_avg = (df\n",
    "              .groupby(cid)\n",
    "              .mean()[features_avg]\n",
    "              .rename(columns={f: f\"{f}_avg\" for f in features_avg})\n",
    "             )\n",
    "    gc.collect()\n",
    "    print('Computed avg', i)\n",
    "    df_min = (df\n",
    "              .groupby(cid)\n",
    "              .min()[features_min]\n",
    "              .rename(columns={f: f\"{f}_min\" for f in features_min})\n",
    "             )\n",
    "    gc.collect()\n",
    "    print('Computed min', i)\n",
    "    df_max = (df\n",
    "              .groupby(cid)\n",
    "              .max()[features_max]\n",
    "              .rename(columns={f: f\"{f}_max\" for f in features_max})\n",
    "             )\n",
    "    gc.collect()\n",
    "    print('Computed max', i)\n",
    "    df = (df.loc[last, features_last]\n",
    "          .rename(columns={f: f\"{f}_last\" for f in features_last})\n",
    "          .set_index(np.asarray(cid[last]))\n",
    "         )\n",
    "    gc.collect()\n",
    "    print('Computed last', i)\n",
    "    df = pd.concat([df, df_min, df_max, df_avg], axis=1)\n",
    "    if i == 'train': df_train = df\n",
    "    else: df_test = df\n",
    "    print(f\"{i} shape: {df.shape}\")\n",
    "    del df, df_avg, df_min, df_max, cid, last\n",
    "\n",
    "target = load_prepare_amex_dataset('train_labels').target.values\n",
    "print(f\"target shape: {target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3608451e-1569-4a8c-a4be-6be2ed1f6c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data nan 122\n",
      "train_data_emp1 nan 51\n",
      "test_data nan 121\n",
      "test_data_emp1 nan 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dataset in ['train_data','train_data_emp1', 'test_data','test_data_emp1']:\n",
    "    df = load_prepare_amex_dataset(dataset)\n",
    "    nanmap = df.isna().sum().reset_index().rename({'index':'feature',0:'nan_count'}, axis=1).to_dict('records')\n",
    "    print(dataset, \"nan\", len([x for x in nanmap if x['nan_count']>0]))\n",
    "    del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a0622f8c-3f1e-45e7-af4f-d611bd8943b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data nan 122\n",
      "train_data_emp1 nan 35\n",
      "test_data nan 121\n",
      "test_data_emp1 nan 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dataset in ['train_data','train_data_emp1', 'test_data','test_data_emp1']:\n",
    "    df = load_prepare_amex_dataset(dataset)\n",
    "    nanmap = df.isna().sum().reset_index().rename({'index':'feature',0:'nan_count'}, axis=1).to_dict('records')\n",
    "    print(dataset, \"nan\", len([x for x in nanmap if x['nan_count']>0]))\n",
    "    del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6166adb4-de23-43f3-aa6f-b17ac7d71b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ FOLD: 1 of 5 -------\n",
      "[LightGBM] [Warning] num_threads is set=12, n_jobs=-1 will be ignored. Current value: num_threads=12\n",
      "[LightGBM] [Warning] early_stopping_round is set=240, early_stopping_rounds=240 will be ignored. Current value: early_stopping_round=240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nisha\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\nisha\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.244114\tvalid_0's amex: 0.768714\n",
      "[200]\tvalid_0's binary_logloss: 0.224744\tvalid_0's amex: 0.780731\n",
      "[300]\tvalid_0's binary_logloss: 0.219885\tvalid_0's amex: 0.78678\n",
      "[400]\tvalid_0's binary_logloss: 0.217816\tvalid_0's amex: 0.789911\n",
      "[500]\tvalid_0's binary_logloss: 0.216722\tvalid_0's amex: 0.792442\n",
      "[600]\tvalid_0's binary_logloss: 0.216108\tvalid_0's amex: 0.792881\n",
      "[700]\tvalid_0's binary_logloss: 0.215648\tvalid_0's amex: 0.793464\n",
      "[800]\tvalid_0's binary_logloss: 0.215339\tvalid_0's amex: 0.793846\n",
      "[900]\tvalid_0's binary_logloss: 0.21518\tvalid_0's amex: 0.793816\n",
      "[1000]\tvalid_0's binary_logloss: 0.215037\tvalid_0's amex: 0.79429\n",
      "[1100]\tvalid_0's binary_logloss: 0.214904\tvalid_0's amex: 0.794676\n",
      "[1200]\tvalid_0's binary_logloss: 0.214832\tvalid_0's amex: 0.795085\n",
      "[1300]\tvalid_0's binary_logloss: 0.214764\tvalid_0's amex: 0.794439\n",
      "[1400]\tvalid_0's binary_logloss: 0.21476\tvalid_0's amex: 0.794845\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Fold 1 | 02:20 |  1210 trees |                Score = 0.79574 | Importance: Gain</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "------ FOLD: 2 of 5 -------\n",
      "[LightGBM] [Warning] num_threads is set=12, n_jobs=-1 will be ignored. Current value: num_threads=12\n",
      "[LightGBM] [Warning] early_stopping_round is set=240, early_stopping_rounds=240 will be ignored. Current value: early_stopping_round=240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nisha\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\nisha\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.246549\tvalid_0's amex: 0.763578\n",
      "[200]\tvalid_0's binary_logloss: 0.226739\tvalid_0's amex: 0.777681\n",
      "[300]\tvalid_0's binary_logloss: 0.221612\tvalid_0's amex: 0.784608\n",
      "[400]\tvalid_0's binary_logloss: 0.219516\tvalid_0's amex: 0.787212\n",
      "[500]\tvalid_0's binary_logloss: 0.218475\tvalid_0's amex: 0.787442\n",
      "[600]\tvalid_0's binary_logloss: 0.217856\tvalid_0's amex: 0.787805\n",
      "[700]\tvalid_0's binary_logloss: 0.217486\tvalid_0's amex: 0.789159\n",
      "[800]\tvalid_0's binary_logloss: 0.217237\tvalid_0's amex: 0.790109\n",
      "[900]\tvalid_0's binary_logloss: 0.217102\tvalid_0's amex: 0.790914\n",
      "[1000]\tvalid_0's binary_logloss: 0.216996\tvalid_0's amex: 0.791399\n",
      "[1100]\tvalid_0's binary_logloss: 0.217003\tvalid_0's amex: 0.791888\n",
      "[1200]\tvalid_0's binary_logloss: 0.216955\tvalid_0's amex: 0.791701\n",
      "[1300]\tvalid_0's binary_logloss: 0.216942\tvalid_0's amex: 0.791372\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Fold 2 | 02:24 |  1061 trees |                Score = 0.79228 | Importance: Gain</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "------ FOLD: 3 of 5 -------\n",
      "[LightGBM] [Warning] num_threads is set=12, n_jobs=-1 will be ignored. Current value: num_threads=12\n",
      "[LightGBM] [Warning] early_stopping_round is set=240, early_stopping_rounds=240 will be ignored. Current value: early_stopping_round=240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nisha\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\nisha\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.245793\tvalid_0's amex: 0.766484\n",
      "[200]\tvalid_0's binary_logloss: 0.226728\tvalid_0's amex: 0.780877\n",
      "[300]\tvalid_0's binary_logloss: 0.221963\tvalid_0's amex: 0.785967\n",
      "[400]\tvalid_0's binary_logloss: 0.219959\tvalid_0's amex: 0.78893\n",
      "[500]\tvalid_0's binary_logloss: 0.218933\tvalid_0's amex: 0.790296\n",
      "[600]\tvalid_0's binary_logloss: 0.218405\tvalid_0's amex: 0.791326\n",
      "[700]\tvalid_0's binary_logloss: 0.218073\tvalid_0's amex: 0.791759\n",
      "[800]\tvalid_0's binary_logloss: 0.217824\tvalid_0's amex: 0.791806\n",
      "[900]\tvalid_0's binary_logloss: 0.217714\tvalid_0's amex: 0.792287\n",
      "[1000]\tvalid_0's binary_logloss: 0.217582\tvalid_0's amex: 0.792359\n",
      "[1100]\tvalid_0's binary_logloss: 0.217481\tvalid_0's amex: 0.792191\n",
      "[1200]\tvalid_0's binary_logloss: 0.217464\tvalid_0's amex: 0.792769\n",
      "[1300]\tvalid_0's binary_logloss: 0.217528\tvalid_0's amex: 0.792357\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Fold 3 | 02:01 |  1060 trees |                Score = 0.79319 | Importance: Gain</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "------ FOLD: 4 of 5 -------\n",
      "[LightGBM] [Warning] num_threads is set=12, n_jobs=-1 will be ignored. Current value: num_threads=12\n",
      "[LightGBM] [Warning] early_stopping_round is set=240, early_stopping_rounds=240 will be ignored. Current value: early_stopping_round=240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nisha\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\nisha\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.245478\tvalid_0's amex: 0.767063\n",
      "[200]\tvalid_0's binary_logloss: 0.225836\tvalid_0's amex: 0.783148\n",
      "[300]\tvalid_0's binary_logloss: 0.220879\tvalid_0's amex: 0.790551\n",
      "[400]\tvalid_0's binary_logloss: 0.218862\tvalid_0's amex: 0.79223\n",
      "[500]\tvalid_0's binary_logloss: 0.217733\tvalid_0's amex: 0.794063\n",
      "[600]\tvalid_0's binary_logloss: 0.217095\tvalid_0's amex: 0.795264\n",
      "[700]\tvalid_0's binary_logloss: 0.21667\tvalid_0's amex: 0.795489\n",
      "[800]\tvalid_0's binary_logloss: 0.216319\tvalid_0's amex: 0.795951\n",
      "[900]\tvalid_0's binary_logloss: 0.216054\tvalid_0's amex: 0.796092\n",
      "[1000]\tvalid_0's binary_logloss: 0.215904\tvalid_0's amex: 0.796103\n",
      "[1100]\tvalid_0's binary_logloss: 0.215779\tvalid_0's amex: 0.79659\n",
      "[1200]\tvalid_0's binary_logloss: 0.215779\tvalid_0's amex: 0.796849\n",
      "[1300]\tvalid_0's binary_logloss: 0.215764\tvalid_0's amex: 0.796583\n",
      "[1400]\tvalid_0's binary_logloss: 0.215767\tvalid_0's amex: 0.796985\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Fold 4 | 02:31 |  1219 trees |                Score = 0.79721 | Importance: Gain</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "------ FOLD: 5 of 5 -------\n",
      "[LightGBM] [Warning] num_threads is set=12, n_jobs=-1 will be ignored. Current value: num_threads=12\n",
      "[LightGBM] [Warning] early_stopping_round is set=240, early_stopping_rounds=240 will be ignored. Current value: early_stopping_round=240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nisha\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\nisha\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.247289\tvalid_0's amex: 0.763508\n",
      "[200]\tvalid_0's binary_logloss: 0.228612\tvalid_0's amex: 0.777116\n",
      "[300]\tvalid_0's binary_logloss: 0.223825\tvalid_0's amex: 0.783676\n",
      "[400]\tvalid_0's binary_logloss: 0.221768\tvalid_0's amex: 0.78657\n",
      "[500]\tvalid_0's binary_logloss: 0.220684\tvalid_0's amex: 0.788745\n",
      "[600]\tvalid_0's binary_logloss: 0.219994\tvalid_0's amex: 0.788639\n",
      "[700]\tvalid_0's binary_logloss: 0.219587\tvalid_0's amex: 0.789183\n",
      "[800]\tvalid_0's binary_logloss: 0.219207\tvalid_0's amex: 0.789472\n",
      "[900]\tvalid_0's binary_logloss: 0.218987\tvalid_0's amex: 0.79058\n",
      "[1000]\tvalid_0's binary_logloss: 0.218902\tvalid_0's amex: 0.790066\n",
      "[1100]\tvalid_0's binary_logloss: 0.218826\tvalid_0's amex: 0.790647\n",
      "[1200]\tvalid_0's binary_logloss: 0.21875\tvalid_0's amex: 0.790618\n",
      "[1300]\tvalid_0's binary_logloss: 0.218718\tvalid_0's amex: 0.790384\n",
      "[1400]\tvalid_0's binary_logloss: 0.218706\tvalid_0's amex: 0.79101\n",
      "[1500]\tvalid_0's binary_logloss: 0.218687\tvalid_0's amex: 0.791006\n",
      "[1600]\tvalid_0's binary_logloss: 0.218715\tvalid_0's amex: 0.790731\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Fold 5 | 03:02 |  1418 trees |                Score = 0.79154 | Importance: Gain</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>['S_5_last', 'B_38_last', 'D_112_last', 'D_142_last', 'B_10_last', 'D_54_last', 'D_61_last', 'D_133_last', 'D_134_last', 'R_27_last', 'D_105_last', 'B_16_last', 'R_13_last', 'D_69_last', 'R_11_last', 'S_24_last', 'B_18_last', 'D_75_last', 'D_120_last', 'S_19_last', 'D_62_last', 'R_12_last', 'D_106_last', 'D_96_last', 'B_2_last', 'D_122_last', 'R_3_last', 'R_20_last', 'D_49_last', 'B_29_last', 'B_42_last', 'R_26_last', 'B_37_last', 'S_8_last', 'B_22_last', 'D_81_last', 'D_46_last', 'R_19_last', 'D_39_last', 'D_125_last', 'D_117_last', 'D_91_last', 'D_116_last', 'B_23_last', 'B_30_last', 'B_32_last', 'B_28_last', 'S_27_last', 'S_11_last', 'B_12_last', 'P_3_last', 'D_71_last', 'D_45_last', 'D_58_last', 'D_141_last', 'D_44_last', 'R_8_last', 'D_72_last', 'R_1_last', 'S_6_last', 'D_53_last', 'R_2_last', 'B_11_last', 'D_65_last', 'B_13_last', 'D_121_last', 'S_25_last', 'D_131_last', 'R_5_last', 'B_9_last', 'D_63_last', 'B_33_last', 'D_77_last', 'P_4_last', 'S_26_last', 'D_126_last', 'B_14_last', 'D_60_last', 'D_48_last', 'B_3_last', 'D_82_last', 'B_5_last', 'D_114_last', 'S_20_last', 'D_70_last', 'S_3_last', 'D_51_last', 'B_39_last', 'B_41_last', 'D_83_last', 'R_10_last', 'D_78_last', 'R_7_last', 'B_25_last', 'D_79_last', 'B_7_last', 'D_145_last', 'B_24_last', 'B_20_last', 'D_52_last', 'D_124_last', 'B_15_last', 'D_119_last', 'D_56_last', 'D_41_last', 'R_15_last', 'B_17_last', 'D_50_last', 'D_127_last', 'B_1_last', 'D_80_last', 'R_9_last', 'B_8_last', 'B_36_last', 'S_16_last', 'D_86_last', 'D_42_last', 'S_13_last', 'D_47_last', 'D_130_last', 'S_12_last', 'S_23_last', 'D_76_last', 'D_55_last', 'B_21_last', 'B_4_last', 'S_22_last', 'B_6_last', 'R_14_last', 'B_19_last', 'D_138_last', 'D_59_last', 'P_2_last', 'S_7_last', 'B_40_last', 'D_43_last', 'S_9_last', 'D_140_last', 'D_73_last', 'B_26_last', 'D_132_last', 'R_4_last', 'R_6_last', 'S_5_min', 'B_14_min', 'D_115_min', 'B_36_min', 'D_112_min', 'D_60_min', 'D_102_min', 'D_48_min', 'B_5_min', 'D_42_min', 'D_74_min', 'D_70_min', 'B_28_min', 'S_11_min', 'S_3_min', 'S_12_min', 'P_3_min', 'S_23_min', 'D_51_min', 'D_71_min', 'R_27_min', 'B_16_min', 'D_45_min', 'D_58_min', 'D_55_min', 'D_128_min', 'D_141_min', 'B_4_min', 'D_83_min', 'D_144_min', 'D_75_min', 'D_78_min', 'B_19_min', 'D_118_min', 'R_1_min', 'D_62_min', 'D_53_min', 'D_145_min', 'D_59_min', 'B_20_min', 'D_113_min', 'B_2_min', 'B_13_min', 'B_15_min', 'D_56_min', 'D_41_min', 'D_119_min', 'D_122_min', 'P_2_min', 'S_7_min', 'D_121_min', 'S_25_min', 'B_17_min', 'B_29_min', 'B_42_min', 'D_50_min', 'S_9_min', 'B_9_min', 'D_140_min', 'B_33_min', 'D_46_min', 'D_39_min', 'D_132_min', 'S_5_max', 'B_14_max', 'D_112_max', 'D_107_max', 'D_142_max', 'D_60_max', 'D_48_max', 'B_10_max', 'B_3_max', 'D_82_max', 'B_5_max', 'D_74_max', 'D_61_max', 'D_133_max', 'D_134_max', 'D_70_max', 'S_3_max', 'R_27_max', 'D_105_max', 'B_16_max', 'D_136_max', 'R_11_max', 'D_128_max', 'B_39_max', 'S_24_max', 'R_10_max', 'D_144_max', 'B_18_max', 'D_78_max', 'R_7_max', 'B_25_max', 'B_7_max', 'D_145_max', 'B_24_max', 'D_52_max', 'D_124_max', 'B_2_max', 'D_122_max', 'B_15_max', 'R_3_max', 'D_119_max', 'R_20_max', 'D_56_max', 'D_41_max', 'D_49_max', 'D_84_max', 'B_29_max', 'B_42_max', 'R_26_max', 'B_17_max', 'D_50_max', 'B_37_max', 'S_8_max', 'D_46_max', 'D_39_max', 'D_125_max', 'D_110_max', 'B_1_max', 'D_80_max', 'D_115_max', 'B_8_max', 'D_91_max', 'B_23_max', 'D_102_max', 'D_135_max', 'S_16_max', 'S_13_max', 'D_42_max', 'S_27_max', 'S_11_max', 'D_47_max', 'S_12_max', 'B_12_max', 'P_3_max', 'S_23_max', 'D_71_max', 'D_76_max', 'D_45_max', 'D_58_max', 'D_55_max', 'B_21_max', 'D_141_max', 'B_4_max', 'D_111_max', 'D_44_max', 'S_22_max', 'B_6_max', 'R_14_max', 'B_19_max', 'R_8_max', 'D_138_max', 'D_118_max', 'D_72_max', 'R_1_max', 'D_59_max', 'D_65_max', 'D_123_max', 'B_13_max', 'P_2_max', 'S_7_max', 'D_121_max', 'S_25_max', 'B_40_max', 'D_43_max', 'D_131_max', 'R_5_max', 'B_9_max', 'B_33_max', 'D_77_max', 'D_140_max', 'D_73_max', 'R_17_max', 'P_4_max', 'D_132_max', 'S_15_max', 'S_26_max', 'R_6_max', 'S_5_avg', 'D_112_avg', 'D_142_avg', 'B_10_avg', 'D_54_avg', 'D_61_avg', 'D_133_avg', 'D_134_avg', 'R_27_avg', 'B_16_avg', 'D_136_avg', 'D_69_avg', 'D_128_avg', 'R_11_avg', 'D_129_avg', 'D_144_avg', 'B_18_avg', 'D_75_avg', 'D_62_avg', 'D_96_avg', 'D_113_avg', 'B_2_avg', 'D_122_avg', 'R_3_avg', 'R_20_avg', 'D_84_avg', 'B_29_avg', 'B_42_avg', 'R_26_avg', 'B_37_avg', 'B_22_avg', 'D_46_avg', 'D_39_avg', 'D_125_avg', 'D_91_avg', 'B_23_avg', 'D_135_avg', 'B_32_avg', 'R_21_avg', 'B_28_avg', 'S_11_avg', 'B_12_avg', 'D_92_avg', 'P_3_avg', 'D_71_avg', 'D_45_avg', 'D_58_avg', 'D_141_avg', 'D_108_avg', 'D_44_avg', 'R_8_avg', 'D_72_avg', 'R_1_avg', 'S_6_avg', 'R_24_avg', 'D_53_avg', 'R_2_avg', 'B_11_avg', 'D_65_avg', 'B_13_avg', 'D_121_avg', 'S_25_avg', 'D_131_avg', 'B_9_avg', 'D_77_avg', 'B_33_avg', 'P_4_avg', 'S_26_avg', 'B_14_avg', 'S_18_avg', 'D_60_avg', 'D_48_avg', 'B_3_avg', 'D_82_avg', 'B_5_avg', 'D_74_avg', 'D_70_avg', 'S_3_avg', 'D_51_avg', 'D_104_avg', 'B_39_avg', 'B_41_avg', 'R_10_avg', 'D_78_avg', 'R_7_avg', 'B_25_avg', 'D_145_avg', 'B_24_avg', 'B_20_avg', 'D_124_avg', 'B_15_avg', 'D_119_avg', 'D_41_avg', 'R_15_avg', 'B_17_avg', 'D_50_avg', 'B_1_avg', 'D_80_avg', 'D_115_avg', 'R_9_avg', 'B_8_avg', 'D_103_avg', 'S_16_avg', 'D_86_avg', 'D_42_avg', 'S_13_avg', 'D_47_avg', 'S_12_avg', 'S_23_avg', 'D_76_avg', 'R_22_avg', 'D_55_avg', 'B_21_avg', 'B_4_avg', 'S_22_avg', 'B_6_avg', 'R_14_avg', 'B_19_avg', 'D_118_avg', 'R_16_avg', 'D_59_avg', 'D_123_avg', 'P_2_avg', 'S_7_avg', 'B_40_avg', 'D_43_avg', 'D_94_avg', 'S_9_avg', 'D_140_avg', 'D_73_avg', 'R_17_avg', 'D_132_avg', 'S_15_avg'] OVERALL SCORE : 0.793412</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2h 21min 32s\n",
      "Wall time: 12min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# variant_df = variant_df[reversed(list(variant_df.columns))]\n",
    "# we keep this copy so that original feature doens't stay on top!!!\n",
    "# it's some weird quirk i found\n",
    "# variant_df[missingvaluefeature+\"_COPY\"] = variant_df[missingvaluefeature]\n",
    "# del variant_df[missingvaluefeature]\n",
    "features_x = [f for f in df_train.columns if f != 'customer_ID' and f != 'target' and f!='S_2']\n",
    "feature_y = 'target'\n",
    "# df_x = df_train[features_x]\n",
    "# df_y = df_train_wt[feature_y]\n",
    "# target =\n",
    "\n",
    "total_splits = 5\n",
    "kf = StratifiedKFold(n_splits=total_splits, shuffle=True)\n",
    "fold_scores = []\n",
    "models = []\n",
    "# NOT TOO SURE about feature_importances when all features are variants!! it's non decisive\n",
    "for fold, (idx_train, idx_dev) in enumerate(kf.split(df_train, target)):\n",
    "    print(f\"------ FOLD: {fold+1} of {total_splits} -------\")\n",
    "    train_x, dev_x, train_y, dev_y, model = None, None, None, None, None\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    X_tr = df_train.iloc[idx_train][features_x]\n",
    "    X_va = df_train.iloc[idx_dev][features_x]\n",
    "    y_tr = target[idx_train]\n",
    "    y_va = target[idx_dev]\n",
    "   \n",
    "    model = create_lgbm_model_with_config(importance_type='gain')\n",
    "    model.fit(X_tr, y_tr,\n",
    "                  eval_set = [(X_va, y_va)], \n",
    "                  eval_metric=[lgbm_eval_metric_amex],\n",
    "                  callbacks=[log_evaluation(100)])\n",
    "\n",
    "    y_va_pred = model.predict_proba(X_va, raw_score=True)\n",
    "    score = fast_amex_metric(y_va, y_va_pred)\n",
    "    n_trees = model.best_iteration_\n",
    "    if n_trees is None: n_trees = model.n_estimators\n",
    "    print(\"-------------------------------------\")\n",
    "    display(HTML(f\"<h3>Fold {fold+1} | {str(datetime.now() - start_time)[-12:-7]} |\"\n",
    "          f\" {n_trees:5} trees |\"\n",
    "          f\"                Score = {score:.5f} | Importance: Gain</h3>\"))\n",
    "    print(\"-------------------------------------\")\n",
    "    # fold_scores.append(score)\n",
    "    track_score(features_x, score)\n",
    "    models.append(model)\n",
    "    # feature_importance_tuples = sorted(zip(X_tr.columns,model.feature_importances_), key=lambda x:x[1])\n",
    "    # # feature_importance_tuples = sorted(zip(X_tr.columns,model.feature_importance(importance_type='gain')), key=lambda x:x[1])\n",
    "    # feature_importance_map = {k:v for k,v in feature_importance_tuples}\n",
    "    # feature_imp = pd.DataFrame(feature_importance_tuples, columns=['feature','importance'])\n",
    "    # fig = pltex.bar(feature_imp, x='importance', y='feature', height=500)\n",
    "    # fig.show()\n",
    "\n",
    "\n",
    "    # track_score(features_x, score)\n",
    "            # feature_importance_tuples = sorted(zip(X_tr.columns,model.feature_importances_), key=lambda x:x[1])\n",
    "            # # feature_importance_tuples = sorted(zip(X_tr.columns,model.feature_importance(importance_type='gain')), key=lambda x:x[1])\n",
    "            # feature_importance_map = {k:v for k,v in feature_importance_tuples}\n",
    "            # feature_imp = pd.DataFrame(feature_importance_tuples, columns=['feature','importance'])\n",
    "            # fig = pltex.bar(feature_imp, x='importance', y='feature', height=500)\n",
    "            # fig.show()\n",
    "\n",
    "show_score(features_x)\n",
    "        \n",
    "    # input(\"-------- ENTER TO CONINUE -------\")\n",
    "       \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "281316b0-0116-40df-9937-a2b206e65fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18763"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b363782e-2426-45b8-a0a0-a6d8c4bb0eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9min 3s\n",
      "Wall time: 8min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df_test = load_prepare_amex_dataset(\"test_data_emp1\")\n",
    "# df_test_last1 = df_test.groupby('customer_ID').last()\n",
    "# df_test_last1 = df_test_last1.reset_index()\n",
    "# df_test_wt = pd.merge(df_test_last1, train_label_df, how='inner', on = 'customer_ID')#.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06178114-6abe-4988-af33-356d5a59063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_x = [f for f in df_test.columns if f not in ['customer_ID', 'target', 'prediction', 'S_2']]\n",
    "X_tr = df_test[features_x]\n",
    "\n",
    "predictions = []\n",
    "for model in models:\n",
    "    preds = model.predict_proba(X_tr)\n",
    "    predictions.append(preds[:,1])#/len(models)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a85e5291-43e1-427e-ba8d-a3e45b6cd493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924621"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d61c8aa9-6115-4ddd-aa0d-5d4f5a6a161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_last1['prediction'] = np.max(predictions, axis=0)\n",
    "df_test_last1[['customer_ID','prediction']].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d876111-9020-48cd-8ff2-62e00c1ef8a0",
   "metadata": {},
   "source": [
    "```\n",
    "Data: EMP0\n",
    "Features: similar aggregation like https://www.kaggle.com/code/ambrosm/amex-lightgbm-quickstart, except Cat aggs\n",
    "Model: 5 fold, lgbm standard\n",
    "Ensemble: choose max prediction value;highest proba out of 5 models\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
