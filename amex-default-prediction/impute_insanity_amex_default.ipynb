{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4dade30-94ed-4d49-bb1e-553b91996bb0",
   "metadata": {},
   "source": [
    "## IMPUTE INSANITY!:\n",
    "### before we move onto neural networks we need to impute the NANs\n",
    "#### How do we even know we've correctly imputed?\n",
    "#### **Try out every impute methods and per feature set train the GBM and see which one got most importance**\n",
    "\n",
    "### TODO\n",
    "- #### [KNN , DeepLearner, other novel ways to impute](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)\n",
    "  - KNN thoughts: we can perhaps use subset of features with no nans, but then the records will become similar, and decision trees already predict the same result for similar looking rows? then what's the point of imputing value same as similar looking rows?\n",
    "- ### DON't Impute rather provide the Flag wether the feature has value or not, kinda like mask; we did this in Optiver (not the transformer sequence mask , but we have mask about weather the second/grouped seconds is missing or not)\n",
    "  - #### [Results on passing information about missing values to neural networks](https://machinelearningmastery.com/binary-flags-for-missing-values-for-machine-learning/)\n",
    "- ### AMEND ABOVE: Impute as well as pass the missing Flag too, so neural network learns better\n",
    "- ### Categorical Imputation: train on data with missing categorical values as target variable! **We can even use TEST set to do this!!!! yay!**\n",
    "  - #### [Training to find missing categorical values](https://www.analyticsvidhya.com/blog/2021/04/how-to-handle-missing-values-of-categorical-variables/)\n",
    "\n",
    "### IMPORTANT FLOAT16 aggregations return NONE!!! mean, sum most of it doesn't work :-/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d809121-a984-441c-8885-a94938548420",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- ### MULTILEVEL IMPUTATION DEFAULT\n",
    "- ### LEVEL 1: out of all the variant+original; select the one with best amex scores DEFAULT_EMP1\n",
    "    - #### This is partial nans\n",
    "- ### Level 2: now that we have partial none df with best variants(including original with nans) ; TRAIN ML MODEL to have PREDICTED/BS variant\n",
    "    - #### In the order of lowest missing values to highest missing values\n",
    "    - #### we Target the feature with lowest amount on NAN as we have maximum training rows; then slowly cascade to more missing row features\n",
    "    - #### once we have computed PREDICTED/BS variants for all of the features with nans\n",
    "- ### Level 3: from DEFAULT_EMP1 vs PREDICTED variant select the one with maximum amex score (this is kinda like back to stage 1 but with improved train_df_wt)\n",
    "    - #### repetetively select and choose the best varint out of the two\n",
    "    - #### DEFAULT_EMP2 is generated!\n",
    "    - #### Now there is a scope of repeating Level 2 again with using base_df of EMP2 and then going to EMP3,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799d3de7-7230-42ee-b699-26d02974f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHERE_THIS = \"local\" # local|kaggle\n",
    "\n",
    "BS_DATASET = \"amex-default-prediction-binarysentient\"\n",
    "if WHERE_THIS == \"kaggle\":\n",
    "    INPUT_PATH = \"/kaggle/input/amex-default-prediction/\"\n",
    "    OUTPUT_PATH = \"/kaggle/working/\"\n",
    "    TEMP_PATH = \"/kaggle/temp/\"\n",
    "elif WHERE_THIS == \"local\":\n",
    "    INPUT_PATH = \"input/amex-default-prediction\"\n",
    "    OUTPUT_PATH = \"working\"\n",
    "    TEMP_PATH = \"temp\"\n",
    "import os\n",
    "BS_PATH = os.path.join(TEMP_PATH,BS_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b8e156e-282d-4a57-8738-46695bcc46cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'r2_score' from 'lightgbm' (C:\\Users\\nisha\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, HTML\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier, log_evaluation, LGBMRegressor, r2_score\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold, KFold\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpltex\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'r2_score' from 'lightgbm' (C:\\Users\\nisha\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from IPython.display import display, HTML\n",
    "from lightgbm import LGBMClassifier, log_evaluation, LGBMRegressor, r2_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import plotly.express as pltex\n",
    "\n",
    "import impute_insanity\n",
    "from impute_insanity import load_prepare_amex_dataset\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "for dirname, _, filenames in os.walk(INPUT_PATH):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f240c1ff-1841-49f9-b7af-c868c9dd855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_features = ['customer_ID']\n",
    "discard_features = ['D_66']\n",
    "datetime_features = ['S_2']\n",
    "cat_features = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_68']\n",
    "target_features = ['target']\n",
    "numeric_features = ['R_2', 'S_25', 'D_118', 'B_5', 'D_60', 'B_21', 'D_115', 'S_15', 'D_84', 'D_122', 'B_9', 'B_33', 'D_53', 'R_24', 'D_94', 'D_56', 'D_139', 'R_28', 'B_3', 'S_20', 'B_31', 'D_133', 'B_2', 'S_12', 'D_71', 'D_96', 'S_7', 'D_72', 'B_36', 'B_41', 'S_5', 'D_41', 'R_22', 'R_8', 'D_140', 'D_47', 'D_89', 'P_2', 'R_19', 'D_59', 'B_23', 'S_3', 'D_145', 'D_103', 'B_19', 'R_20', 'D_73', 'D_136', 'D_141', 'D_142', 'B_22', 'D_46', 'B_29', 'B_25', 'D_128', 'B_18', 'D_86', 'D_109', 'B_8', 'B_17', 'R_17', 'B_12', 'D_54', 'D_74', 'S_16', 'B_6', 'D_49', 'D_80', 'S_8', 'B_7', 'D_144', 'B_27', 'B_26', 'R_25', 'R_23', 'D_82', 'D_111', 'B_10', 'D_113', 'R_4', 'D_48', 'R_26', 'S_23', 'B_11', 'D_104', 'D_134', 'D_79', 'P_3', 'D_132', 'D_137', 'D_135', 'B_28', 'D_88', 'S_13', 'D_51', 'D_61', 'D_75', 'D_69', 'R_16', 'S_6', 'S_17', 'D_93', 'B_20', 'D_112', 'D_123', 'D_130', 'B_1', 'D_78', 'D_92', 'S_27', 'D_44', 'B_16', 'R_5', 'D_43', 'S_18', 'B_15', 'D_39', 'D_50', 'D_55', 'S_9', 'D_105', 'D_70', 'R_18', 'D_125', 'D_58', 'S_24', 'D_110', 'D_42', 'R_6', 'D_81', 'R_7', 'D_138', 'D_52', 'R_27', 'D_124', 'D_45', 'D_91', 'D_108', 'S_22', 'B_14', 'D_83', 'R_13', 'D_87', 'S_19', 'D_131', 'R_21', 'B_40', 'R_3', 'D_65', 'B_13', 'D_129', 'D_119', 'B_32', 'R_9', 'B_24', 'D_127', 'D_106', 'D_102', 'R_1', 'R_14', 'B_37', 'D_107', 'R_15', 'R_12', 'P_4', 'R_10', 'D_121', 'R_11', 'S_11', 'D_76', 'D_143', 'B_39', 'B_42', 'D_62', 'B_4', 'S_26', 'D_77']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2e62d91-78a3-44d5-a4dc-3697f03206a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "## TODO: FAST AMEX implementation is not accurate; convert the dataframe accurate version to directly work with numpy arrays\n",
    "# @yunchonggan's fast metric implementation\n",
    "# From https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "def fast_amex_metric(y_true: np.array, y_pred: np.array) -> float:\n",
    "\n",
    "    # count of positives and negatives\n",
    "    n_pos = y_true.sum()\n",
    "    n_neg = y_true.shape[0] - n_pos\n",
    "\n",
    "    # sorting by descring prediction values\n",
    "    indices = np.argsort(y_pred)[::-1]\n",
    "    preds, target = y_pred[indices], y_true[indices]\n",
    "\n",
    "    # filter the top 4% by cumulative row weights\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_filter = cum_norm_weight <= 0.04\n",
    "\n",
    "    # default rate captured at 4%\n",
    "    d = target[four_pct_filter].sum() / n_pos\n",
    "\n",
    "    # weighted gini coefficient\n",
    "    lorentz = (target / n_pos).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    # max weighted gini coefficient\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    # normalized weighted gini coefficient\n",
    "    g = gini / gini_max\n",
    "\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "# Need Lightgbm supported eval metric\n",
    "#Custom eval function expects a callable with following signatures: func(y_true, y_pred), func(y_true, y_pred, weight) or func(y_true, y_pred, weight, group) and returns (eval_name, eval_result, is_higher_better) or list of (eval_name, eval_result, is_higher_better):\n",
    "def lgbm_eval_metric_amex(y_true, y_pred):\n",
    "    amex_metric = fast_amex_metric(y_true, y_pred)\n",
    "    return ('amex', amex_metric, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "959bc1be-77fe-42db-8dde-de902027aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_score_track = {}\n",
    "def track_score(key, score):\n",
    "    key = str(key)\n",
    "    if key not in fold_score_track:\n",
    "        fold_score_track[key] = []\n",
    "    fold_score_track[key].append(score)\n",
    "\n",
    "def show_score(key):\n",
    "    key = str(key)\n",
    "    display(HTML(f\"<h3>{key} OVERALL SCORE : {np.mean(fold_score_track[key]):0.6f}</h3>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3608451e-1569-4a8c-a4be-6be2ed1f6c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data nan 122\n",
      "train_data_emp1 nan 51\n",
      "test_data nan 121\n",
      "test_data_emp1 nan 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dataset in ['train_data','train_data_emp1', 'test_data','test_data_emp1']:\n",
    "    df = load_prepare_amex_dataset(dataset)\n",
    "    nanmap = df.isna().sum().reset_index().rename({'index':'feature',0:'nan_count'}, axis=1).to_dict('records')\n",
    "    print(dataset, \"nan\", len([x for x in nanmap if x['nan_count']>0]))\n",
    "    del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0622f8c-3f1e-45e7-af4f-d611bd8943b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_emp1 nan 34\n",
      "test_data_emp1 nan 37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_nan_columns = {}\n",
    "for dataset in ['train_data_emp1','test_data_emp1']:# ['train_data','train_data_emp1', 'test_data','test_data_emp1']:\n",
    "    df = load_prepare_amex_dataset(dataset)\n",
    "    nanmap = df.isna().sum().reset_index().rename({'index':'feature',0:'nan_count'}, axis=1).to_dict('records')\n",
    "    print(dataset, \"nan\", len([x for x in nanmap if x['nan_count']>0]))\n",
    "    dataset_nan_columns[dataset] = [x['feature'] for x in nanmap if x['nan_count']>0]\n",
    "    del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b0d822e-1d0f-477c-9e0b-159b507dad10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D_86', 'S_12', 'S_17']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_only_missing = list(set(dataset_nan_columns['test_data_emp1']).difference(set(dataset_nan_columns['train_data_emp1'])))\n",
    "test_only_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f15e059-744c-4e51-b5c2-2336b868bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_train_df = load_prepare_amex_dataset('test_data_emp1_nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48cc5925-4c26-4811-aeed-b3ec0d2818a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan 3\n"
     ]
    }
   ],
   "source": [
    "nanmap = try_train_df.isna().sum().reset_index().rename({'index':'feature',0:'nan_count'}, axis=1).to_dict('records')\n",
    "print(\"nan\", len([x for x in nanmap if x['nan_count']>0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fe8c87d-99ed-4337-bf80-bb9a9b7eb753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del try_train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "742a8170-903a-4590-a9eb-0a777d2e203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_scored_df = load_prepare_amex_dataset(f\"variant_scores_emp1\")\n",
    "variant_scored_df\n",
    "variant_scored_df = variant_scored_df[~variant_scored_df['feature_variant'].str.contains('D_66')]\n",
    "variant_scored_df.to_parquet(os.path.join(BS_PATH,'variant_scores_emp1.parquet'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2265dc8-0c6f-4a81-a803-564891fba5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature': 'S_12', 'nan_count': 2385},\n",
       " {'feature': 'S_17', 'nan_count': 12535},\n",
       " {'feature': 'D_86', 'nan_count': 1067}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in nanmap if x['nan_count']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fed28ca2-2f9d-4136-8d3f-faa5104dfafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_scores_df = load_prepare_amex_dataset(f\"variant_scores_emp1\")\n",
    "variant_scores_df = variant_scores_df.sort_values(['score','feature_variant'], ascending=False)\n",
    "# variant_scored_df\n",
    "# variant_scored_df = variant_scored_df[~variant_scored_df['feature_variant'].str.contains('D_64')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83338b70-2d26-40c0-8632-41e68c5a6148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_variant</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [feature_variant, score]\n",
       "Index: []"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variant_scores_df[variant_scores_df['feature_variant'].str.contains('D_66')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58261442-6aec-4a7d-9edc-b8a0414bde68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_variant</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>P_2_global_max</td>\n",
       "      <td>0.638529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>P_2_global_median</td>\n",
       "      <td>0.638485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>P_2_global_mean</td>\n",
       "      <td>0.638406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>P_2_global_min</td>\n",
       "      <td>0.638382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>P_2_local_mean</td>\n",
       "      <td>0.638281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>D_110_local_mean</td>\n",
       "      <td>0.021387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>D_110_local_linear_interpolate</td>\n",
       "      <td>0.021387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>D_110_global_median</td>\n",
       "      <td>0.021387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>D_110_global_mean</td>\n",
       "      <td>0.021387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>D_109</td>\n",
       "      <td>0.020994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>944 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    feature_variant     score\n",
       "331                  P_2_global_max  0.638529\n",
       "332               P_2_global_median  0.638485\n",
       "329                 P_2_global_mean  0.638406\n",
       "330                  P_2_global_min  0.638382\n",
       "333                  P_2_local_mean  0.638281\n",
       "..                              ...       ...\n",
       "861                D_110_local_mean  0.021387\n",
       "862  D_110_local_linear_interpolate  0.021387\n",
       "860             D_110_global_median  0.021387\n",
       "857               D_110_global_mean  0.021387\n",
       "80                            D_109  0.020994\n",
       "\n",
       "[944 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variant_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87a08f1f-76f1-44fa-a307-e72c9688b0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>D_116</th>\n",
       "      <th>D_116_global_most_frequent</th>\n",
       "      <th>D_116_global_least_frequent</th>\n",
       "      <th>D_116_global_unknown</th>\n",
       "      <th>D_116_local_most_frequent</th>\n",
       "      <th>D_116_local_least_frequent</th>\n",
       "      <th>D_116_local_nearest_interpolate_mf</th>\n",
       "      <th>D_116_local_nearest_interpolate_lf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531446</th>\n",
       "      <td>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531447</th>\n",
       "      <td>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531448</th>\n",
       "      <td>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531449</th>\n",
       "      <td>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531450</th>\n",
       "      <td>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5531451 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               customer_ID D_116  \\\n",
       "0        0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...   0.0   \n",
       "1        0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...   0.0   \n",
       "2        0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...   0.0   \n",
       "3        0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...   0.0   \n",
       "4        0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...   0.0   \n",
       "...                                                    ...   ...   \n",
       "5531446  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...   0.0   \n",
       "5531447  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...   0.0   \n",
       "5531448  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...   0.0   \n",
       "5531449  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...   0.0   \n",
       "5531450  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...   0.0   \n",
       "\n",
       "        D_116_global_most_frequent D_116_global_least_frequent  \\\n",
       "0                              0.0                         0.0   \n",
       "1                              0.0                         0.0   \n",
       "2                              0.0                         0.0   \n",
       "3                              0.0                         0.0   \n",
       "4                              0.0                         0.0   \n",
       "...                            ...                         ...   \n",
       "5531446                        0.0                         0.0   \n",
       "5531447                        0.0                         0.0   \n",
       "5531448                        0.0                         0.0   \n",
       "5531449                        0.0                         0.0   \n",
       "5531450                        0.0                         0.0   \n",
       "\n",
       "        D_116_global_unknown D_116_local_most_frequent  \\\n",
       "0                        0.0                       0.0   \n",
       "1                        0.0                       0.0   \n",
       "2                        0.0                       0.0   \n",
       "3                        0.0                       0.0   \n",
       "4                        0.0                       0.0   \n",
       "...                      ...                       ...   \n",
       "5531446                  0.0                       0.0   \n",
       "5531447                  0.0                       0.0   \n",
       "5531448                  0.0                       0.0   \n",
       "5531449                  0.0                       0.0   \n",
       "5531450                  0.0                       0.0   \n",
       "\n",
       "        D_116_local_least_frequent D_116_local_nearest_interpolate_mf  \\\n",
       "0                              0.0                                0.0   \n",
       "1                              0.0                                0.0   \n",
       "2                              0.0                                0.0   \n",
       "3                              0.0                                0.0   \n",
       "4                              0.0                                0.0   \n",
       "...                            ...                                ...   \n",
       "5531446                        0.0                                0.0   \n",
       "5531447                        0.0                                0.0   \n",
       "5531448                        0.0                                0.0   \n",
       "5531449                        0.0                                0.0   \n",
       "5531450                        0.0                                0.0   \n",
       "\n",
       "        D_116_local_nearest_interpolate_lf  \n",
       "0                                      0.0  \n",
       "1                                      0.0  \n",
       "2                                      0.0  \n",
       "3                                      0.0  \n",
       "4                                      0.0  \n",
       "...                                    ...  \n",
       "5531446                                0.0  \n",
       "5531447                                0.0  \n",
       "5531448                                0.0  \n",
       "5531449                                0.0  \n",
       "5531450                                0.0  \n",
       "\n",
       "[5531451 rows x 9 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_variants = load_prepare_amex_dataset(f\"train_data_D_116\")\n",
    "feature_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15010cab-671d-4090-875c-7291786ed804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the df we refer to here are the emp1 version, so df_train is df_train_emp1\n",
    "# df_train_nan_mask is NaNs in original df\n",
    "\n",
    "\n",
    "df_train_nan_mask = load_prepare_amex_dataset(\"train_data_nan_mask\")\n",
    "df_test_nan_mask = load_prepare_amex_dataset(\"test_data_nan_mask\")\n",
    "df_train_rows = len(df_train_nan_mask)\n",
    "df_test_rows = len(df_test_nan_mask)\n",
    "df_all_nan_mask = pd.concat([df_train_nan_mask, df_test_nan_mask],ignore_index=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f76974fe-5767-4254-83ef-9e870404cb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_features_df = df_train_nan_mask.sum().reset_index().rename({'index':'feature',0:'nan_count'}, axis=1)\n",
    "len(nan_features_df[nan_features_df['nan_count']>0]['feature'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "126b3394-b55a-4228-aede-d4ac8a547493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_features_df = df_test_nan_mask.sum().reset_index().rename({'index':'feature',0:'nan_count'}, axis=1)\n",
    "len(nan_features_df[nan_features_df['nan_count']>0]['feature'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96dbc821-16aa-4550-8ca5-7e3eb9df2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_features_df = df_all_nan_mask.sum().reset_index().rename({'index':'feature',0:'nan_count'}, axis=1)\n",
    "missingvaluefeatures = nan_features_df[nan_features_df['nan_count']>0]['feature'].tolist()\n",
    "len(missingvaluefeatures)\n",
    "missingvaluefeature = missingvaluefeatures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e6ab93f-8a35-4cf6-bc7c-9d6926085b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_train, df_train_test, df_test_train, df_test_test = None, None, None, None\n",
    "for dataset in ['train','test']:\n",
    "    thedf = load_prepare_amex_dataset(f\"{dataset}_data_emp1\")\n",
    "    thedf['index'] = thedf.index\n",
    "    thedf['dataset'] = f\"{dataset}\"\n",
    "    exec(f\"df_{dataset}_train = thedf.loc[~df_{dataset}_nan_mask[missingvaluefeature]]\")\n",
    "    exec(f\"df_{dataset}_test = thedf.loc[df_{dataset}_nan_mask[missingvaluefeature]]\")\n",
    "    del thedf\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "204dd8c8-376c-441a-b6b0-053757c03370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_train = pd.concat([df_train_train, df_test_train],ignore_index=True)\n",
    "del df_train_train, df_test_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ae4042f-7614-4b36-9234-cda67ebedf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lgbm_model_with_config(random_state=1, n_estimators=5000, importance_type=None, early_stopping_rounds=None, modeltype=\"classifier\"):\n",
    "    \"\"\"\n",
    "    Creates model with our desired and some default hyper params\n",
    "    importance_type: split|gain\n",
    "    \"\"\"\n",
    "    if early_stopping_rounds is None:\n",
    "        early_stopping_rounds = n_estimators//10\n",
    "    if modeltype == \"classifier\":\n",
    "        return LGBMClassifier(n_estimators=n_estimators,\n",
    "                              learning_rate=0.09, reg_lambda=50,\n",
    "                              min_child_samples=2400,\n",
    "                              num_leaves=95, num_threads=12,\n",
    "                              colsample_bytree=0.19,early_stopping_rounds=early_stopping_rounds,\n",
    "                              max_bins=511, random_state=random_state, importance_type=importance_type)\n",
    "    if modeltype == \"regressor\":\n",
    "        return LGBMRegressor(n_estimators=n_estimators,\n",
    "                              learning_rate=0.03, reg_lambda=50,\n",
    "                              min_child_samples=2400,\n",
    "                              num_leaves=95, num_threads=12,\n",
    "                              colsample_bytree=0.19,early_stopping_rounds=early_stopping_rounds,\n",
    "                              max_bins=511, random_state=random_state, importance_type=importance_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c9f9b8-1bdc-4d4e-8228-21c587883201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc70ee-7167-4fd5-b4d3-3c7c588d235b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "578011aa-d20e-416e-96db-7542b82354a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_variant</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R_7</td>\n",
       "      <td>0.191363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R_7_global_mean</td>\n",
       "      <td>0.191363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R_7_global_min</td>\n",
       "      <td>0.191363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R_7_global_max</td>\n",
       "      <td>0.191363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R_7_global_median</td>\n",
       "      <td>0.191363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>D_68_global_unknown</td>\n",
       "      <td>0.159087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>D_68_local_most_frequent</td>\n",
       "      <td>0.155616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>D_68_local_least_frequent</td>\n",
       "      <td>0.155923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>D_68_local_nearest_interpolate_mf</td>\n",
       "      <td>0.156455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>D_68_local_nearest_interpolate_lf</td>\n",
       "      <td>0.156330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>968 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature_variant     score\n",
       "0                                  R_7  0.191363\n",
       "1                      R_7_global_mean  0.191363\n",
       "2                       R_7_global_min  0.191363\n",
       "3                       R_7_global_max  0.191363\n",
       "4                    R_7_global_median  0.191363\n",
       "..                                 ...       ...\n",
       "963                D_68_global_unknown  0.159087\n",
       "964           D_68_local_most_frequent  0.155616\n",
       "965          D_68_local_least_frequent  0.155923\n",
       "966  D_68_local_nearest_interpolate_mf  0.156455\n",
       "967  D_68_local_nearest_interpolate_lf  0.156330\n",
       "\n",
       "[968 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variant_scored_df = load_prepare_amex_dataset(f\"variant_scores_emp1\")\n",
    "variant_scored_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9db03f0e-ce3a-4dc2-b2fa-edbf29a02333",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_scored_df = load_prepare_amex_dataset(f\"variant_scores_emp1\")\n",
    "variant_scored_df\n",
    "# variant_scored_df = variant_scored_df[~variant_scored_df['feature_variant'].str.contains('D_64')]\n",
    "# variant_scored_df.to_parquet(os.path.join(BS_PATH,'variant_scores_emp1.parquet'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6166adb4-de23-43f3-aa6f-b17ac7d71b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ FOLD: 1 of 5 -------\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 18.6 GiB for an array with shape (13431480, 186) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:32\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\sklearn.py:895\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y,\n\u001b[0;32m    889\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, init_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    890\u001b[0m         eval_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    891\u001b[0m         eval_init_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    892\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m'\u001b[39m, feature_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, categorical_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    893\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, init_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 895\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m                \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m                \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    273\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:2605\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   2598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[0;32m   2599\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[0;32m   2600\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   2601\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[0;32m   2602\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2603\u001b[0m     )\n\u001b[0;32m   2604\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[1;32m-> 2605\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2606\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[0;32m   2607\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1815\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[1;32m-> 1815\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1819\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[0;32m   1821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1538\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m   1536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_from_csc(data, params_str, ref_dataset)\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m-> 1538\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__init_from_np2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1654\u001b[0m, in \u001b[0;36mDataset.__init_from_np2d\u001b[1;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_void_p()\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mat\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32 \u001b[38;5;129;01mor\u001b[39;00m mat\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[1;32m-> 1654\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mmat\u001b[38;5;241m.\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# change non-float data to float data, need to copy\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(mat\u001b[38;5;241m.\u001b[39mreshape(mat\u001b[38;5;241m.\u001b[39msize), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 18.6 GiB for an array with shape (13431480, 186) and data type float64"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# variant_df = variant_df[reversed(list(variant_df.columns))]\n",
    "# we keep this copy so that original feature doens't stay on top!!!\n",
    "# it's some weird quirk i found\n",
    "# variant_df[missingvaluefeature+\"_COPY\"] = variant_df[missingvaluefeature]\n",
    "# del variant_df[missingvaluefeature]\n",
    "\n",
    "features_x = [f for f in df_all_train.columns if f not in ['customer_ID', 'target', 'S_2', 'index','dataset',missingvaluefeature]]\n",
    "feature_y = missingvaluefeature\n",
    "# df_x = df_train[features_x]\n",
    "# df_y = df_train_wt[feature_y]\n",
    "# target =\n",
    "target = df_all_train[missingvaluefeature]\n",
    "\n",
    "total_splits = 5\n",
    "\n",
    "# kf = StratifiedKFold(n_splits=total_splits, shuffle=True)\n",
    "kf = KFold(n_splits = total_splits, shuffle=True)\n",
    "fold_scores = []\n",
    "models = []\n",
    "# NOT TOO SURE about feature_importances when all features are variants!! it's non decisive\n",
    "for fold, (idx_train, idx_dev) in enumerate(kf.split(df_all_train, target)):\n",
    "    print(f\"------ FOLD: {fold+1} of {total_splits} -------\")\n",
    "    train_x, dev_x, train_y, dev_y, model = None, None, None, None, None\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    X_tr = df_all_train.iloc[idx_train][features_x]\n",
    "    X_va = df_all_train.iloc[idx_dev][features_x]\n",
    "    y_tr = target[idx_train]\n",
    "    y_va = target[idx_dev]\n",
    "   \n",
    "    model = create_lgbm_model_with_config(modeltype='regressor',importance_type='gain')\n",
    "    model.fit(X_tr, y_tr,\n",
    "                  eval_set = [(X_va, y_va)], \n",
    "                  eval_metric=['l2'],\n",
    "                  callbacks=[log_evaluation(10)]\n",
    "             )\n",
    "    n_trees = model.best_iteration_\n",
    "    if n_trees is None: n_trees = model.n_estimators\n",
    "  \n",
    "    models.append(model)\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "281316b0-0116-40df-9937-a2b206e65fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18763"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b363782e-2426-45b8-a0a0-a6d8c4bb0eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9min 3s\n",
      "Wall time: 8min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df_test = load_prepare_amex_dataset(\"test_data_emp1\")\n",
    "# df_test_last1 = df_test.groupby('customer_ID').last()\n",
    "# df_test_last1 = df_test_last1.reset_index()\n",
    "# df_test_wt = pd.merge(df_test_last1, train_label_df, how='inner', on = 'customer_ID')#.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06178114-6abe-4988-af33-356d5a59063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_x = [f for f in df_test.columns if f not in ['customer_ID', 'target', 'prediction', 'S_2']]\n",
    "X_tr = df_test[features_x]\n",
    "\n",
    "predictions = []\n",
    "for model in models:\n",
    "    preds = model.predict_proba(X_tr)\n",
    "    predictions.append(preds[:,1])#/len(models)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a85e5291-43e1-427e-ba8d-a3e45b6cd493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924621"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d61c8aa9-6115-4ddd-aa0d-5d4f5a6a161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_last1['prediction'] = np.max(predictions, axis=0)\n",
    "df_test_last1[['customer_ID','prediction']].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d876111-9020-48cd-8ff2-62e00c1ef8a0",
   "metadata": {},
   "source": [
    "```\n",
    "Data: EMP0\n",
    "Features: similar aggregation like https://www.kaggle.com/code/ambrosm/amex-lightgbm-quickstart, except Cat aggs\n",
    "Model: 5 fold, lgbm standard\n",
    "Ensemble: choose max prediction value;highest proba out of 5 models\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
